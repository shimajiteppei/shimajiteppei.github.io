<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Ubuntu blog</title><link>https://ubuntu.com//blog/feed</link><description>Ubuntu blog feed</description><atom:link href="https://ubuntu.com//blog/feed" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>Python Feedgen</generator><lastBuildDate>Thu, 13 Nov 2025 12:06:59 +0000</lastBuildDate><item><title>Canonical expands total coverage for Ubuntu LTS releases to 15 years with Legacy add-on</title><link>https://ubuntu.com//blog/canonical-expands-total-coverage-for-ubuntu-lts-releases-to-15-years-with-legacy-add-on</link><description>&lt;p&gt;Ubuntu Pro now supports LTS releases for up to 15 years through the Legacy add-on. More security, more stability, and greater control over upgrade timelines for enterprises.&lt;/p&gt;
</description><content:encoded>
&lt;figure class="wp-block-pullquote"&gt;&lt;blockquote&gt;&lt;p&gt;Expansion ensures business continuity without forcing major upgrades&lt;/p&gt;&lt;/blockquote&gt;&lt;/figure&gt;
&lt;p&gt;Today, Canonical announced the expansion of the Legacy add-on for Ubuntu Pro, extending total coverage for Ubuntu LTS releases to 15 years. Starting with Ubuntu 14.04 LTS (Trusty Tahr), this extension brings the full benefits of Ubuntu Pro ‚Äì including continuous security patching, compliance tooling and support for your OS ‚Äì to long-lived production systems.&lt;/p&gt;
&lt;p&gt;In highly regulated or hardware-dependent industries, upgrades threaten to disrupt tightly controlled security and compliance. For many organizations, maintaining production systems for more than a decade is complex, but remains a more sensible option than a full upgrade.&lt;/p&gt;
&lt;p&gt;That‚Äôs why, in 2024, we first &lt;a href="https://canonical.com/blog/canonical-expands-long-term-support-to-12-years-starting-with-ubuntu-14-04-lts"&gt;introduced the Legacy add-on&lt;/a&gt; for Ubuntu Pro, starting with Ubuntu 14.04 LTS (Trusty Tahr). The Legacy add-on increased the total maintenance window for Ubuntu LTS releases to 12 years: five years of standard security maintenance, five years of Expanded Security Maintenance (ESM), and two years of additional coverage with the Legacy add-on ‚Äì with optional support throughout. Due to the positive reception and growing interest in longer lifecycle coverage, we‚Äôre excited to now extend the Legacy add-on to 5 years, bringing a 15-year security maintenance and support window to Ubuntu LTS releases.&lt;/p&gt;
&lt;h1 class="wp-block-heading"&gt;A 15-year lifecycle for stability&lt;/h1&gt;
&lt;p&gt;Throughout this 15-year window, Ubuntu Pro provides continuous security maintenance across the entire Ubuntu base, kernel, and key open source components. Canonical‚Äôs security team actively scans, triages, and backports critical, high, and select medium CVEs to all maintained LTS releases, ensuring security without forcing disruptive major upgrades that break compatibility or require re-certification.&lt;/p&gt;
&lt;p&gt;Break/fix support remains an optional add-on. When production issues arise, you can get access to our Support team through this service and troubleshoot with experts who contribute to Ubuntu every day, who‚Äôve seen similar problems before and know how to resolve them quickly.&lt;/p&gt;
&lt;p&gt;The scope of the Legacy add-on itself is unchanged, but the commitment is longer, giving users additional years to manage transition timelines and maintain compliance.¬†&lt;/p&gt;
&lt;p&gt;This updated coverage applies from Ubuntu 14.04 LTS onward. With this extension, Ubuntu 14.04 LTS is now supported until April 2029, a full 15 years after its debut.&lt;/p&gt;
&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="" height="511" loading="lazy" sizes="(min-width: 1071px) 1071px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1071/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F75eb%2Fimage.png" srcset="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F75eb%2Fimage.png 460w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F75eb%2Fimage.png 620w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F75eb%2Fimage.png 1036w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1681/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F75eb%2Fimage.png 1681w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1920/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F75eb%2Fimage.png 1920w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1071/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F75eb%2Fimage.png 1071w" width="1071"/&gt;&lt;/figure&gt;
&lt;p&gt;By committing to a 15-year lifecycle, Canonical gives users:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Realistic timelines for planning and executing major migrations&lt;/li&gt;
&lt;li&gt;Continuous security and compliance coverage for long-lived systems&lt;/li&gt;
&lt;li&gt;Flexibility to modernize infrastructure strategically rather than reactively&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Infrastructure is complicated, and upgrades carry real costs and risks. This expansion acknowledges those realities and gives you the support duration your deployments actually require.&lt;/p&gt;
&lt;h1 class="wp-block-heading"&gt;A simple path to extended coverage&lt;/h1&gt;
&lt;p&gt;Current Ubuntu Pro subscriptions will continue uninterrupted. No re-enrollment, no reinstallation, no surprise migration projects.&lt;/p&gt;
&lt;p&gt;The Legacy add-on is available after the first 10 years of coverage (standard support plus ESM), priced at a 50% premium over standard Ubuntu Pro. This applies whether you‚Äôre approaching that milestone with 16.04 LTS or already using the Legacy add-on with 14.04 LTS.&lt;/p&gt;
&lt;p&gt;To activate coverage beyond ESM, &lt;a href="https://ubuntu.com/contact-us/form?product=pro"&gt;contact Canonical Sales&lt;/a&gt; or reach out to your account manager.¬†¬†&lt;/p&gt;
&lt;p&gt;For more information about the Legacy add-on, visit our &lt;a href="https://ubuntu.com/pro"&gt;Ubuntu Pro page&lt;/a&gt;.¬†¬†&lt;/p&gt;
&lt;p&gt;Learn more:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://ubuntu.com/pro"&gt;Ubuntu Pro¬†&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ubuntu.com/support"&gt;Canonical Support&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://canonical.com/case-study/sbi-bits"&gt;Read how SBI BITS ensures business continuity with Canonical‚Äôs support&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;/p&gt;
</content:encoded><author>Lidia Luna Puerta (Lidia Luna Puerta)</author><category>LTS</category><category>Support</category><category>Ubuntu Pro</category><pubDate>Thu, 13 Nov 2025 09:11:04 +0000</pubDate></item><item><title>Canonical releases FIPS-enabled Kubernetes</title><link>https://ubuntu.com//blog/canonical-releases-fips-enabled-kubernetes</link><description>&lt;p&gt;Today at KubeCon North America, Canonical, the publisher of Ubuntu, released support to enable FIPS mode in its Kubernetes distribution, providing everything needed to create and manage a scalable cluster suitable for high-security and Federal deployments.&lt;/p&gt;
</description><content:encoded>
&lt;p&gt;&lt;em&gt;Deploy a FedRAMP-ready Kubernetes cluster and application suite, with FIPS 140-3 crypto and DISA-STIG hardening&lt;/em&gt;,&lt;/p&gt;
&lt;p&gt;Today at KubeCon North America, Canonical, the publisher of Ubuntu, released support to enable FIPS mode in its Kubernetes distribution, providing everything needed to create and manage a scalable cluster suitable for high-security or Federal deployments.¬†As of version 1.34, Canonical Kubernetes is available with a built-in FIPS 140-3 capability that uses certified cryptographic modules. Your deployment with this FIPS capability can be easily hardened to DISA-STIG standards using comprehensive documentation when deployed as a snap package. &lt;/p&gt;
&lt;p&gt;KubeCon attendees in Atlanta can learn more about FIPS-enabled Canonical Kubernetes at booth 821.¬†&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;What is Canonical Kubernetes?&lt;/h2&gt;
&lt;p&gt;Canonical Kubernetes is a performant, lightweight, and securely designed CNCF-conformant distribution of Kubernetes. It provides everything needed for a fully functioning cluster, including a container runtime, a CNI, DNS services, an ingress gateway, metrics server, and more. New versions of Canonical Kubernetes ship within a week of the upstream release, and Long Term Support (LTS) versions (which are released every 2 years) are fully supported and security maintained by Canonical for up to 12 years. Long Term Support for Ubuntu and FIPS-enabled Canonical Kubernetes is offered through an &lt;a href="http://ubuntu.com/pro"&gt;Ubuntu Pro subscription.&lt;/a&gt; Canonical‚Äôs FIPS 140-3 compliant Kubernetes is also available as part of the NVIDIA AI Factory for Government reference design.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Gain stability with the option to upgrade for new features&lt;/h2&gt;
&lt;p&gt;Canonical is the first software provider to offer 12 years of support for Kubernetes, which is far beyond the support window offered by upstream CNCF and other vendors. Upstream Kubernetes is typically maintained and &lt;a href="https://kubernetes.io/releases/patch-releases/#support-period"&gt;supported for about 14 months&lt;/a&gt; by the Kubernetes community, with 3 releases per year. In comparison, Canonical  maintains an LTS release every 2 years, in line with the Ubuntu LTS release cadence. &lt;/p&gt;
&lt;p&gt;Traditionally, Kubernetes clusters must be &lt;a href="https://kubernetes.io/releases/version-skew-policy/"&gt;upgraded one version at a time&lt;/a&gt;. However, Canonical‚Äôs ‚Äúinterim‚Äù versions will be supported for 1 year past the next LTS release, allowing  customers to upgrade within 1 year of the next LTS release, without downtime, all while knowing their cluster is fully covered by security maintenance.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Get reliable security maintenance&lt;/h2&gt;
&lt;p&gt;Each component of the Kubernetes stack is backed by Canonical‚Äôs CVE patching service. Our dedicated security team triages all relevant vulnerabilities and backports upstream fixes to the currently supported software versions, ensuring a completely stable base without breaking existing deployments.¬†&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Comply with FedRAMP requirements&lt;/h2&gt;
&lt;p&gt;Canonical has been publishing FIPS-certified cryptographic modules for Ubuntu since 2016. These modules are vital for customers across the Federal sector and for on-premises and public clouds, powering a wide range of FedRAMP deployments. With the availability of Canonical Kubernetes and its built-in FIPS 140-3 mode using certified cryptographic modules, customers will have a faster and more direct route to meet their FedRAMP requirements.&lt;/p&gt;
&lt;p&gt;FIPS 140-3 functionality requires Kubernetes to be deployed on top of a FIPS-enabled Ubuntu LTS host Operating System. Canonical Kubernetes enables Kubernetes DISA-STIG, and allows you to deploy onto a host OS hardened to DISA-STIG guidelines using the Ubuntu Security Guide (USG) tool. What‚Äôs more, applicable STIG controls can be applied to enable hardened containers, along with embedded FIPS cryptographic libraries. Ubuntu STIG hardening has been extensively tested and deployed across the Federal landscape, making it a proven route to meeting FedRAMP security standards. &lt;/p&gt;
&lt;p&gt;FIPS modules and STIG hardening are available with an &lt;a href="https://ubuntu.com/pro"&gt;Ubuntu Pro&lt;/a&gt; subscription. Ubuntu Pro subscriptions apply on a per-machine basis, which means that any containerized application running on a Pro-enabled host machine is also included within Pro when the Pro token is enabled.¬†&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Visit us at our booth 821 at &lt;/strong&gt;&lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/"&gt;&lt;strong&gt;KubeCon North America&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; on November 11-13, 2025 for an in-person conversation about how Canonical Kubernetes powers FedRAMP compliant deployments.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;About Canonical&lt;/h2&gt;
&lt;p&gt;Canonical, the publisher of Ubuntu, provides open source security, support ,and services. Our portfolio covers critical systems, from the smallest devices to the largest clouds, from the kernel to containers, from databases to AI. With customers that include top tech brands, emerging startups, governments and home users, Canonical delivers trusted open source for everyone.¬†&lt;/p&gt;
&lt;p&gt;Learn more at &lt;a href="https://canonical.com/"&gt;https://canonical.com/&lt;/a&gt;¬†&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Further Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://ubuntu.com/engage/secure-federal-cloud-native-stack-k8s"&gt;Register for an expert webinar discussing FIPS and STIG compliant Canonical Kubernetes &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://documentation.ubuntu.com/canonical-kubernetes/latest/"&gt;Canonical Kubernetes Documentation&lt;/a&gt;¬†&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ubuntu.com/blog/canonical-delivers-kubernetes-platform-and-open-source-security-with-nvidia-enterprise-ai-factory-validated-design"&gt;Canonical delivers Kubernetes platform and open-source security with NVIDIA Enterprise AI Factory validated design &lt;/a&gt;¬†&lt;/li&gt;
&lt;li&gt;&lt;a href="https://documentation.ubuntu.com/canonical-kubernetes/latest/snap/howto/install/disa-stig/"&gt;DISA-STIG for Canonical Kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content:encoded><author>Canonical (Canonical)</author><category>canonical kubernetes</category><category>DISA STIG</category><category>FIPS</category><category>FIPS certification</category><category>Hardening</category><category>kubernetes</category><category>Ubuntu Pro</category><pubDate>Tue, 11 Nov 2025 14:45:33 +0000</pubDate></item><item><title>Canonical announces optimized Ubuntu images for Google Cloud‚Äôs Axion N4A Virtual Machines</title><link>https://ubuntu.com//blog/canonical-announces-optimized-ubuntu-images-for-google-clouds-axion-n4a-virtual-machines</link><description>&lt;p&gt;This new release brings the stability and security of Ubuntu to Axion-based N4A virtual machines on Google Compute Engine. November 6, 2025 ‚Äì Today Canonical, the publishers of Ubuntu, and Google Cloud announced the immediate availability of optimized Ubuntu images for the new Axion-based N4A virtual machines (VMs) on Google Compute Engine. This collaboration brings [&amp;hellip;]&lt;/p&gt;
</description><content:encoded>
&lt;p&gt;This new release brings the stability and security of Ubuntu to Axion-based N4A virtual machines on Google Compute Engine.&lt;/p&gt;
&lt;p&gt;November 6, 2025 ‚Äì Today Canonical, the publishers of Ubuntu, and Google Cloud announced the immediate availability of optimized Ubuntu images for the new Axion-based N4A virtual machines (VMs) on Google Compute Engine. This collaboration brings the stability, security, and expansive ecosystem of Ubuntu, the world‚Äôs most popular cloud operating system, to Google Cloud‚Äôs most cost-effective N-series offering, enabling enterprises to maximize the total cost of ownership (TCO) for a wide range of general-purpose workloads.&lt;/p&gt;
&lt;p&gt;The new N4A VMs are powered by Google‚Äôs custom-designed Axion ARM-based CPUs and offer up to 105% better price performance and 80% better performance-per-watt than comparable, current-generation x86-based VMs. By integrating optimized Ubuntu images at launch, Canonical helps ensure developers and operators can immediately take advantage of this breakthrough efficiency for demanding workloads.&lt;/p&gt;
&lt;p&gt;Canonical has long supported ARM infrastructure, helping to ensure that Ubuntu provides a consistent, reliable, and secure experience across heterogeneous computing environments. Our deep experience in solving the challenges of mixed x86 and ARM deployments allows us to bring a robust and fully optimized operating system to the N4A series from day one.&lt;/p&gt;
&lt;p&gt;The availability of optimized Ubuntu on N4A ensures developers can use the familiar packages and libraries of the latest Long-Term Support (LTS) releases, guaranteeing longevity and simplifying migration. This is crucial for businesses looking to adopt N4A‚Äôs cost savings without compromising on operational consistency across Google Cloud‚Äôs Compute Engine, Google Kubernetes Engine (GKE), and other services.&lt;/p&gt;
&lt;h3 class="wp-block-heading"&gt;Seamless integration and reliability from Day One&lt;/h3&gt;
&lt;p&gt;These optimized Ubuntu images are backed by rigorous testing to help ensure enterprise-grade stability and compatibility with Google Cloud‚Äôs core features.&lt;/p&gt;
&lt;p&gt;Canonical and Google Cloud have executed thorough validation across the entire image lifecycle, confirming that Ubuntu on N4A performs exceptionally well with Google Cloud services and VMs. This extensive testing includes validation of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Secure Boot integrity:&lt;/strong&gt; Full compatibility and successful execution of google-secure-boot, ensuring the highest levels of system integrity from the moment of launch.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Initialization and configuration:&lt;/strong&gt; Robust confirmation of the cloud-init configuration process, including network, user, and password authentication (cloud-init-password-auth-test), guaranteeing reliable deployment and user setup.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lifecycle management:&lt;/strong&gt; Successful execution of startup and shutdown scripts (google-startup, google-shutdown-script, and their URL-based variants), critical for automated maintenance and application orchestration.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compute Engine feature compatibility:&lt;/strong&gt; Validation of core Google Cloud functionality, including accurate disk resizing (google-disk-size), and general system integration (google-general), helping to ensure that Ubuntu images behave predictably within the Compute Engine environment.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This comprehensive testing suite allows customers to deploy Ubuntu on N4A with total confidence.&lt;/p&gt;
&lt;h3 class="wp-block-heading"&gt;Getting started&lt;/h3&gt;
&lt;p&gt;To get started, simply select the N4A machine type and choose your preferred Ubuntu image when creating a VM in Google Cloud Compute Engine, or when configuring node settings in GKE.&lt;/p&gt;
&lt;p&gt;The optimized images are available now in the public preview regions for N4A (us-central1, us-east4, europe-west3, and europe-west4).&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;About Canonical¬†&lt;/h2&gt;
&lt;p&gt;Canonical, the publisher of Ubuntu, provides open source security, support and services. Our portfolio covers critical systems, from the smallest devices to the largest clouds, from the kernel to containers, from databases to AI. With customers that include top tech brands, emerging startups, governments and home users, Canonical delivers trusted open source for everyone. Learn more at&lt;a href="https://canonical.com/"&gt; https://canonical.com/&lt;/a&gt;¬†&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Read more&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://ubuntu.com/security/certifications/docs/usg"&gt;The Ubuntu Security Guide documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ubuntu.com/gcp"&gt;Ubuntu on Google Cloud&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content:encoded><author>Hugo Huang (Hugo Huang)</author><category>ARM</category><category>Axion</category><category>Google Cloud</category><category>Ubuntu</category><pubDate>Mon, 10 Nov 2025 22:08:10 +0000</pubDate></item><item><title>Generating accessible color palettes for design systems ‚Ä¶ inspired by APCA!</title><link>https://ubuntu.com//blog/generating-color-palettes-for-design-systems-inspired-by-apca</link><description>&lt;p&gt;This is the first of two blog posts about how we created the color palette for a new design system at Canonical. In this post I share my journey into perceptually uniform color spaces and perceptual contrast algorithms.&amp;nbsp; If you&amp;#8217;re already familiar with these concepts, skip to this section (or visit the Github repository) to [&amp;hellip;]&lt;/p&gt;
</description><content:encoded>
&lt;p&gt;This is the first of two blog posts about how we created the color palette for a new design system at Canonical. In this post I share my journey into perceptually uniform color spaces and perceptual contrast algorithms.¬†&lt;/p&gt;
&lt;p&gt;If you‚Äôre already familiar with these concepts, &lt;a href="#inspired-by-apca"&gt;skip to this&lt;strong&gt; &lt;/strong&gt;section&lt;/a&gt; (or &lt;a href="https://github.com/dgtlntv/perceptual-contrast-palette"&gt;visit the Github repository&lt;/a&gt;) to see how I reverse-engineered the Accessible Perceptual Contrast Algorithm (APCA) to generate perceptually contrasting color palettes. In the next post, I will share why we didn‚Äôt choose this solution and what we chose instead.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;How humans perceive color&lt;/h2&gt;
&lt;p&gt;I was nerd sniped by a colleague with this article by Matthew Str√∂m, ‚Äú&lt;a href="https://matthewstrom.com/writing/how-to-pick-the-least-wrong-colors/"&gt;How to pick the least wrong colors&lt;/a&gt;‚Äù about using &lt;a href="https://en.wikipedia.org/wiki/Color_appearance_model"&gt;perceptually uniform color spaces&lt;/a&gt; to pick data visualization colors. At that point, I hadn‚Äôt known about perceptually uniform color spaces like &lt;a href="https://bottosson.github.io/posts/oklab/"&gt;Oklab and Oklch&lt;/a&gt;.&lt;/p&gt;
&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="" height="768" loading="lazy" sizes="(min-width: 1920px) 1920px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_webp,q_auto,fl_sanitize,c_fill,w_1920/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F8edf%2Frgb-cube.webp" srcset="https://res.cloudinary.com/canonical/image/fetch/f_webp,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F8edf%2Frgb-cube.webp 460w, https://res.cloudinary.com/canonical/image/fetch/f_webp,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F8edf%2Frgb-cube.webp 620w, https://res.cloudinary.com/canonical/image/fetch/f_webp,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F8edf%2Frgb-cube.webp 1036w, https://res.cloudinary.com/canonical/image/fetch/f_webp,q_auto,fl_sanitize,c_fill,w_1681/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F8edf%2Frgb-cube.webp 1681w, https://res.cloudinary.com/canonical/image/fetch/f_webp,q_auto,fl_sanitize,c_fill,w_1920/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F8edf%2Frgb-cube.webp 1920w" width="1920"/&gt;&lt;/figure&gt;
&lt;p&gt;‚ÄúNormal‚Äù color spaces, such as RGB, are structured in such a way that machines can easily process colors. Therefore, RGB has very inhuman characteristics. If you imagine the color space as a geometric shape, for example, RGB would be a cube. The naive assumption would be that colors we perceive as similar are close to each other in this cube, right? However, this is not the case. Surprisingly, human color perception does not correspond to a perfect cube. Who would have thought?&lt;/p&gt;
&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="" height="768" loading="lazy" sizes="(min-width: 1920px) 1920px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_webp,q_auto,fl_sanitize,c_fill,w_1920/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Ff44a%2Foklch-3d-shape.webp" srcset="https://res.cloudinary.com/canonical/image/fetch/f_webp,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Ff44a%2Foklch-3d-shape.webp 460w, https://res.cloudinary.com/canonical/image/fetch/f_webp,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Ff44a%2Foklch-3d-shape.webp 620w, https://res.cloudinary.com/canonical/image/fetch/f_webp,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Ff44a%2Foklch-3d-shape.webp 1036w, https://res.cloudinary.com/canonical/image/fetch/f_webp,q_auto,fl_sanitize,c_fill,w_1681/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Ff44a%2Foklch-3d-shape.webp 1681w, https://res.cloudinary.com/canonical/image/fetch/f_webp,q_auto,fl_sanitize,c_fill,w_1920/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Ff44a%2Foklch-3d-shape.webp 1920w" width="1920"/&gt;&lt;/figure&gt;
&lt;p&gt;Perceptually uniform color spaces support human perception, not computers. While RGB‚Äôs are consistent in how a color displays on a monitor, PUCs are consistent with how we actually see the color. As a result, their 3D shapes are not perfect geometric shapes, such as the one from Oklch (pictured above). Shocker!&lt;/p&gt;
&lt;p&gt;This property of perceptually uniform color spaces, which aligns more closely with actual human color perception, holds enormous potential for UI design and the wider design spectrum. For example, it‚Äôs much easier to create color palettes in which the brightness of different colors appears more uniform in the same ‚Äúgradation‚Äù. This potential fascinated me, so I took a deeper dive into perceptually uniform color spaces and &lt;a href="https://www.handprint.com/HP/WCL/color1.html"&gt;human perception of colors and contrasts in general&lt;/a&gt;.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;How humans perceive contrast&lt;/h2&gt;
&lt;p&gt;One of the things I learned during my research was the shortcomings of the contrast algorithm currently recommended in the &lt;a href="https://www.w3.org/WAI/WCAG21/Understanding/contrast-minimum.html"&gt;WCAG guidelines&lt;/a&gt;, a recommendation based on the &lt;a href="https://cdn.standards.iteh.ai/samples/16875/4e1bf8a0a2d34083a45e301f1e30884c/ISO-9241-3-1992.pdf"&gt;ISO-9241-3&lt;/a&gt; standard. The author of APCA, myndex, does an excellent job of documenting the &lt;a href="https://github.com/Myndex/SAPC-APCA/discussions/30"&gt;shortcomings of WCAG&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Essentially, WCAG produces both false positives and false negatives when it evaluates contrast between two colors. Meaning, WCAG approvals aren‚Äôt necessarily accessible because some combinations with high contrast fail and some with low contrast pass. APCA is a contrast algorithm that is more closely aligned with human contrast perception and is therefore much better at evaluating contrast than WCAG.&lt;/p&gt;
&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="" height="740" loading="lazy" sizes="(min-width: 588px) 588px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_webp,q_auto,fl_sanitize,c_fill,w_588/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fe6c3%2Fapca-vs-wcag.webp" srcset="https://res.cloudinary.com/canonical/image/fetch/f_webp,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fe6c3%2Fapca-vs-wcag.webp 460w, https://res.cloudinary.com/canonical/image/fetch/f_webp,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fe6c3%2Fapca-vs-wcag.webp 620w, https://res.cloudinary.com/canonical/image/fetch/f_webp,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fe6c3%2Fapca-vs-wcag.webp 1036w, https://res.cloudinary.com/canonical/image/fetch/f_webp,q_auto,fl_sanitize,c_fill,w_588/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fe6c3%2Fapca-vs-wcag.webp 588w" width="588"/&gt;&lt;/figure&gt;
&lt;p&gt;At that time, I also was going to start creating a new color palette for Canonical‚Äôs design system. So I expanded my research to include how different color spaces and contrast algorithms can be used to create color palettes. In this context, I also read another article by Matthew Str√∂m on color generation, titled ‚Äú&lt;a href="https://matthewstrom.com/writing/generating-color-palettes/"&gt;How to Generate Color Palettes for Design Systems&lt;/a&gt;.‚Äù This article was one of the most important sources of inspiration for my further work and this blog post; In particular, Str√∂m‚Äôs principle of using contrasts to determine color gradations, which made me wonder whether it could be developed further.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Generating color palettes for design systems‚Ä¶&lt;/h2&gt;
&lt;p&gt;To support my work creating a new color palette for Canonical‚Äôs design system, I also researched how color spaces and contrast algorithms can be used to make color palettes. In his article Str√∂m explores combining contrast algorithms and perceptually uniform color spaces to generate color palettes.&lt;/p&gt;
&lt;p&gt;Contrast is one of the most important aspects of working with color in user interfaces (and other media). There must be sufficient contrast between two colors so that people can distinguish between them.¬†Str√∂m believes that contrast should determine the gradation between colors in a palette. Applied to Str√∂m‚Äôs palette, this means that every pair of colors with a distance of 500 will have the WCAG mandated contrast ratio of 4.5:1.&lt;/p&gt;
&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="" height="768" loading="lazy" sizes="(min-width: 1920px) 1920px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_webp,q_auto,fl_sanitize,c_fill,w_1920/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F53b7%2Fapca-vs-wcag-algorithm.webp" srcset="https://res.cloudinary.com/canonical/image/fetch/f_webp,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F53b7%2Fapca-vs-wcag-algorithm.webp 460w, https://res.cloudinary.com/canonical/image/fetch/f_webp,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F53b7%2Fapca-vs-wcag-algorithm.webp 620w, https://res.cloudinary.com/canonical/image/fetch/f_webp,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F53b7%2Fapca-vs-wcag-algorithm.webp 1036w, https://res.cloudinary.com/canonical/image/fetch/f_webp,q_auto,fl_sanitize,c_fill,w_1681/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F53b7%2Fapca-vs-wcag-algorithm.webp 1681w, https://res.cloudinary.com/canonical/image/fetch/f_webp,q_auto,fl_sanitize,c_fill,w_1920/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F53b7%2Fapca-vs-wcag-algorithm.webp 1920w" width="1920"/&gt;&lt;/figure&gt;
&lt;p&gt;In a color palette where the contrast between two shades is consistent, it‚Äôs easy to choose accessible color pairs. Choose any two shades in the palette that are a certain distance apart, and you‚Äôve got an accessible color pair. You no longer need to manually check all color combinations in your user interface. In an internal survey of designers at Canonical, we found that selecting accessible color pairs is an important concern for designers. Therefore, a color palette in which it is easy to select accessible color pairs seemed ideal for us.&lt;/p&gt;
&lt;h2 class="wp-block-heading" id="inspired-by-apca"&gt;‚Ä¶ inspired by APCA!&lt;/h2&gt;
&lt;p&gt;Matthew Str√∂m used the WCAG algorithm in his blog post to good effect, but as mentioned earlier, the WCAG contrast algorithm has its drawbacks. I was curious to see if it would be possible to follow the same principle (basing color palette gradation on contrast) but replace the WCAG algorithm with a perceptual contrast algorithm; in fact, even Str√∂m mentioned in his article that it would be an interesting experiment. I found the idea of trying it with perceptual contrast exciting and began to investigate its feasibility.&lt;/p&gt;
&lt;p&gt;So began my journey to create a color palette inspired by APCA contrast algorithm principles.&lt;/p&gt;
&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="" height="1854" loading="lazy" sizes="(min-width: 1224px) 1224px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1224/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F2b63%2Fapca-math.png" srcset="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F2b63%2Fapca-math.png 460w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F2b63%2Fapca-math.png 620w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F2b63%2Fapca-math.png 1036w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1681/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F2b63%2Fapca-math.png 1681w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1920/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F2b63%2Fapca-math.png 1920w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1224/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F2b63%2Fapca-math.png 1224w" width="1224"/&gt;&lt;figcaption class="wp-element-caption"&gt;The APCA formula&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;First, I had to create a reverse perceptual contrast algorithm. APCA takes two colors and outputs a number between -108 and 106 (where 0 is low contrast and the extreme values are high contrast) to indicate how contrasting the color pair is. Reversing the algorithm means restructuring it so that we can specify a color and a desired contrast ratio to the algorithm, and it returns a color that meets those criteria. Due to its complexity, reversing a perceptual contrast algorithm was much harder than reversing the WCAG algorithm.&lt;/p&gt;
&lt;p&gt;I knew that the &lt;a href="https://github.com/Myndex/apca-w3"&gt;apca-w3 package&lt;/a&gt; already had a ‚Äúreverse APCA‚Äù function. Originally, I thought I would have to go beyond the capabilities of this function (it can only perform the reversal with grayscale colors). As a side project during a climbing trip with friends, I therefore tried to sketch out the reversal of the APCA algorithm on a napkin myself (with the help of a physicist friend, as I‚Äôm not that good at math myself).&lt;/p&gt;
&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="" height="1280" loading="lazy" sizes="(min-width: 1984px) 1984px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1984/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F96f7%2Fnapkin.png" srcset="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F96f7%2Fnapkin.png 460w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F96f7%2Fnapkin.png 620w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F96f7%2Fnapkin.png 1036w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1681/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F96f7%2Fnapkin.png 1681w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1920/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F96f7%2Fnapkin.png 1920w" width="1984"/&gt;&lt;/figure&gt;
&lt;p&gt;Much of the APCA algorithm‚Äôs complexity stems from the fact that there are four possible cases and the equation looks different depending on the case. The four cases we need to consider for our inverse algorithm are the polarity (is the text lighter than the background) and which of the two variables we want to solve for (text or background).&lt;/p&gt;
&lt;p&gt;So for the inverse algorithm, we need to consider four cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Case 1:&lt;/strong&gt; Light text on a dark background, solving for text&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Case 2:&lt;/strong&gt; Dark text on light background, solving for text&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Case 3:&lt;/strong&gt; Light text on dark background, solving for background&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Case 4:&lt;/strong&gt; Dark text on light background, solving for background&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I will show my process for the first case. The process for the other cases is basically the same, but different substitutions and signs must be used depending on the case.&lt;/p&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity"/&gt;
&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="" height="1742" loading="lazy" sizes="(min-width: 1020px) 1020px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1020/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fe20d%2Freversal.png" srcset="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fe20d%2Freversal.png 460w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fe20d%2Freversal.png 620w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fe20d%2Freversal.png 1036w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1681/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fe20d%2Freversal.png 1681w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1920/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fe20d%2Freversal.png 1920w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1020/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fe20d%2Freversal.png 1020w" width="1020"/&gt;&lt;/figure&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity"/&gt;
&lt;p&gt;Repeating the same process for the other cases we get the following 4 equations for our 4 cases:&lt;/p&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity"/&gt;
&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="" height="780" loading="lazy" sizes="(min-width: 895px) 895px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_895/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fa47d%2Fall-cases.png" srcset="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fa47d%2Fall-cases.png 460w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fa47d%2Fall-cases.png 620w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fa47d%2Fall-cases.png 1036w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1681/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fa47d%2Fall-cases.png 1681w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_895/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fa47d%2Fall-cases.png 895w" width="895"/&gt;&lt;/figure&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity"/&gt;
&lt;p&gt;Finally, in APCA, all input Y values must be clamped, and the Y value returned as the output of the inverse function must be unclamped. The two functions for clamping and unclamping Y are as follows:&lt;/p&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity"/&gt;
&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="" height="1024" loading="lazy" sizes="(min-width: 1260px) 1260px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1260/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fb34a%2Funclamp.png" srcset="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fb34a%2Funclamp.png 460w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fb34a%2Funclamp.png 620w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fb34a%2Funclamp.png 1036w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1681/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fb34a%2Funclamp.png 1681w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1920/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fb34a%2Funclamp.png 1920w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1260/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fb34a%2Funclamp.png 1260w" width="1260"/&gt;&lt;/figure&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity"/&gt;
&lt;p&gt;After completing all the scary calculations, I was ready to translate it all into code. In doing so, I realized that I had only determined the required Y component (in the XYZ color space) of a color with the correct contrast value distance, but not a full color. So, the formula is essentially capable of determining a grayscale color that has the correct contrast distance to the input color ‚Äì exactly what the existing reverse APCA function can do üòÖ.&lt;/p&gt;
&lt;p&gt;I took another look at Str√∂m‚Äôs article and realized that the Y component was actually all I needed to generate the palettes. So I could have just used the function available in the apca-w3 package‚Ä¶ So if you are considering a similar project, you can save yourself (and your physicist friends) the napkin calculations and either use the existing &lt;code&gt;reverseAPCA()&lt;/code&gt; function in the apca-w3 package or my code below.&lt;/p&gt;
&lt;p&gt;I still thought it was a good learning experience to reverse it myself, and since apca-w3 is not completely open source (it doesn‚Äôt have a standard open source license), I also thought it would be nice to have an implementation of the reverse algorithm with a truly open source license. I‚Äôm not sure if what I did is compatible with the &lt;a href="https://git.apcacontrast.com/documentation/LICENSE.html"&gt;APCA trademark license&lt;/a&gt;, so I‚Äôll refrain from claiming that my result is APCA-compliant. The code for my inverse perceptual contrast finder, inspired by APCA algorithm principles, is as follows:&lt;/p&gt;
&lt;script src="https://ubuntu.com/static/js/dist/prism.js?v=a763671"&gt;&lt;/script&gt;
&lt;pre class="language-javascript" style="max-height: 600px; overflow: auto;"&gt;&lt;code class="language-javascript"&gt;/**
 * Constants used in perceptual contrast calculations
 * Inspired by the formula found at https://github.com/Myndex/apca-w3/blob/c012257167d822f91bc417120bdb82e1b854b4a4/src/apca-w3.js#L146
 */

const PERCEPTUAL_CONTRAST_CONSTANTS: {
    BLACK_THRESHOLD: number
    BLACK_CLAMP: number
    OFFSET: number
    SCALE: number
    MAGIC_OFFSET_IN: number
    MAGIC_OFFSET_OUT: number
    MAGIC_FACTOR: number
    MAGIC_EXPONENT: number
    MACIG_FACTOR_INVERSE: number
} = {
    BLACK_THRESHOLD: 0.022,
    BLACK_CLAMP: 1.414,
    OFFSET: 0.027,
    SCALE: 1.14,
    MAGIC_OFFSET_IN: 0.0387393816571401,
    MAGIC_OFFSET_OUT: 0.312865795870758,
    MAGIC_FACTOR: 1.9468554433171,
    MAGIC_EXPONENT: 0.283343396420869 / 1.414,
    MACIG_FACTOR_INVERSE: 1 / 1.9468554433171,
}

/**
 * Removes clamping from near-black colors to restore original values
 * Inspired by the formula found at: https://github.com/Myndex/apca-w3/blob/c012257167d822f91bc417120bdb82e1b854b4a4/src/apca-w3.js#L403
 * @param y - The clamped luminance value to be unclamped
 * @returns The unclamped luminance value
 */
function unclampY(y: number): number {
    return y &amp;gt; PERCEPTUAL_CONTRAST_CONSTANTS.BLACK_THRESHOLD
        ? y
        : Math.pow(
              (y + PERCEPTUAL_CONTRAST_CONSTANTS.MAGIC_OFFSET_IN) *
                  PERCEPTUAL_CONTRAST_CONSTANTS.MAGIC_FACTOR,
              PERCEPTUAL_CONTRAST_CONSTANTS.MAGIC_EXPONENT
          ) *
              PERCEPTUAL_CONTRAST_CONSTANTS.MACIG_FACTOR_INVERSE -
              PERCEPTUAL_CONTRAST_CONSTANTS.MAGIC_OFFSET_OUT
}

/**
 * Applies clamping to near-black colors to prevent contrast calculation issues
 * Inspired by the formula found at: https://github.com/Myndex/apca-w3/blob/c012257167d822f91bc417120bdb82e1b854b4a4/src/apca-w3.js#L381
 * @param y - The luminance value to be clamped
 * @returns The clamped luminance value
 */
function clampY(y: number): number {
    return y &amp;gt;= PERCEPTUAL_CONTRAST_CONSTANTS.BLACK_THRESHOLD
        ? y
        : y +
              Math.pow(
                  PERCEPTUAL_CONTRAST_CONSTANTS.BLACK_THRESHOLD - y,
                  PERCEPTUAL_CONTRAST_CONSTANTS.BLACK_CLAMP
              )
}

/**
 * Reverses perceptual contrast calculations to find a matching luminance
 * Inspired by the formula found at: https://github.com/Myndex/apca-w3/blob/c012257167d822f91bc417120bdb82e1b854b4a4/images/APCAw3_0.1.17_APCA0.0.98G.svg
 * @param contrast - Target contrast value (between 5 and 106.04066)
 * @param y - Known luminance value (between 0 and 1)
 * @param bgIsDarker - Whether the background is darker than the text
 * @param lookingFor - What we're solving for: "txt" (text color) or "bg" (background color)
 * @returns The calculated luminance value, or false if no valid solution exists
 */
export function reversePerceptualContrast(
    contrast: number = 75, // Default contrast of 75
    y: number = 1, // Default luminance of 1
    bgIsDarker: boolean = false, // Default assumes background is lighter
    lookingFor: "txt" | "bg" = "txt" // Default solves for text color
): number | false {
    contrast = Math.abs(contrast)
    let output: number | undefined

    if (!(y &amp;gt; 0 &amp;amp;&amp;amp; y &amp;lt;= 1)) {
        console.log("y is not a valid value (y &amp;gt; 0 &amp;amp;&amp;amp; y &amp;lt;= 1)")
        return false
    }

    if (!(contrast &amp;gt;= 5 &amp;amp;&amp;amp; contrast &amp;lt;= 106.04066)) {
        console.log(
            "contrast is not a valid value (contrast &amp;gt;= 5 &amp;amp;&amp;amp; contrast &amp;lt;= 106.04066)"
        )
        return false
    }

    // Apply clamping to input luminance
    y = clampY(y)

    // Calculate output luminance based on what we're looking for and background darkness
    // You could do these calculations here more DRY, but I find that it is easier to
    // understand the derivation from the original calculation with the if statements.

    if (lookingFor === "txt") {
        if (bgIsDarker) {
            // For light text on dark background
            output =
                (y ** 0.65 -
                    (-contrast / 100 - PERCEPTUAL_CONTRAST_CONSTANTS.OFFSET) *
                        (1 / PERCEPTUAL_CONTRAST_CONSTANTS.SCALE)) **
                (1 / 0.62)
        } else if (!bgIsDarker) {
            // For dark text on light background
            output =
                (y ** 0.56 -
                    (contrast / 100 + PERCEPTUAL_CONTRAST_CONSTANTS.OFFSET) *
                        (1 / PERCEPTUAL_CONTRAST_CONSTANTS.SCALE)) **
                (1 / 0.57)
        }
    } else if (lookingFor === "bg") {
        if (bgIsDarker) {
            // For dark background with light text
            output =
                (y ** 0.62 +
                    (-contrast / 100 - PERCEPTUAL_CONTRAST_CONSTANTS.OFFSET) *
                        (1 / PERCEPTUAL_CONTRAST_CONSTANTS.SCALE)) **
                (1 / 0.65)
        } else if (!bgIsDarker) {
            // For light background with dark text
            output =
                (y ** 0.57 +
                    (contrast / 100 + PERCEPTUAL_CONTRAST_CONSTANTS.OFFSET) *
                        (1 / PERCEPTUAL_CONTRAST_CONSTANTS.SCALE)) **
                (1 / 0.56)
        }
    }

    // Unclamp the output value if valid
    if (output !== undefined &amp;amp;&amp;amp; !isNaN(output)) {
        output = unclampY(output)
    }

    // Validate final output
    if (
        output === undefined ||
        isNaN(output) ||
        !(output &amp;gt; 0 &amp;amp;&amp;amp; output &amp;lt;= 1)
    ) {
        console.log("A color with the specifications does not exist")
        return false
    } else {
        return output
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After performing the perceptual contrast inversion, all I had to do was combine my code for reverse perceptual contrast with Str√∂m's code:&lt;/p&gt;
&lt;script src="https://ubuntu.com/static/js/dist/prism.js?v=a763671"&gt;&lt;/script&gt;
&lt;pre class="language-javascript" style="max-height: 600px; overflow: auto;"&gt;&lt;code class="language-javascript"&gt;import Color from "colorjs.io"

/**
 * Converts OKHSl color to sRGB array
 * @param {OkHSL} hsl - Array containing [hue, saturation, lightness]
 *   hue: number (0-360) - The hue angle in degrees
 *   saturation: number (0-1) - The saturation value
 *   lightness: number (0-1) - The lightness value
 * @returns {[number, number, number]} sRGB array [r, g, b] in 0-255 range
 */
export function okhslToSrgb(
    hsl: [number, number, number],
): [number, number, number] {
    // Create new color in OKHSl space
    let c = new Color("okhsl", hsl)
    // Convert to sRGB color space
    c = c.to("srgb")

    return [c.srgb[0] * 255, c.srgb[1] * 255, c.srgb[2] * 255]
}

/**
 * Converts Y (luminance) value to OKHSL lightness
 * Inspired by the formula found at https://github.com/Myndex/apca-w3/blob/c012257167d822f91bc417120bdb82e1b854b4a4/src/apca-w3.js#L418
 * @param {number} y - Linear luminance value (0-1)
 * @returns {number} OKHSL lightness value (0-1)
 */
export function yToOkhslLightness(y: number): number {
    const srgbComponent = y ** (1 / 2.4)
    const c = new Color("srgb", [srgbComponent, srgbComponent, srgbComponent])
    return c.okhsl[2]
}

/**
 * Color scale object with hex color values keyed by scale number
 */
interface ColorScale {
    [step: number]: [number, number, number]
}

/**
 * Compensates for the Bezold-Br√ºcke effect where colors appear more purplish in shadows
 * and more yellowish in highlights by shifting the hue up to 5 degrees
 * Derived from https://mattstromawn.com/writing/generating-color-palettes/#putting-it-all-together%3A-all-the-code-you-need
 * Copyright (c) 2025 Matthew Str√∂m-Awn
 * Licensed under MIT. See LICENSE file.
 * @param step - Scale step value (0-1000)
 * @param baseHue - Starting hue in degrees (0-360)
 * @returns Adjusted hue value
 * @throws If parameters are invalid
 */
function computeHue(step: number, baseHue: number): number {
    // Normalize step from 0-1000 range to 0-1
    const normalizedStep = step / 1000

    // Validate normalizedStep is between 0 and 1
    if (normalizedStep &amp;lt; 0 || normalizedStep &amp;gt; 1) {
        throw new Error("step must produce a normalized value between 0 and 1")
    }

    // Validate baseHue is between 0 and 360
    if (baseHue &amp;lt; 0 || baseHue &amp;gt; 360) {
        throw new Error("baseHue must be a number between 0 and 360")
    }

    if (baseHue === 0) {
        return baseHue
    }

    return baseHue + 5 * (1 - normalizedStep)
}

/**
 * Creates a parabolic function for chroma/saturation that peaks at middle values
 * This ensures colors are most vibrant in the middle of the scale while being
 * more subtle at the extremes
 * Derived from https://mattstromawn.com/writing/generating-color-palettes/#putting-it-all-together%3A-all-the-code-you-need
 * Copyright (c) 2025 Matthew Str√∂m-Awn
 * Licensed under MIT. See LICENSE file.
 * @param step - Scale step value (0-1000)
 * @param minChroma - Minimum chroma/saturation value (0-1)
 * @param maxChroma - Maximum chroma/saturation value (0-1)
 * @returns Calculated chroma value
 * @throws If parameters are invalid
 */
function computeChroma(
    step: number,
    minChroma: number,
    maxChroma: number,
): number {
    const normalizedStep = step / 1000

    // Validate normalizedStep is between 0 and 1
    if (normalizedStep &amp;lt; 0 || normalizedStep &amp;gt; 1) {
        throw new Error("step must produce a normalized value between 0 and 1")
    }

    // Validate chroma values are between 0 and 1 and properly ordered
    if (minChroma &amp;lt; 0 || minChroma &amp;gt; 1 || maxChroma &amp;lt; 0 || maxChroma &amp;gt; 1) {
        throw new Error("Chroma values must be numbers between 0 and 1")
    }
    if (minChroma &amp;gt; maxChroma) {
        throw new Error("minChroma must be less than or equal to maxChroma")
    }

    const chromaDifference = maxChroma - minChroma
    return (
        -4 * chromaDifference * Math.pow(normalizedStep, 2) +
        4 * chromaDifference * normalizedStep +
        minChroma
    )
}

/**
 * Computes OKHSL lightness from a target contrast step using perceptual contrast
 * Derived from https://mattstromawn.com/writing/generating-color-palettes/#putting-it-all-together%3A-all-the-code-you-need
 * Copyright (c) 2025 Matthew Str√∂m-Awn
 * Licensed under MIT. See LICENSE file.
 * @param step - Scale step value (0-1000)
 * @returns OKHSL lightness value (0-1)
 * @throws If target luminance cannot be calculated
 */
function computeLightness(step: number): number {
    // Clip values below minimum threshold to full lightness (white)
    if (step &amp;lt; 50) {
        return 1
    }

    // Rescale 50-999 to perceptual contrast's 5-106.04066 range
    const perceptualContrast = 5 + ((step - 50) * (106.04066 - 5)) / (1000 - 50)

    const targetLuminance = reversePerceptualContrast(
        perceptualContrast,
        1,
        false,
        "txt",
    )

    if (targetLuminance === false) {
        throw new Error(
            `Problem calculating the target luminance for step ${step}`,
        )
    }

    return yToOkhslLightness(targetLuminance)
}

/**
 * Options for generating a color scale
 */
export interface GenerateColorScaleOptions {
    /** Base hue in degrees (0-360) */
    baseHue: number
    /** Minimum chroma/saturation (0-1) */
    minChroma: number
    /** Maximum chroma/saturation (0-1) */
    maxChroma: number
    /** Array of scale values to generate (integer values between 0-1000) */
    steps: number[]
}

/**
 * Generates a complete color scale with accessible contrast levels
 * @param options - Configuration object for color scale generation
 * @returns Scale object with color srgb values keyed by scale number
 */
export function generateColorScale(
    options: GenerateColorScaleOptions,
): ColorScale {
    const { baseHue, minChroma, maxChroma, steps } = options

    if (baseHue &amp;lt; 0 || baseHue &amp;gt; 360) {
        throw new Error("baseHue must be a number between 0 and 360")
    }

    if (minChroma &amp;lt; 0 || minChroma &amp;gt; 1 || maxChroma &amp;lt; 0 || maxChroma &amp;gt; 1) {
        throw new Error("Chroma values must be numbers between 0 and 1")
    }

    if (minChroma &amp;gt; maxChroma) {
        throw new Error("minChroma must be less than or equal to maxChroma")
    }

    if (
        steps.some((step) =&amp;gt; step &amp;lt; 0 || step &amp;gt; 1000 || !Number.isInteger(step))
    ) {
        throw new Error("All steps must be integers between 0 and 1000")
    }

    // Generate the color scale using map and reduce
    return steps.reduce&lt;colorscale&gt;((scale, step) =&amp;gt; {
        const h = computeHue(step, baseHue)
        const s = computeChroma(step, minChroma, maxChroma)
        const l = computeLightness(step)

        const srgb = okhslToSrgb([h, s, l])

        return { ...scale, [step]: srgb }
    }, {})
}
&lt;/colorscale&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And just like that, we can generate a color palette with predictable perceptual contrast based shades:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Shade&lt;/th&gt;
&lt;th style="padding-left:18px;"&gt;Gray&lt;/th&gt;
&lt;th style="padding-left:18px;"&gt;Blue&lt;/th&gt;
&lt;th style="padding-left:18px;"&gt;Green&lt;/th&gt;
&lt;th style="padding-left:18px;"&gt;Red&lt;/th&gt;
&lt;th style="padding-left:18px;"&gt;Yellow&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#fff;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#fff&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#fff;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#fff&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#fff;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#fff&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#fff;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#fff&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#fff;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#fff&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#e9e9e9;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#e9e9e9&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#e4eaf4;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#e4eaf4&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#dfeee1;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#dfeee1&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#f4e6e4;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#f4e6e4&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#f2e8dc;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#f2e8dc&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#d7d7d7;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#d7d7d7&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#c7d9f5;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#c7d9f5&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#a9eab2;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#a9eab2&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#f5ccc7;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#f5ccc7&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#f3d1a9;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#f3d1a9&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#c4c4c4;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#c4c4c4&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#a5c6fa;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#a5c6fa&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#66e37e;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#66e37e&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#faaea5;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#faaea5&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#f7b666;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#f7b666&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#b1b1b1;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#b1b1b1&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#81b2fe;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#81b2fe&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#32d25b;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#32d25b&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#fd8c81;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#fd8c81&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#f09c1b;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#f09c1b&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;50&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#9c9c9c;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#9c9c9c&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#5a9cff;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#5a9cff&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#00bd43;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#00bd43&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#ff5f58;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#ff5f58&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#d88900;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#d88900&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#878787;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#878787&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#3083f8;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#3083f8&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#2ba142;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#2ba142&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#f32c34;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#f32c34&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#bb7608;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#bb7608&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;70&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#707070;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#707070&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#2a6ecb;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#2a6ecb&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#3b8343;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#3b8343&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#c13938;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#c13938&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#9a6317;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#9a6317&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;80&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#585858;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#585858&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#2e5892;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#2e5892&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#38643a;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#38643a&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#8c3a37;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#8c3a37&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#754f23;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#754f23&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;90&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#3c3c3c;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#3c3c3c&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#2c3d56;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#2c3d56&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#2f422f;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#2f422f&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#543230;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#543230&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#4b3926;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#4b3926&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#000;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#000&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#000;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#000&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#000;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#000&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#000;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#000&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div style="display:flex;align-items:center;white-space:nowrap;"&gt;
&lt;span style="display:inline-block;width:16px;height:16px;background:#000;border:1px solid #ccc;margin-right:4px;flex-shrink:0;"&gt;&lt;/span&gt;
&lt;span&gt;#000&lt;/span&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You can find the entire code in a &lt;a href="https://github.com/dgtlntv/perceptual-contrast-palette"&gt;Github repository&lt;/a&gt;. I mentioned that I did all this work in preparation for developing a new color palette for Canonical's design system. But in the end, we decided (for good reasons) to go with the WCAG-based approach, which I will write about in my next blog post. So stay tuned üôÇ&lt;/p&gt;
</content:encoded><author>Maximilian Blazek (Maximilian Blazek)</author><category>Design</category><pubDate>Mon, 10 Nov 2025 12:49:14 +0000</pubDate></item><item><title>Web Engineering: Celebrating Our Third Annual Hack Week</title><link>https://ubuntu.com//blog/web-engineering-celebrating-our-third-annual-hack-week</link><description>&lt;p&gt;The Web Engineering team is thrilled to announce the successful conclusion of our third annual Hack Week! Over the past three years, this initiative has become a cornerstone of our collaborative spirit and commitment to innovation. With 126 significant contributions to date, Hack Week provides a dedicated space for our engineers to tackle challenging problems, [&amp;hellip;]&lt;/p&gt;
</description><content:encoded>
&lt;p&gt;The Web Engineering team is thrilled to announce the successful conclusion of our third annual Hack Week! Over the past three years, this initiative has become a cornerstone of our collaborative spirit and commitment to innovation. With 126 significant contributions to date, Hack Week provides a dedicated space for our engineers to tackle challenging problems, refine existing systems, and push the boundaries of what‚Äôs possible.&lt;/p&gt;
&lt;p&gt;The key goals of these events is allowing us to talk with confidence about the true open source nature of our work. We get the opportunity to addressing issues we‚Äôve identified upstream in projects that we use to benefit ourselves and others. By dedicating time to these fixes, we not only improve the stability and performance of our foundational technologies but also empower our team to gain a deeper understanding of complex systems and our dependency tree. The direct engagement with these challenges allows us to truly experience the difficulties firsthand, fostering a unique learning environment. These invaluable learnings are then taken back to our daily projects, where we reflect on the insights gained and implement improvements that benefit all our ongoing work. We are proud of the dedication displayed by everyone involved, and we look forward to continuing this initiative into the future with impactful contributions.&lt;/p&gt;
&lt;p&gt;This year we focused on providing accessibility contributions to our internal corporate message application called &lt;a href="https://mattermost.com/"&gt;Mattermost&lt;/a&gt;. All contributions are listed below:&lt;/p&gt;
&lt;figure class="wp-block-table"&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://github.com/Murtaza-Ax/Color-Converter/pull/1"&gt;Murtaza-Ax/Color-Converter&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://github.com/mattermost/mattermost/pull/34132"&gt;Mattermost #34132&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://github.com/mattermost/mattermost/issues/34128"&gt;Mattermost ##34128&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://github.com/biomejs/biome/pull/7749"&gt;Biome #7749&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://github.com/mattermost/mattermost/issues/26961"&gt;Mattermost #26961&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://github.com/mattermost/mattermost/pull/34141"&gt;Mattermost #34141&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://github.com/mattermost/mattermost/pull/34142"&gt;Mattermost #34142&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://github.com/psycopg/psycopg/pull/1184"&gt;Psycopg #1184&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://github.com/upptime/uptime-monitor/pull/269"&gt;Upptime#269&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://github.com/heroui-inc/heroui/issues/5810"&gt;HeroUI #5810&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://github.com/heroui-inc/heroui/pull/5811"&gt;HeroUI #5811&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://github.com/mattermost/mattermost/issues/34153"&gt;Mattermost #34153&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://github.com/mattermost/mattermost/pull/34154"&gt;Mattermost #34154&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://github.com/TheAlgorithms/JavaScript/pull/1842"&gt;TheAlgorithms/JavaScript #1842&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://github.com/google/adk-web/pull/161"&gt;adk-web #161&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://github.com/heroui-inc/heroui/issues/5814"&gt;HeroUI #5814&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://github.com/heroui-inc/heroui/pull/5813"&gt;HeroUI #5813&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://github.com/TryGhost/Ghost/pull/25183"&gt;Ghost #25183&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://github.com/heroui-inc/heroui/issues/5818"&gt;HeroUI #5818&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://github.com/heroui-inc/heroui/pull/5819"&gt;HeroUI #5819&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://github.com/TryGhost/Ghost/pull/25195"&gt;Ghost #25195&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://github.com/upptime/uptime-monitor/pull/271"&gt;Upptime#271&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://github.com/edlerd/scrabble/pull/3"&gt;Scrabble #3&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://github.com/mattermost/mattermost/pull/34196"&gt;Mattermost #34196&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://github.com/RDjarbeng/countdown/pull/155"&gt;Countdown #155&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://github.com/TryGhost/Ghost/pull/25197"&gt;Ghost #25197&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;Open source encourages compatibility with standards, making locking users in more difficult. This is why we love the freedom open source offers. Open source software allows for sharing knowledge, gaining knowledge, and practising. It promotes transparency in data collection and software systems. Freedom, therefore, is the gift that keeps on giving.&lt;/p&gt;
&lt;p&gt;Please have a look at¬†&lt;a href="https://github.com/canonical/"&gt;our open-source projects&lt;/a&gt;¬†and reach out to us via the issues if anything is unclear.&lt;/p&gt;
</content:encoded><author>Anthony Dillon (Anthony Dillon)</author><category>engineering</category><category>Open source</category><category>web</category><pubDate>Thu, 06 Nov 2025 11:22:28 +0000</pubDate></item><item><title>Azure VM utils now included in Ubuntu: boosting cloud workloads</title><link>https://ubuntu.com//blog/ubuntu-azure-vm-utils-included</link><description>&lt;p&gt;Ubuntu images on Microsoft Azure have recently started shipping with the open source package azure-vm-utils included by default. This change provides essential utilities and udev rules to optimize the Linux experience on Azure, resulting in more reliable disks, smoother networking on accelerated setups, and fewer tweaks to get things running.&lt;/p&gt;
</description><content:encoded>
&lt;p&gt;Ubuntu images on Microsoft Azure have recently started shipping with the open source package &lt;a href="https://learn.microsoft.com/en-us/azure/virtual-machines/linux/azure-virtual-machine-utilities"&gt;&lt;strong&gt;azure-vm-utils&lt;/strong&gt;&lt;/a&gt; included by default. Azure VM utils is a package that provides essential utilities and udev rules to optimize the Linux experience on Azure. This change results in more reliable disks, smoother networking on accelerated setups, and fewer tweaks to get things running. Here‚Äôs what you need to know:&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;What‚Äôs changing&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Smoother storage on modern Azure VMs&lt;/strong&gt;: Ubuntu now provides consistent device naming across SCSI and NVMe, reducing post-reboot surprises and easing automation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Better handling of accelerated networking&lt;/strong&gt;: environments using MANA or Mellanox benefit from safer defaults that avoid double-managing passthrough interfaces.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Less image customization&lt;/strong&gt;: the utility and rules that many platform teams previously added now ship in the image, removing one more custom step from your pipelines.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Why it matters&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fewer post-boot surprises&lt;/strong&gt;: predictable device names keep fstab, cloud-init and provisioning scripts stable across VM families and reboots.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Smoother NVMe adoption&lt;/strong&gt;: newer VM families lean NVMe-first for performance; built-in rules make that transition painless while keeping SCSI setups working.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Less to maintain: &lt;/strong&gt;the stock image now handles Azure disk naming and accelerated NICs (MANA/Mellanox), so teams can drop custom udev/Netplan snippets and avoid fstab surprises after reboots.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;How to Get It&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;For New VMs&lt;/strong&gt;: No action is needed. The package is included by default in new Ubuntu images.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;For Existing VMs&lt;/strong&gt;: You can install the package directly from the Ubuntu archive, where it‚Äôs available for all current LTS and interim releases: sudo apt update &amp;amp;&amp;amp; sudo apt install azure-vm-utils&lt;br/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Quick ways to verify&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;azure-nvme-id --version ¬† ¬† ¬† ¬† ¬† # tool present&lt;br/&gt;find /dev/disk/azure -type l¬† ¬† ¬† # predictable Azure disk links&lt;/code&gt;&lt;/p&gt;
</content:encoded><author>Jehudi (Jehudi)</author><category>Microsoft Azure</category><pubDate>Wed, 05 Nov 2025 15:02:15 +0000</pubDate></item><item><title>Edge Networking gets smarter: AI and 5G in action</title><link>https://ubuntu.com//blog/edge-networking-gets-smarter-ai-and-5g-in-action</link><description>&lt;p&gt;Organizations everywhere are pushing AI and networks closer to the edge. With that expansion comes a challenge: how do you ensure reliable performance, efficiency, and security outside of the data center? Worker safety, healthcare automation, and the success of mobile private networks depend on a robust technology stack that can withstand real-world challenges and still [&amp;hellip;]&lt;/p&gt;
</description><content:encoded>
&lt;p&gt;Organizations everywhere are pushing AI and networks closer to the edge. With that expansion comes a challenge: how do you ensure reliable performance, efficiency, and security outside of the data center? Worker safety, healthcare automation, and the success of mobile private networks depend on a robust technology stack that can withstand real-world challenges and still deliver results. Canonical has partnered with Dell Technologies, Intel, Druid, Airspan and Ecrio to publish a new solution brief addressing this question. The brief highlights how a fully integrated, edge-ready platform can meet the growing demand for intelligent, secure, and real-time computing at the edge.¬†&lt;/p&gt;
&lt;p&gt;The brief showcases how to build a strong foundation for edge AI and networking by using a Dell PowerEdge XR8000 ruggedized edge network+compute platform consisting of two server sleds powered by Intel Xeon Scalable processors. Both sleds are running Canonical‚Äôs software infrastructure stack, which combines &lt;a href="https://ubuntu.com/desktop"&gt;Ubuntu&lt;/a&gt;, &lt;a href="https://canonical.com/microcloud"&gt;MicroCloud&lt;/a&gt;, and &lt;a href="https://ubuntu.com/kubernetes"&gt;Canonical Kubernetes&lt;/a&gt;. On the first sled, MicroCloud hosts two VMs: Airspan Control Platform (ACP) manages the 5G radio units, and Druid Raemis provides the cloud-native 5G core orchestrated by Canonical Kubernetes. The second sled hosts Ecrio‚Äôs iota-e platform, also managed by Canonical Kubernetes, which enables AI-powered real-time image-recognition, voice, video, and messaging services. These capabilities support critical business processes such as worker coordination in industrial settings, emergency response in healthcare, and secure team communications in remote or hazardous environments.&lt;/p&gt;
&lt;p&gt;Download the solution brief to learn how this integrated platform supports advanced use cases, including AI-driven safety monitoring, smart factory operations, and 5G connectivity at the edge.&lt;/p&gt;
&lt;h4 class="wp-block-heading"&gt;In the solution brief, you‚Äôll discover how to:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Deploy AI and event detection workloads on optimized, securely designed infrastructure&lt;/li&gt;
&lt;li&gt;Operate private 5G and RAN control software on edge-virtualized environments&lt;/li&gt;
&lt;li&gt;Streamline orchestration and lifecycle management with Canonical Kubernetes and MicroCloud&lt;/li&gt;
&lt;li&gt;Detect safety and operational risks in real time using integrated AI inference&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;a aria-label=" (opens in a new tab)" class="p-button--small p-button--positive" href="https://ubuntu.com/engage/ai-powered-edge-networking" rel="noreferrer noopener" target="_blank"&gt;Download the full solution brief&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For more information on how Canonical supports your edge and AI journey, visit our related content:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://canonical.com/solutions/ai"&gt;Open source AI for the enterprise&lt;/a&gt;&lt;br/&gt;Discover how Canonical enables AI workloads from cloud to edge with tools for model training, trusted deployment, and lifecycle management. This webpage outlines Canonical‚Äôs full AI stack, from Ubuntu-optimized hardware acceleration to MLOps best practices, with links to blogs, whitepapers, and deployment guides.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://canonical.com/solutions/telco"&gt;Canonical Telco solutions&lt;br/&gt;&lt;/a&gt;Learn how Canonical helps telecom operators modernize their infrastructure using open source technologies. This hub covers solutions for 5G core networks and Radio Access Networks (RAN) built on Ubuntu, Canonical Kubernetes, &lt;a href="https://canonical.com/openstack"&gt;OpenStack&lt;/a&gt;, &lt;a href="https://canonical.com/maas"&gt;MAAS&lt;/a&gt; and &lt;a href="https://juju.is/"&gt;Juju&lt;/a&gt;. You‚Äôll find case studies and insights into telco-grade performance and security.&lt;/li&gt;
&lt;/ul&gt;
</content:encoded><author>Benjamin Ryzman (Benjamin Ryzman)</author><category>AI</category><category>Edge AI</category><category>edge networking</category><category>inference</category><category>telco edge cloud</category><pubDate>Wed, 05 Nov 2025 07:11:49 +0000</pubDate></item><item><title>Why we brought hardware-optimized GenAI inference to Ubuntu¬†</title><link>https://ubuntu.com//blog/genai-inference</link><description>&lt;p&gt;On October 23rd, we announced the beta availability of silicon-optimized AI models in Ubuntu. Developers can locally install DeepSeek R1 and Qwen 2.5 VL with a single command, benefiting from maximized hardware performance and automated dependency management. Application developers can access the local API of a quantized generative AI (GenAI) model with runtime optimizations for [&amp;hellip;]&lt;/p&gt;
</description><content:encoded>
&lt;p&gt;On October 23rd, we &lt;a href="https://canonical.com/blog/canonical-releases-inference-snaps"&gt;announced&lt;/a&gt; the beta availability of &lt;a href="https://documentation.ubuntu.com/inference-snaps/reference/snaps/"&gt;silicon-optimized AI models&lt;/a&gt; in Ubuntu. Developers can locally install &lt;a href="https://github.com/deepseek-ai/DeepSeek-R1"&gt;DeepSeek R1&lt;/a&gt; and &lt;a href="https://github.com/QwenLM/Qwen-VL"&gt;Qwen 2.5 VL&lt;/a&gt; with a single command, benefiting from maximized hardware performance and automated dependency management.&lt;/p&gt;
&lt;p&gt;Application developers can access the local API of a quantized generative AI (GenAI) model with runtime optimizations for efficient performance on their CPU, GPU, or NPU.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="" height="1046" loading="lazy" sizes="(min-width: 1346px) 1346px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1346/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fc6dc%2Fimage.png" srcset="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fc6dc%2Fimage.png 460w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fc6dc%2Fimage.png 620w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fc6dc%2Fimage.png 1036w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1681/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fc6dc%2Fimage.png 1681w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1920/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fc6dc%2Fimage.png 1920w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1346/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fc6dc%2Fimage.png 1346w" width="1346"/&gt;&lt;/figure&gt;
&lt;p&gt;&lt;em&gt;Architecture of the new &lt;/em&gt;&lt;a href="https://documentation.ubuntu.com/inference-snaps/contributing/#tooling"&gt;&lt;em&gt;open-source tool&lt;/em&gt;&lt;/a&gt;&lt;em&gt; enabling developers to bundle¬† different combinations of runtimes and weights into a single &lt;/em&gt;&lt;a href="https://snapcraft.io/docs"&gt;&lt;em&gt;snap&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, deploying the most optimal stack on the host machine&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;By meeting developers at the intersection of silicon and GenAI models, we package, distribute and manage all the necessary components to run AI apps on any machine that runs Ubuntu. Developers can now install pre-trained and fine-tuned AI models that automatically detect the underlying silicon requirements, from how much memory and what GPU or NPU they need, to which software components and default configurations must be included.&lt;/p&gt;
&lt;p&gt;What‚Äôs the vision behind &lt;a href="https://www.youtube.com/live/bEEamxJ60aI?t=23812s"&gt;the announcement&lt;/a&gt;, and how did we pull it off?&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Ubuntu: the standard distribution platform for AI models&lt;/h2&gt;
&lt;p&gt;We aim to make Ubuntu the standard distribution platform for generative AI models. Doing so will enable developers to integrate AI seamlessly into their applications and run them optimally across &lt;a href="https://ubuntu.com/desktop"&gt;desktops&lt;/a&gt;, &lt;a href="https://ubuntu.com/server"&gt;servers&lt;/a&gt;, and &lt;a href="https://ubuntu.com/core"&gt;edge devices&lt;/a&gt;. We believe machine learning (ML) workloads will soon be as fundamental to compute platforms as traditional software dependencies are today, and generative AI models will be a basic part of the compute experience.¬†&lt;/p&gt;
&lt;p&gt;But wait: isn‚Äôt that already true? ¬†Aren‚Äôt AI models already everywhere, and don‚Äôt we all play with LLMs &lt;a href="https://spectrum.ieee.org/ai-energy-use"&gt;around 25 times per day&lt;/a&gt;?¬†&lt;/p&gt;
&lt;p&gt;Yes, but there‚Äôs a key distinction. Let me use an analogy to illustrate it.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h3 class="wp-block-heading"&gt;From fragmentation to curated archives of software&lt;/h3&gt;
&lt;p&gt;In the early days of Linux, software distribution was fragmented and cumbersome. Developers had to manually download, compile, and configure source code from individual projects, often tracking down missing libraries, resolving version conflicts, and maintaining complex build environments by hand.¬†&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="" height="180" loading="lazy" sizes="(min-width: 403px) 403px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_403/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fb867%2FScreenshot-from-2025-10-29-14-04-05.png" srcset="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_403/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fb867%2FScreenshot-from-2025-10-29-14-04-05.png 403w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_806/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fb867%2FScreenshot-from-2025-10-29-14-04-05.png 806w" width="403"/&gt;&lt;/figure&gt;
&lt;p&gt;&lt;em&gt;While in the early 90s, software was distributed via floppy disks, Slackware and Debian Linux soon ushered in a system of curated archives of software, usually pre-compiled to save time.&lt;/em&gt; &lt;em&gt;Source: &lt;/em&gt;&lt;a href="https://www.debian.org/"&gt;&lt;em&gt;https://www.debian.org/&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;As each distribution had its own conventions, packaging tools, and repositories, installing software was an error-prone and time-consuming process. The lack of a unified delivery mechanism slowed down open-source development and created barriers to adoption.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="" height="960" loading="lazy" sizes="(min-width: 1280px) 1280px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1280/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fc85f%2Fimage-1.png" srcset="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fc85f%2Fimage-1.png 460w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fc85f%2Fimage-1.png 620w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fc85f%2Fimage-1.png 1036w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1681/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fc85f%2Fimage-1.png 1681w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1920/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fc85f%2Fimage-1.png 1920w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1280/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fc85f%2Fimage-1.png 1280w" width="1280"/&gt;&lt;/figure&gt;
&lt;p&gt;&lt;em&gt;In &lt;/em&gt;&lt;a href="https://lists.ubuntu.com/archives/ubuntu-announce/2004-October/000003.html"&gt;&lt;em&gt;October 2004&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, the first release of Ubuntu was out. It shipped with a fairly fixed set of packages in the Ubuntu archive, for which users received security updates and bug fixes over the internet. To get new software, developers still had to hunt down source code and compile it themselves.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;What changed?&lt;/p&gt;
&lt;p&gt;Fast-forward to a few years later, and &lt;a href="https://canonical.com/blog/canonical-announces-launch-of-launchpad-personal-package-archive-service-for-developers"&gt;in 2007&lt;/a&gt;, Canonical introduced &lt;a href="https://launchpad.net/ubuntu/+ppas"&gt;Personal Package Archives (PPA)&lt;/a&gt;, giving developers a hosted build service to publish and share their own software. Discovering new software on Linux was still hard, from living in unknown PPAs to GitHub repositories with daily builds of all kinds of new software. To fix this, Canonical later introduced &lt;a href="https://snapcraft.io/docs"&gt;snaps&lt;/a&gt;, containerized software packages that simplified cross-distribution delivery, updates and security.&lt;/p&gt;
&lt;p&gt;Standing on the shoulders of giants and building on Debian, Ubuntu helped transform that experience, becoming the aggregation point for open-source software (OSS). Ubuntu consolidated thousands of upstream projects into a coherent, trusted ecosystem that developers could rely on, without needing to understand every dependency or build chain behind it. Ubuntu helped unify and streamline the open-source ecosystem.&lt;/p&gt;
&lt;p&gt;A strong packaging foundation, combined with a steady release cadence and curated repositories, lowered the barrier for both developers and enterprises. Ubuntu became the default, trusted layer for distributing and maintaining open-source software.¬†&lt;/p&gt;
&lt;p&gt;What if we could do that with AI models?&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h3 class="wp-block-heading"&gt;GenAI models as basic building blocks of future compute¬†&lt;/h3&gt;
&lt;p&gt;Today, software packages are the basic building blocks of compute. Developers routinely install packages, add PPAs, and pull from various vendors, third parties, or the community, without giving it much thought.¬†&lt;/p&gt;
&lt;p&gt;We believe that AI models will soon occupy the same space as first-class citizens of a compute stack. They‚Äôll be treated as standard system components, installed, updated and optimized just like any other dependency. We‚Äôll no longer worry about the details of how to juggle the dependencies of various AI models, just as we don‚Äôt think about which repositories the packages your projects depend on come from. Developing software will naturally include integrating ML workloads, and models will be as ubiquitous and invisible in the developer experience as traditional packages are today. LLMs will become part of the commodity layer of compute, evolving into dependencies that containerized workloads rely on: composable, versioned, and hardware-optimized.¬†&lt;/p&gt;
&lt;p&gt;In making Ubuntu, we mastered the art of distributing open-source software to millions of users. Now we are applying that expertise to AI model distribution. Ubuntu is moving toward making AI models a native element of the compute environment. We‚Äôre shifting AI models from external tools to an integral part of the stack. Bringing silicon-optimized AI models natively to Ubuntu is the first step in making them a built-in component of the compute experience itself.¬†&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;What are silicon-optimized models?&lt;/h2&gt;
&lt;p&gt;In &lt;a href="https://canonical.com/blog/canonical-releases-inference-snaps"&gt;the announcement&lt;/a&gt;, we introduced Intel and Ampere ‚Äì optimized &lt;a href="https://snapcraft.io/deepseek-r1"&gt;DeepSeek R1&lt;/a&gt; and &lt;a href="https://snapcraft.io/qwen-vl"&gt;Qwen VL&lt;/a&gt;, two leading examples of Generative AI models.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/deepseek-ai/DeepSeek-R1"&gt;DeepSeek R1&lt;/a&gt; is a reasoning Large Language Model (LLM) designed to decompose prompts into structured chains of thought, enabling complex reasoning and problem solving. &lt;a href="https://github.com/QwenLM/Qwen-VL"&gt;Qwen VL&lt;/a&gt;, on the other hand, is a multimodal LLM that accepts both text and images as inputs, representing the latest generation of vision-language models. Both are transformer-based but tuned and packaged to exploit different runtime patterns and hardware characteristics.&lt;/p&gt;
&lt;p&gt;Let‚Äôs be more specific. The term &lt;em&gt;model&lt;/em&gt; is often used loosely, but in practice, it refers to an entire model family built around a base model. Base models are massive neural networks trained on broad datasets to capture general knowledge. These base models are frequently fine-tuned, retrained, or adapted to specialize in specific tasks, such as instruction following or domain-specific reasoning. For instance, &lt;a href="https://arxiv.org/abs/1706.03762"&gt;transformer-based LLMs&lt;/a&gt; share a common architecture built on components such as self-attention, multi-head attention, and large embedding matrices. From this base, families of foundational models and fine-tuned derivatives, such as instruction-tuned models, adapter-based variants, or task-specific fine-tunes, can be developed.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h3 class="wp-block-heading"&gt;Inference with fine-tuned models&lt;/h3&gt;
&lt;p&gt;Let‚Äôs look at an example of some of the Mistral models from &lt;a href="https://mistral.ai/"&gt;Mistral AI&lt;/a&gt;.¬†&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="" height="543" loading="lazy" sizes="(min-width: 699px) 699px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_699/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F7fd3%2Fimage.png" srcset="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F7fd3%2Fimage.png 460w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F7fd3%2Fimage.png 620w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F7fd3%2Fimage.png 1036w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_699/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F7fd3%2Fimage.png 699w" width="699"/&gt;&lt;/figure&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;On the left-hand side, we have the model vendor, in this case, Mistral AI, which trains and distributes the foundational base models. Fine-tuned derivatives, such as &lt;a href="https://mistral.ai/news/announcing-mistral-7b"&gt;mistral-7b-instruct&lt;/a&gt;, are then adapted for instruction-based use cases, responding to prompts in a structured, context-aware manner.&lt;/p&gt;
&lt;p&gt;Another model family might look similar but target different objectives or architectures:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="" height="351" loading="lazy" sizes="(min-width: 1040px) 1040px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1040/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Feafc%2Fimage.png" srcset="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Feafc%2Fimage.png 460w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Feafc%2Fimage.png 620w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Feafc%2Fimage.png 1036w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1681/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Feafc%2Fimage.png 1681w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1920/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Feafc%2Fimage.png 1920w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1040/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Feafc%2Fimage.png 1040w" width="1040"/&gt;&lt;/figure&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;However, a ‚Äúmodel‚Äù¬† ‚Äì whether base or fine-tuned ‚Äì is not particularly useful on its own. It‚Äôs essentially a collection of learned weights: billions of numerical parameters, with no runtime context. What matters to developers is the inference stack, the combination of a trained model and an optimized runtime that makes inference possible. In the literature, the term ‚Äúmodel‚Äù often refers to the complete model artifact, including the tokenizer, pre and post-processing components, and runtime format, e.g., &lt;a href="https://docs.pytorch.org/docs/stable/checkpoint.html"&gt;PyTorch checkpoint&lt;/a&gt;, &lt;a href="https://github.com/onnx/onnx"&gt;ONNX&lt;/a&gt;, and &lt;a href="https://github.com/ggml-org/ggml"&gt;GGML&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="" height="332" loading="lazy" sizes="(min-width: 1325px) 1325px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1325/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fadd7%2Fimage-1.png" srcset="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fadd7%2Fimage-1.png 460w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fadd7%2Fimage-1.png 620w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fadd7%2Fimage-1.png 1036w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1681/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fadd7%2Fimage-1.png 1681w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1920/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fadd7%2Fimage-1.png 1920w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1325/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fadd7%2Fimage-1.png 1325w" width="1325"/&gt;&lt;/figure&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;Inference stacks include inference engines, the software responsible for executing the model efficiently on specific hardware. Besides the weights of the pre-trained model, e.g. the weights of &lt;a href="https://github.com/QwenLM/Qwen-VL"&gt;Qwen2.5 VL&lt;/a&gt; quantized at Q4_K, an engine will typically include the execution logic, optimizations to efficiently perform matrix multiplications and supporting subsystems. Examples include Nvidia‚Äôs &lt;a href="https://github.com/NVIDIA/TensorRT"&gt;TensorRT&lt;/a&gt;, Intel‚Äôs &lt;a href="https://github.com/openvinotoolkit/openvino"&gt;OpenVINO&lt;/a&gt;, Apache‚Äôs &lt;a href="https://github.com/apache/tvm"&gt;TVM&lt;/a&gt;, and vendor-specific runtimes for NPUs. Multiple stacks can exist for the same model, each tailored to different architectures or performance targets. These engines differ in supported features, kernel implementations, and hardware integration layers ‚Äì reflecting differences in CPU, GPU, NPU, or accelerator capabilities.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="" height="1195" loading="lazy" sizes="(min-width: 1600px) 1600px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1600/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F42c5%2Fimage.png" srcset="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F42c5%2Fimage.png 460w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F42c5%2Fimage.png 620w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F42c5%2Fimage.png 1036w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1681/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F42c5%2Fimage.png 1681w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1920/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F42c5%2Fimage.png 1920w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1600/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F42c5%2Fimage.png 1600w" width="1600"/&gt;&lt;/figure&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;For instance, for a fine-tuned model such as mistral-7b-instruct, one might find multiple inference stacks:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="" height="901" loading="lazy" sizes="(min-width: 864px) 864px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_864/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F77a4%2Fimage.jpeg" srcset="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F77a4%2Fimage.jpeg 460w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F77a4%2Fimage.jpeg 620w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F77a4%2Fimage.jpeg 1036w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1681/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F77a4%2Fimage.jpeg 1681w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_864/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F77a4%2Fimage.jpeg 864w" width="864"/&gt;&lt;/figure&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h3 class="wp-block-heading"&gt;Optimizing hardware for inference&lt;/h3&gt;
&lt;p&gt;Running LLMs efficiently depends critically on the underlying hardware and available system resources. This is why optimized versions of fine-tuned models are often required. A common form of optimization is quantization, reducing the numerical precision of a model‚Äôs parameters (for example, converting 16-bit floating-point weights to 8-bit or 4-bit integers). Quantization reduces the model‚Äôs memory footprint and computational load, enabling faster inference or deployment on smaller devices, often with minimal degradation in accuracy.&lt;/p&gt;
&lt;p&gt;Beyond quantization, optimization can be silicon-specific. Different hardware architectures, e.g. GPUs, TPUs, NPUs, or specialized AI accelerators, exhibit unique performance characteristics: compute density, memory bandwidth, and energy efficiency. Vendors exploit these characteristics through hardware-aware model variants, which are fine-tuned or compiled to maximize performance on their specific silicon.&lt;/p&gt;
&lt;p&gt;For silicon vendors, demonstrating superior inference performance directly translates into market differentiation. Even marginal improvements ‚Äì a 2% gain in throughput or latency on a leading LLM ‚Äì can have significant implications when scaled across data centers or deployed at the edge.&lt;/p&gt;
&lt;p&gt;This performance race fuels intense investment in AI model optimization across the hardware ecosystem. Each vendor aims to maximize effective TFLOPS and real-world inference efficiency. The result is an expanding landscape of hardware-optimized model variants, from aggressively quantized models that fit within strict memory limits to GPU-tuned builds exploiting tensor cores and parallel compute pipelines.&lt;/p&gt;
&lt;p&gt;Furthermore, model packaging and runtime format affect deployability, as one needs optimized artefacts per target, e.g. TorchScript/&lt;a href="https://github.com/onnx/onnx"&gt;ONNX&lt;/a&gt; for GPUs, vendor-compiled binaries for NPUs, &lt;a href="https://github.com/ggml-org/ggml"&gt;GGML&lt;/a&gt; or int8 CPU builds for constrained devices.&lt;/p&gt;
&lt;p&gt;As a consequence, developers building embedded AI apps are stuck dealing with API keys, per-token subscriptions, and apps that only work when connected to fast internet. Packaging and distributing AI-powered software is hard. Developers must contend with dozens of silicon types, hundreds of hardware configurations, and an ever-growing number of models and variants ‚Äì while also managing dependencies, model updates, runtime engines, API servers, optimizations, and more.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Simplifying AI development: abstracting complexity away&lt;/h2&gt;
&lt;p&gt;Is it possible to abstract that complexity away? Today, developers build on Ubuntu without needing to think about the underlying hardware. The same principle should apply to AI: what if, in the future, a developer could simply code for DeepSeek,¬† without worrying about selecting the optimal fine-tuned variant, choosing the right inference engine, or targeting a specific silicon architecture?&lt;/p&gt;
&lt;p&gt;This is the challenge we set out for ourselves, bridging the gap between the potential of AI and its practical adoption. Our goal is to bring the right models directly into developers‚Äô hands and make LLMs part of everyday software development&lt;/p&gt;
&lt;p&gt;We envision a world where application developers can target an AI model, not a stack, and seamlessly use hardware-specific optimizations under the hood. To truly harness the potential of AI, developers shouldn‚Äôt have to worry about quantization levels, inference runtimes, or attaching API keys. They should simply develop against a consistent model interface.&lt;/p&gt;
&lt;p&gt;Unfortunately, today‚Äôs AI ecosystem is still fragmented. Developer environments lack a standard packaging and distribution model, making AI deployment costly, inconsistent, and complex. Teams often spend significant time configuring, benchmarking, and tuning inference stacks for different accelerators, work that demands deep expertise and still leaves hardware underutilized.&lt;/p&gt;
&lt;p&gt;This is why, through Canonical‚Äôs strong ecosystem partnerships, we introduced an abstraction layer that gives users access to develop using a known model while integrating the hardware-specific stacks. Last week, we &lt;a href="https://canonical.com/blog/canonical-releases-inference-snaps"&gt;announced&lt;/a&gt; the public beta release of AI models on &lt;a href="https://releases.ubuntu.com/24.04/"&gt;Ubuntu 24.04 LTS&lt;/a&gt;, with &lt;a href="https://snapcraft.io/deepseek-r1"&gt;DeepSeek R1&lt;/a&gt; and &lt;a href="https://snapcraft.io/qwen-vl"&gt;Qwen 2.5 VL&lt;/a&gt; builds optimized for Intel and Ampere hardware. ¬†Developers can locally install those snaps pre-tuned for their silicon, without wrestling with dependencies or manual setup.¬† Our snap approach enables development against a model‚Äôs standard API on Ubuntu, while relying on optimized builds engineered by Canonical, Intel, and Ampere.&lt;/p&gt;
&lt;p&gt;Silicon-vendor optimizations will now be automatically included when detecting the hardware. For example, when installing the &lt;a href="https://snapcraft.io/qwen-vl"&gt;Qwen VL snap&lt;/a&gt; on an amd64 workstation, the system will automatically select the most suitable version ‚Äì whether optimized for Intel integrated or discrete GPUs, Intel NPUs, Intel CPUs, or NVIDIA GPUs (with CUDA acceleration). Similarly, on arm64 systems using Ampere Altra/One processors, the version optimized for those CPUs will be used. If none of these optimizations match the hardware, Qwen VL will automatically fall back to a generic CPU engine to ensure compatibility.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Canonical‚Äôs silicon partnerships: planning for the future&lt;/h2&gt;
&lt;p&gt;As we saw, the performance of AI models is tightly bound to the silicon layer. Optimizing for silicon covers multiple layers, from reduced numeric precision, to operator fusion and kernel fusion, memory layout and tiling changes, and vendor-specific kernel implementations. The inference stack itself, from &lt;a href="https://github.com/NVIDIA/TensorRT"&gt;TensorRT&lt;/a&gt;, &lt;a href="https://github.com/onnx/onnx"&gt;ONNX&lt;/a&gt; Runtime, &lt;a href="https://github.com/openvinotoolkit/openvino"&gt;OpenVINO/oneAPI&lt;/a&gt;,¬† and vendor NPUs‚Äô runtimes, materially affects latency, throughput and resource utilization. By working with silicon leaders, Canonical can now deliver robust, stable, locally optimized models that run efficiently on desktops, servers, and edge devices, reducing reliance on massive cloud GPU deployments, lowering costs and energy use, improving latency, and keeping sensitive data on-device. Each model can be installed with a single command, without manual setup or dependency management. Once installed, the snap automatically detects the underlying silicon, currently optimized for Intel CPUs, GPUs and NPUs, and¬† Ampere CPUs, applying the most effective combination of inference engine and model variant.¬†&lt;/p&gt;
&lt;p&gt;With &lt;a href="https://ubuntu.com/core"&gt;Ubuntu Core&lt;/a&gt;, &lt;a href="https://ubuntu.com/desktop"&gt;Desktop&lt;/a&gt; and &lt;a href="https://ubuntu.com/server"&gt;Server&lt;/a&gt;, we already provide a consistent OS experience to millions of developers across verticals and form factors. We are now eager to extend our collaborations with the silicon community and broader ecosystem, and are perfectly placed to enable AI models across the compute spectrum.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://canonical-inference-snaps-docs.readthedocs-hosted.com/"&gt;Read the docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://canonical-inference-snaps-docs.readthedocs-hosted.com/contributing/#contribute-to-our-code"&gt;Access the source code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/canonical/inference-snaps/discussions"&gt;Share your feedback&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://canonical.com/blog/canonical-releases-inference-snaps"&gt;Read the press release&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/live/bEEamxJ60aI?t=23812s"&gt;Watch the live announcement&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content:encoded><author>Edoardo Barbieri (Edoardo Barbieri)</author><category>genai</category><category>inference</category><category>LLMs</category><pubDate>Thu, 30 Oct 2025 08:42:28 +0000</pubDate></item><item><title>Canonical and NVIDIA BlueField-4: a foundation for zero-trust high performance infrastructure</title><link>https://ubuntu.com//blog/canonical-and-nvidia-bluefield-4-a-foundation-for-zero-trust-high-performance-infrastructure</link><description>&lt;p&gt;At NVIDIA GTC Washington D.C., Canonical is pleased to support the arrival of the NVIDIA BlueField-4 &amp;#8211; the newest generation of the data processing unit (DPU) family. NVIDIA BlueField-4 is an accelerated infrastructure platform for gigascale AI factories. By combining NVIDIA Grace CPU and NVIDIA ConnectX-9 networking, it delivers 6x the compute power of BlueField-3 [&amp;hellip;]&lt;/p&gt;
</description><content:encoded>
&lt;p&gt;At NVIDIA GTC Washington D.C., Canonical is pleased to support the arrival of the NVIDIA BlueField-4 ‚Äì the newest generation of the data processing unit (DPU) family. &lt;a href="https://www.nvidia.com/en-us/networking/products/data-processing-unit/"&gt;NVIDIA BlueField-4&lt;/a&gt; is an accelerated infrastructure platform for gigascale AI factories. By combining &lt;a href="https://www.nvidia.com/en-us/data-center/grace-cpu/"&gt;NVIDIA Grace CPU&lt;/a&gt; and &lt;a href="https://www.nvidia.com/en-us/networking/ethernet-adapters/"&gt;NVIDIA ConnectX-9 networking&lt;/a&gt;, it delivers 6x the compute power of BlueField-3 and 800 Gb/s throughput to accelerate these systems. BlueField-4features multi-tenant networking, rapid data access, AI runtime security, and enables¬† high-performance inference processing. Running natively on BlueField-4, &lt;a href="https://developer.nvidia.com/networking/doca"&gt;NVIDIA DOCA microservices&lt;/a&gt; deliver containerized services to simplify and scale AI infrastructure.&lt;/p&gt;
&lt;p&gt;As with previous generations, BlueField-4 supports the Ubuntu¬† OS, which comes with Canonical‚Äôs security maintenance and support. This development is the latest from Canonical‚Äôs longstanding collaboration with NVIDIA to advance the state of DPU-driven infrastructure.¬†&lt;/p&gt;
&lt;h1 class="wp-block-heading"&gt;A securely-designed foundation¬†&lt;/h1&gt;
&lt;p&gt;Zero-trust architecture places emphasis on the integrity of infrastructure, which is isolated from untrusted workloads. No component, workload, or user is implicitly trusted, and every interaction within the system is continuously verified and enforced by NVIDIA BlueField at the infrastructure level. In this model, the DPU acts as a hardware-based control and enforcement plane, isolating workloads, validating software integrity, and handling encryption and network policy enforcement independently from the host CPU.&lt;/p&gt;
&lt;p&gt;NVIDIA BlueField-4 supports multi-service architectures with native service function chaining, zero-trust tenant isolation, and software defined infrastructure control. Running natively on BlueField-4, NVIDIA DOCA microservices deliver prebuilt, containerized services for AI networking, orchestration, real-time threat detection, and data acceleration‚Äìsimplifying operations and enabling enterprises and service providers to scale AI securely and efficiently. Enterprises can also deploy validated, BlueField-accelerated applications from leading software providers, enabling advanced infrastructure acceleration and cybersecurity capabilities that enhance the platform‚Äôs value.&lt;/p&gt;
&lt;h1 class="wp-block-heading"&gt;Ubuntu 24.04 LTS on BlueField-4&lt;/h1&gt;
&lt;p&gt;Ubuntu plays a key role in supporting the overall security posture of zero-trust BlueField-4 infrastructure. BlueField-4 effectively introduces a dedicated control and enforcement domain alongside the host system, meaning it meets the same security and compliance expectations as any other infrastructure component in the data center. In highly regulated environments, where every element is expected to be hardened and certifiable, the software foundation of BlueField becomes just as important, if not more, as that of the host.&lt;/p&gt;
&lt;p&gt;Because the BlueField software stack is based on Ubuntu 24.04 LTS, it benefits from Canonical‚Äôs signed packages and reproducible build processes. Expanded Security Maintenance (ESM) provides long-term maintenance guarantees. Ubuntu Pro extends this foundation with continuous CVE monitoring, patch delivery, and compliance tooling, giving operators a clear view of security status and patch levels. When DPUs are deployed in environments that require FIPS, DISA-STIG, or similar compliance frameworks, this is essential. These features, supported in the NVIDIA AI Factory for Government reference design, ensure organizations can integrate BlueField-4 into sensitive infrastructure with confidence, knowing that the underlying operating system aligns with their existing security and compliance processes.&lt;/p&gt;
&lt;p&gt;In terms of performance, Canonical publishes optimized Ubuntu images, designed to get the most out of BlueField-4, which combines NVIDIA Grace CPU and NVIDIA ConnectX-9 networking. With NVIDIA Grace, a CPU already &lt;a href="https://ubuntu.com/certified/202311-32325"&gt;certified on Ubuntu 24.04 LTS&lt;/a&gt;, operators can deploy with confidence, knowing their platforms have undergone comprehensive validation across performance, reliability, and interoperability. In practical terms, this includes an optimized Ubuntu kernel which combines with NVIDIA drivers on Grace CPU architecture to provide efficient scheduling and accelerated I/O performance on its Arm-based cores.¬†&lt;/p&gt;
&lt;h1 class="wp-block-heading"&gt;Advanced networking with Ubuntu 24.04 LTS&lt;/h1&gt;
&lt;p&gt;Ubuntu 24.04 LTS provides a robust foundation for service function chaining and software-defined networking (SDN) in BlueField-4 deployments. Ubuntu‚Äôs networking stack is optimized for deterministic performance, low latency, and full hardware acceleration.&lt;/p&gt;
&lt;p&gt;In environments where complex network services, such as firewalls, load balancers, and intrusion detection, must operate in sequence at line rate, Ubuntu‚Äôs Linux kernel is optimized for BlueField and enables high performance service function chaining. Developers can opt to use Canonical‚Äôs open virtual network (OVN), which integrates tightly with NVIDIA OVS-DOCA (Open vSwitch) to offload data plane operations directly onto the BlueField-4 programmable platform. This allows for traffic steering, encapsulation, and flow processing to occur entirely within the DPU, freeing host resources and ensuring wirespeed throughput even in multi-tenant or multi-domain deployments.&lt;/p&gt;
&lt;h1 class="wp-block-heading"&gt;Use cases for telco and public sector&lt;/h1&gt;
&lt;h2 class="wp-block-heading"&gt;5G Core and edge networking&lt;/h2&gt;
&lt;p&gt;Service providers can offload user plane function (UPF) and service chaining to BlueField-4, accelerating 5G core workloads running on Ubuntu OpenStack and Kubernetes. With secure tenant isolation via BlueField Advanced Secure Trusted Resource Architecture, operators can enforce zero-trust policies across multi-tenant, high-throughput environments.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Cybersecurity and mission-critical systems&lt;/h2&gt;
&lt;p&gt;In mission-critical settings, BlueField-4 with Ubuntu enables line-rate intrusion detection, data encryption, and air-gapped control planes, executing directly in the DPU for minimal latency and maximum assurance. With Ubuntu‚Äôs FIPS validation and DISA-STIG compliance, organizations can deploy infrastructure that meets stringent operational and regulatory standards.&lt;/p&gt;
&lt;h1 class="wp-block-heading"&gt;A shared vision for the future&lt;/h1&gt;
&lt;p&gt;Canonical and NVIDIA have already demonstrated the power of combining Ubuntu, Kubernetes, and DOCA for networking acceleration, as described in our earlier post: &lt;a href="https://canonical.com/blog/canonical-kubernetes-meets-nvidia-doca-platform-framework-dpf-building-the-future-of-dpu-driven-infrastructure"&gt;Canonical Kubernetes Meets NVIDIA DOCA Platform Framework (DPF)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Canonical and NVIDIA share a commitment to advancing open, programmable, and securely-designed infrastructure. With BlueField-4 on Ubuntu 24.04 LTS, organizations gain a validated, compliant, and high-performance platform to power the next era of AI, telco, and government infrastructure.&lt;/p&gt;
&lt;p&gt;Together, we‚Äôre enabling governments, operators, and enterprises to deploy scalable, securely maintained, and future-proof infrastructure at gigascale.&lt;/p&gt;
</content:encoded><author>Benjamin Ryzman (Benjamin Ryzman)</author><category>AI</category><category>networking</category><category>nvidia</category><pubDate>Tue, 28 Oct 2025 19:24:39 +0000</pubDate></item><item><title>Global-ready from day one</title><link>https://ubuntu.com//blog/localization-testing</link><description>&lt;p&gt;How Anbox Cloud streamlines localization testing Wherever users are based, they expect apps to just work, whether in Japanese, Arabic, or Spanish. But anyone who‚Äôs touched localization knows it‚Äôs more than translation. Real quality comes from testing how your app behaves across languages, layouts, and regions ‚Äì and doing it fast. If you&amp;#8217;re shipping apps [&amp;hellip;]&lt;/p&gt;
</description><content:encoded>
&lt;h2 class="wp-block-heading"&gt;How Anbox Cloud streamlines localization testing&lt;/h2&gt;
&lt;p&gt;Wherever users are based, they expect apps to just work, whether in Japanese, Arabic, or Spanish. But anyone who‚Äôs touched localization knows it‚Äôs more than translation. Real quality comes from testing how your app behaves across languages, layouts, and regions ‚Äì and doing it fast.&lt;/p&gt;
&lt;p&gt;If you‚Äôre shipping apps for automotive or gaming, localization gets complex fast. It‚Äôs never just about translation: you‚Äôre adapting to different layouts, alphabet types, interaction models, and hardware quirks. You‚Äôre aiming for pixel-perfect across every region, while teams are spread across time zones and builds keep coming.&lt;/p&gt;
&lt;p&gt;Anbox Cloud cuts through all of that: enabling real-time, browser-based localization testing at scale. No APK sharing. No device juggling. Enabling you to localize at speed, and at scale.&lt;/p&gt;
&lt;h1 class="wp-block-heading"&gt;A consistent experience, everywhere&lt;/h1&gt;
&lt;p&gt;Let‚Äôs examine a common use case: an automotive Tier 1 supplier building in-vehicle Infotainment (IVI) apps for multiple original equipment manufacturers (OEMs). One app, many markets. Each with different languages, reading directions, and screen resolutions. The traditional approach to testing means emailing builds to local quality assurance (QA) teams, or trying to simulate every scenario in-house. If you‚Äôve experienced this approach first hand, then you‚Äôll know it is both fragile and slow.&lt;/p&gt;
&lt;p&gt;With Anbox Cloud, all you have to do is launch a container with the right locale in seconds. Your team (in Berlin, Tokyo, or Washington) gets a secure URL, opens their browser, and tests live. No flashing. No setup delays. No exposure of early builds or IP.&lt;/p&gt;
&lt;p&gt;Because it runs in the cloud, you control access, enforce authentication, and test in real-time, without sending binaries halfway across the world.&lt;/p&gt;
&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="" height="852" loading="lazy" sizes="(min-width: 1280px) 1280px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1280/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fcf37%2Fimage.png" srcset="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fcf37%2Fimage.png 460w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fcf37%2Fimage.png 620w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fcf37%2Fimage.png 1036w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1681/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fcf37%2Fimage.png 1681w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1920/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fcf37%2Fimage.png 1920w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1280/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fcf37%2Fimage.png 1280w" width="1280"/&gt;&lt;/figure&gt;
&lt;h1 class="wp-block-heading"&gt;Localization QA at scale: the gaming angle&lt;/h1&gt;
&lt;p&gt;Now let‚Äôs switch to mobile gaming, where localization isn‚Äôt just a checkbox: it‚Äôs revenue. A game that looks fine in English can break in Turkish, or wrap badly in Finnish. Fonts, line breaks, layout shifts, it all matters. And you don‚Äôt want to hear about it from your players.&lt;/p&gt;
&lt;p&gt;Global studios know the pain: you need to test across devices, screen densities, and locales. And you need it to be fast, so your players don‚Äôt end up furious.&lt;/p&gt;
&lt;p&gt;With Anbox Cloud, you can spin up multiple configurations in parallel, simulate different regions, and let your QA team jump into live sessions: no APK installs, no physical devices, just a browser and a link.&lt;/p&gt;
&lt;p&gt;Test your UI flow. Click through quests. Break things early.&lt;/p&gt;
&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="" height="960" loading="lazy" sizes="(min-width: 1280px) 1280px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1280/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F0b34%2Fimage.png" srcset="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F0b34%2Fimage.png 460w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F0b34%2Fimage.png 620w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F0b34%2Fimage.png 1036w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1681/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F0b34%2Fimage.png 1681w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1920/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F0b34%2Fimage.png 1920w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1280/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F0b34%2Fimage.png 1280w" width="1280"/&gt;&lt;/figure&gt;
&lt;h1 class="wp-block-heading"&gt;Why it matters&lt;/h1&gt;
&lt;p&gt;In a world where users bounce in seconds, localized quality is a differentiator. A misaligned button or clipped string may seem minor, but users notice. And they judge.&lt;/p&gt;
&lt;p&gt;In automotive, UI glitches aren‚Äôt just annoying, they‚Äôre a risk. In gaming, they represent potential lost revenue.&lt;/p&gt;
&lt;p&gt;Anbox Cloud brings everything together: faster feedback, real-time testing, and zero APK distribution. Everything runs in the cloud. Everything stays under your control.&lt;/p&gt;
&lt;figure class="wp-block-image size-full"&gt;&lt;a href="https://ubuntu.com/engage/android-apps-streaming"&gt;&lt;img alt="" height="275" loading="lazy" sizes="(min-width: 1600px) 1600px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1600/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F6a2b%2Fimage.png" srcset="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F6a2b%2Fimage.png 460w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F6a2b%2Fimage.png 620w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F6a2b%2Fimage.png 1036w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1681/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F6a2b%2Fimage.png 1681w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1920/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F6a2b%2Fimage.png 1920w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1600/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F6a2b%2Fimage.png 1600w" width="1600"/&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;h1 class="wp-block-heading"&gt;Automation that scales with you&lt;/h1&gt;
&lt;p&gt;Localization QA isn‚Äôt just a manual task. With the right tooling, it becomes part of your CI: fast, repeatable, and invisible when it works.&lt;/p&gt;
&lt;p&gt;Canonical provides an official GitHub Action to spin up Anbox Cloud dynamically in your pipeline. That means you can launch full Android containers, connect via ADB, and run tests automatically, all from a GitHub workflow. No emulators: just Android, on demand.&lt;/p&gt;
&lt;p&gt;Spin up a fresh Android container, connect to it using &lt;a href="https://documentation.ubuntu.com/anbox-cloud/howto/android/access-android-instance/"&gt;anbox-connect&lt;/a&gt;, and run your UI tests across configurations and locales. The same &lt;a href="https://documentation.ubuntu.com/anbox-cloud/howto/anbox/control-ams-remotely/"&gt;amc CLI&lt;/a&gt; that developers use locally works inside your runner, letting you orchestrate test flows, parallelize across devices, or gate PRs on localization correctness.&lt;/p&gt;
&lt;p&gt;Each Android container is accurately replicated, so every test starts from a known baseline. This means that you can also run simultaneous sessions in parallel without sacrificing performance.&lt;/p&gt;
&lt;h1 class="wp-block-heading"&gt;Get started today&lt;/h1&gt;
&lt;p&gt;Whether you‚Äôre localizing a safety-critical IVI interface or pushing a mobile game to 30 markets, Anbox Cloud helps you test, adapt, and scale.&lt;/p&gt;
&lt;p&gt;And here‚Äôs the best part: Anbox Cloud is included in Ubuntu Pro, which is free for personal use and up to five machines. With GitHub integration and built-in automation, your QA process stays in sync with your development pace.&lt;/p&gt;
&lt;p&gt;Want to get hands-on? Check out our &lt;a href="https://documentation.ubuntu.com/anbox-cloud/#"&gt;official documentation&lt;/a&gt;, or get started with &lt;a href="https://documentation.ubuntu.com/anbox-cloud/howto/install-appliance/install-on-github/"&gt;the Anbox Cloud Appliance&lt;/a&gt;.¬†&lt;/p&gt;
&lt;p&gt;Curious to know how Anbox Cloud fits into your pipeline? &lt;a href="https://anbox-cloud.io/#get-in-touch"&gt;Talk to us&lt;/a&gt;.&lt;/p&gt;
&lt;h1 class="wp-block-heading"&gt;Further reading&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://documentation.ubuntu.com/anbox-cloud/#"&gt;Official documentation&lt;/a&gt;&lt;br/&gt;&lt;a href="https://documentation.ubuntu.com/anbox-cloud/howto/install-appliance/install-on-github/"&gt;Anbox Cloud Appliance&lt;/a&gt;&lt;/p&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity"/&gt;
&lt;p&gt;Android is a trademark of Google LLC. Anbox Cloud uses assets available through the Android Open Source Project.&lt;/p&gt;
</content:encoded><author>Bertrand Boisseau (Bertrand Boisseau)</author><category>Anbox</category><category>anbox cloud</category><category>Anbox Cloud Appliance</category><pubDate>Mon, 27 Oct 2025 08:00:00 +0000</pubDate></item><item><title>Canonical announces new optimized Ubuntu image for Thundercomm RUBIK Pi 3</title><link>https://ubuntu.com//blog/rubik-pi-3-thundercomm-canonical</link><description>&lt;p&gt;Ubuntu now runs natively on the Thundercomm RUBIK Pi 3 developer board ‚Äì a lightweight Pi created for AI developers which runs on the Qualcomm Dragonwing‚Ñ¢ QCS6490 processor.&lt;/p&gt;
</description><content:encoded>
&lt;h3 class="wp-block-heading"&gt;Ubuntu now runs natively on the Thundercomm RUBIK Pi 3 developer board ‚Äì a lightweight Pi created for AI developers which runs on the Qualcomm Dragonwing‚Ñ¢ QCS6490 processor.&lt;/h3&gt;
&lt;p&gt;October 23, 2025 ‚Äì Today Canonical, the publisher of Ubuntu, announced an optimized, pre-installed Ubuntu image for RUBIK Pi 3 ‚Äì a powerful AI developer board built on Dragonwing QCS6490. These new, optimized Ubuntu images reduce time to market through out-of-the-box functionality, and offer official long-term support from Canonical. The new Ubuntu image is also available to download and install for current users of RUBIK Pi 3.¬†&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;A rapid AI development platform, powered by Ubuntu&lt;/strong&gt;&lt;/h2&gt;
&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="" height="1543" loading="lazy" sizes="(min-width: 2676px) 2676px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_2676/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F5a97%2F340.png" srcset="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F5a97%2F340.png 460w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F5a97%2F340.png 620w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F5a97%2F340.png 1036w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1681/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F5a97%2F340.png 1681w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1920/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F5a97%2F340.png 1920w" width="2676"/&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Credit: &lt;a href="https://www.thundercomm.com/product/rubik-pi/"&gt;Thundercomm&lt;/a&gt;&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;AI is a fast-paced industry, and time is of the essence when launching new products. These optimized Ubuntu images offer developers on RUBIK Pi 3 access to cutting-edge open source, combined with the stability and robustness that Ubuntu is known for. These new images complement RUBIK Pi 3‚Äôs ease of use and accessibility. The images work seamlessly out of the box, and are fine-tuned for hardware performance and resource efficiency.¬†&lt;/p&gt;
&lt;p&gt;‚ÄúBy delivering an optimized Ubuntu image preloaded on the powerful RUBIK Pi 3, we‚Äôre offering an integrated, securely designed, and supported foundation for AI developers, said Cindy Goldberg, VP of Silicon Alliances at Canonical. ‚ÄúOur partnership with Qualcomm and Thundercomm allows developers to move from a concept to a deployed solution with speed and confidence.‚Äù¬† ¬†&lt;/p&gt;
&lt;p&gt;‚ÄúAt Thundercomm, we‚Äôre committed to lowering the barriers to AI innovation,‚Äù said Ali Mesri, Sr Vice President, Business Development at Thundercomm. ‚ÄúWith the optimized Ubuntu image on RUBIK Pi 3, developers now have a unified platform that combines Qualcomm‚Äôs performance, Canonical‚Äôs stability, and Thundercomm‚Äôs deep system integration ‚Äî enabling faster, more reliable AI deployment from concept to production.‚Äù&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Powerful features on an accessible board&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;RUBIK Pi 3 is designed to make innovative hardware more accessible to AI developers. It offers a full-stack, end-to-end solution that is performant at low power. RUBIK Pi 3 consumes less than 6.5W, whilst at the same time being equipped with a 12 tops ML accelerator, 8-core GPU, integrated Wi-Fi and Bluetooth, 8GB LPDDR4x RAM, and 128GB UFS 2.2 storage.&lt;/p&gt;
&lt;p&gt;Built on Dragonwing QCS6490, RUBIK Pi 3 includes access to the Qualcomm&lt;sup&gt;¬Æ&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;¬†AI Hub with pre-optimized models, and access to the Edge Impulse MLOps platform for training and deployment. Canonical‚Äôs new optimized Ubuntu images are the latest development RUBIK Pi 3 offers ‚Äì see how everything fits together in the table below:&lt;/p&gt;
&lt;figure class="wp-block-table"&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Feature&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Optimized Ubuntu images&lt;/td&gt;&lt;td&gt;Prototype, test and deploy edge AI solutions faster with the world‚Äôs most popular enterprise Linux.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Edge AI silicon&lt;/td&gt;&lt;td&gt;High-performance, low-power AI chips.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Qualcomm AI hub&lt;/td&gt;&lt;td&gt;Pre-optimized models for vision, audio, and NLP.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;IMSDK&lt;/td&gt;&lt;td&gt;Intelligent Multimedia SDK, for developing HW accelerated multimedia and AI applications.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;QIRP SDK&lt;/td&gt;&lt;td&gt;Qualcomm&lt;sup&gt;¬Æ¬† &lt;/sup&gt;Intelligent Robotics Product SDK for developing Robotics applications with ROS/ROS2.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Containers&lt;/td&gt;&lt;td&gt;Containerized SDKs and applications with access to HW acceleration (such as NPU, GPU, and VZPU).&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Integrated developer environment&lt;/td&gt;&lt;td&gt;Qualcomm&lt;sup&gt;¬Æ &lt;/sup&gt;VSCode IDE Extensions to simplify device setup and application development environment&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt; &lt;a aria-label=" (opens in a new tab)" class="p-button--small p-button--positive" href="https://ubuntu.com/download/qualcomm-iot#rubikpi3" rel="noreferrer noopener" target="_blank"&gt;Download the optimized Ubuntu image for Rubik Pi 3&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Visit the &lt;a href="https://rubikpi.ai/"&gt;Thundercomm RUBIK Pi3 page&lt;/a&gt; to get your board and start building with the optimized Ubuntu image today. &lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;About Canonical¬†&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Canonical, the publisher of Ubuntu, provides open source security, support and services. Our portfolio covers critical systems, from the smallest devices to the largest clouds, from the kernel to containers, from databases to AI. With customers that include top tech brands, emerging startups, governments and home users, Canonical delivers trusted open source for everyone. Learn more at&lt;a href="https://canonical.com/"&gt; https://canonical.com/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Qualcomm branded products are products of Qualcomm Technologies, Inc. and/or its subsidiaries. Qualcomm patents are licensed by Qualcomm Incorporated.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Qualcomm is a trademark or registered trademark of Qualcomm Incorporated.&lt;/em&gt;&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Further reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://canonical.com/blog/qualcomm-and-canonical-announce-strategic-collaboration"&gt;Canonical announces collaboration with Qualcomm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://canonical.com/blog/canonical-announces-public-beta-of-optimized-ubuntu-image-for-qualcomm-iot-platform"&gt;Canonical announces public beta of optimized Ubuntu image for Qualcomm IoT platforms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://canonical.com/blog/ubuntu-ga-for-qualcomm-dragonwing"&gt;Canonical launches general availability of Ubuntu for Qualcomm Dragonwing Platforms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content:encoded><author>Canonical (Canonical)</author><pubDate>Thu, 23 Oct 2025 15:40:00 +0000</pubDate></item><item><title>Introducing Canonical Academy</title><link>https://ubuntu.com//blog/introducing-canonical-academy</link><description>&lt;p&gt;Validate your skills and advance your career with recognized qualifications from the publishers of Ubuntu. Canonical today announced the launch of Canonical Academy, a new platform that enables individuals and enterprises to validate their open source skills with qualifications designed and maintained by the engineers behind Ubuntu. &lt;/p&gt;
</description><content:encoded>
&lt;h3 class="wp-block-heading"&gt;Validate your skills and advance your career with recognized qualifications from the publishers of Ubuntu&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;London, October 23 2025&lt;/strong&gt; ‚Äì Canonical today announced the launch of Canonical Academy, a new platform that enables individuals and enterprises to validate their open source skills with qualifications designed and maintained by the engineers behind Ubuntu. The first available track is the SysAdmin track, which includes four exams that test practical expertise with Linux and Ubuntu. Successful candidates can earn digital badges that prove their ability to employers and peers.&lt;/p&gt;
&lt;p&gt;Canonical Academy exams are designed to prepare learners for the real world, with modular, self-paced assessments that fit into busy schedules. The SysAdmin track launches with three exams today:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Using Linux Terminal&lt;/strong&gt; ‚Äì available now to the public&lt;br/&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Using Ubuntu Desktop&lt;/strong&gt; ‚Äì available in beta for community testing&lt;br/&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Using Ubuntu Server&lt;/strong&gt; ‚Äì available in beta for community testing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br/&gt;The fourth exam is currently in development and will be announced soon. To achieve the SysAdmin qualification, users must earn all the badges in the corresponding SysAdmin track.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Real-world exams, built by Ubuntu experts&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Traditional certifications can leave gaps between theory and practice. Canonical Academy takes a different approach, creating assessments based on the challenges IT professionals face every day. With the launch of the SysAdmin track, candidates can prove their ability to navigate the Linux terminal, configure Ubuntu desktops, and manage servers in environments that mirror the workplace. New qualification tracks are also in development to broaden the exam portfolio.¬†&lt;/p&gt;
&lt;blockquote class="wp-block-quote"&gt;
&lt;p&gt;&lt;em&gt;‚ÄúCanonical Academy grounds its qualifications in realistic professional applications. Current, technical professional needs are the foundation of¬† our development process, from the earliest identification of critical job skills for the target occupation, to the design of realistic custom cloud exam environments, to the industry expert code review of every hands-on item.‚Äù&lt;/em&gt;&lt;/p&gt;
&lt;cite&gt;&lt;span data-huuid="12952881989412452864"&gt;&lt;span&gt;‚Äì&lt;/span&gt;&lt;/span&gt; Adrianna Frick, Academy Team Lead, Canonical&lt;/cite&gt;&lt;/blockquote&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Modular and self-driven learning&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Canonical Academy is designed to fit the needs of modern learners. Each exam is modular, meaning professionals can progress at their own pace while building towards an overarching qualification badge.¬† The system allows individuals to focus on the skills they need most, while enterprises can map training to specific roles.&lt;/p&gt;
&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="" height="513" loading="lazy" sizes="(min-width: 513px) 513px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_513/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F87bf%2FAsset-125%402x.png" srcset="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F87bf%2FAsset-125%402x.png 460w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F87bf%2FAsset-125%402x.png 620w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_513/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F87bf%2FAsset-125%402x.png 513w" width="513"/&gt;&lt;/figure&gt;
&lt;p&gt;&lt;em&gt;Badges indicate the year of the Ubuntu LTS release the exam is based on. All current exams are aligned with Ubuntu 24.04 LTS. Updated exams for Ubuntu 26.04 LTS are expected in September 2026.¬†&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Each exam is supported by a study guide based on the official exam content, helping test takers prepare effectively through guided, self-paced learning.&lt;/p&gt;
&lt;blockquote class="wp-block-quote"&gt;
&lt;p&gt;‚Äú&lt;em&gt;The ‚ÄòUsing Linux Terminal‚Äô qualification provided me with a well-rounded understanding of the Ubuntu ecosystem. I‚Äôm enthusiastic about bringing this program to Indonesia to empower local talent with the open source skills that are increasingly in demand across the industry.&lt;/em&gt;‚Äú&lt;/p&gt;
&lt;cite&gt;&lt;span data-huuid="12952881989412452864"&gt;&lt;span&gt;‚Äì&lt;/span&gt;&lt;/span&gt; &lt;em&gt;Safira Zahira, Product Manager, Sivali Cloud Technology&lt;/em&gt;&lt;/cite&gt;&lt;/blockquote&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Proof of skill that employers can trust&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;With Canonical Academy, successful candidates receive verifiable digital badges that demonstrate open source competence. Backed by Canonical, these credentials provide credible evidence of technical ability in a competitive job market.&lt;/p&gt;
&lt;blockquote class="wp-block-quote"&gt;
&lt;p&gt;&lt;em&gt;‚ÄúThe test and the entire in-web-browser desktop environment was super cozy. I think the exam is incredibly valuable and will be a fantastic resource to a lot of aspiring and experienced Ubuntu users worldwide. Frankly, I thought I‚Äôd nail the Using Linux Terminal exam in 30 minutes.¬† But instead I was sweating.‚Äù&lt;/em&gt;&lt;/p&gt;
&lt;cite&gt;&lt;span data-huuid="12952881989412452864"&gt;&lt;span&gt;‚Äì&lt;/span&gt;&lt;/span&gt; Nathan Haines, Community Council Member, Ubuntu&lt;/cite&gt;&lt;/blockquote&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Get started today&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;Using Linux Terminal&lt;/strong&gt; exam is open to everyone today.¬†&lt;/p&gt;
&lt;p&gt;If you are looking to contribute to Canonical Academy, you can sign up to be a Subject Matter Expert (SME) or a beta tester. As a subject matter expert, you‚Äôll help define topics, advise on future content, and guide the direction of open source skills assessments. As a tester, you‚Äôll preview and critique exams in development, shape the user experience, and access full exams at discounted rates.&lt;/p&gt;
&lt;a href="https://canonical.com/academy"&gt;&lt;p class="p-button--positive"&gt;Learn more&lt;/p&gt;&lt;/a&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;About Canonical&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Canonical, the publisher of Ubuntu, provides open source security, support and services. Our portfolio covers critical systems, from the smallest devices to the largest clouds, from the kernel to containers, from databases to AI. With customers that include top tech brands, emerging startups, governments and home users, Canonical delivers trusted open source for everyone.¬†&lt;/p&gt;
&lt;p&gt;Learn more at &lt;a href="https://canonical.com/"&gt;https://canonical.com/&lt;/a&gt;¬†&lt;/p&gt;
</content:encoded><author>Canonical (Canonical)</author><category>Canonical Academy</category><pubDate>Thu, 23 Oct 2025 15:00:00 +0000</pubDate></item></channel></rss>