<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Ubuntu blog</title><link>https://ubuntu.com//blog/feed</link><description>Ubuntu blog feed</description><atom:link href="https://ubuntu.com//blog/feed" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>Python Feedgen</generator><lastBuildDate>Thu, 19 Feb 2026 12:07:02 +0000</lastBuildDate><item><title>Predict, compare, and reduce costs with our S3 cost calculator</title><link>https://ubuntu.com//blog/predict-compare-and-reduce-costs-with-our-s3-cost-calculator</link><description>&lt;p&gt;Previously I have written about how useful public cloud storage can be when starting a new project without knowing how much data you will need to store.&amp;nbsp; However, as datasets grow&amp;nbsp; over time, the costs of public cloud storage can become overwhelming.&amp;nbsp; This is where an on premise, or co-located, self-hosted storage system becomes advantageous: [&amp;hellip;]&lt;/p&gt;
</description><content:encoded>
&lt;p&gt;Previously I have &lt;a href="https://ubuntu.com/blog/cloud-storage-pricing-how-to-optimise-tco"&gt;written&lt;/a&gt; about how useful public cloud storage can be when starting a new project without knowing how much data you will need to store.  However, as datasets grow  over time, the costs of public cloud storage can become overwhelming.  This is where an on premise, or co-located, self-hosted storage system becomes advantageous: it provides the greatest range of benefits, including cost, performance, security, and data sovereignty.  In this article we will briefly cover the storage use cases that might be suitable for storing on your own storage system, and what the cost savings could look like.&lt;/p&gt;
&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="" height="3123" loading="lazy" sizes="(min-width: 3868px) 3868px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_3868/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fdaa5%2FArtboard-133%404x.png" srcset="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fdaa5%2FArtboard-133%404x.png 460w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fdaa5%2FArtboard-133%404x.png 620w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fdaa5%2FArtboard-133%404x.png 1036w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1681/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fdaa5%2FArtboard-133%404x.png 1681w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1920/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fdaa5%2FArtboard-133%404x.png 1920w" width="3868"/&gt;&lt;/figure&gt;
&lt;h2 class="wp-block-heading"&gt;Growing storage workloads&lt;/h2&gt;
&lt;p&gt;Cloud computing services such as AWS S3, Azure Blob, and GCP GCS provide immediate access to compute, storage, and networking resources, which allow you scale up and down depending on current and future needs.  This can be critical for when you have sudden major increases in use, for example at product launches or real-life events that spike usage, such as sporting events. Once the spike has dropped back to normal, you can scale resources back to typical levels.  However storage usage tends to be a little different: while the rate of access (or IO) may change for peaks in workloads, most datasets only grow in capacity.  &lt;/p&gt;
&lt;p&gt;For example, datasets like an archive or media repository typically consume the greatest amount of storage, have well-understood growth rates, and are very unlikely to ever decrease in size, as the data they hold is of some reasonable importance.  &lt;/p&gt;
&lt;p&gt;Sometimes datasets like these are also seldom accessed, but it is important that the data remains readily accessible.  While public clouds do provide different classes of storage service, offering reduced performance or significantly increased retrieval times, at a lower cost per GB, a self-hosted solution can still be more cost effective, depending on your use case.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;A different way&lt;/h2&gt;
&lt;p&gt;For data sets that have predictable growth rates and therefore do not need the burstability of public cloud storage, a more cost-effective approach is to utilize an on-premise or co-located storage system.  The location is important: you should maintain close proximity with the majority of users. This could be in an office, or a co-location facility near to a public cloud where you utilize compute services.&lt;/p&gt;
&lt;p&gt;Building your own storage system (with the help of &lt;a href="https://ubuntu.com/ceph/managed"&gt;Canonical&lt;/a&gt;, should you need it) puts you in control of the hardware used in that storage system, when and how it should grow, and also provides you with greater control over a core business asset:data.&lt;/p&gt;
&lt;p&gt;In our &lt;a href="https://ubuntu.com/ceph/pricing-calculator"&gt;S3 cost calculator&lt;/a&gt;, you can get an indication of the total cost of ownership (TCO) for various capacities, in both fully managed and self-operated configurations, and for different project durations.  For fairness, list prices have been used for both self-hosted and public cloud configurations, however it is commonplace for organizations to negotiate with vendors.&lt;/p&gt;
&lt;p&gt;Generally, self-hosted solutions give the most advantageous TCO for longer periods of time – and they can even impart significant benefits (such as a quick return on investment) to short-term projects, when there is a large enough data set.  To see this in action, try our &lt;a href="https://ubuntu.com/ceph/pricing-calculator"&gt;TCO calculator&lt;/a&gt;.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Cloud storage cost comparison calculator&lt;/h2&gt;
&lt;figure class="wp-block-image size-full"&gt;&lt;a href="https://ubuntu.com/ceph/pricing-calculator"&gt;&lt;img alt="" height="1574" loading="lazy" sizes="(min-width: 2242px) 2242px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_2242/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F4116%2FScreenshot-2026-02-16-at-16.37.48.png" srcset="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F4116%2FScreenshot-2026-02-16-at-16.37.48.png 460w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F4116%2FScreenshot-2026-02-16-at-16.37.48.png 620w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F4116%2FScreenshot-2026-02-16-at-16.37.48.png 1036w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1681/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F4116%2FScreenshot-2026-02-16-at-16.37.48.png 1681w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1920/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F4116%2FScreenshot-2026-02-16-at-16.37.48.png 1920w" width="2242"/&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;p&gt;To recap: a self-hosted storage solution can provide a complementary solution to growing storage needs in the public cloud, with the main focus on cost control. However, as you would also retain full control over the solution and your data, there is an improved security and sovereignty posture too.&lt;/p&gt;
&lt;p&gt;For a deep dive into the considerations and best practices you need to think about when designing and deploying your self-hosted storage solutions, you can read our dedicated and detailed whitepaper. It intensively breaks down the economics of your options, explains hardware selection and networking (using private interconnects) using a practical 2.5PB dataset as an example. &lt;/p&gt;
&lt;p&gt;&lt;a href="https://ubuntu.com/engage/optimize-cloud-storage-costs"&gt;Download the whitepaper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can also see this theory in action by watching our hands-on webinar, where we walk you through this process in person, step-by-step, to build a truly cost-effective self-hosted storage solution. &lt;/p&gt;
&lt;p&gt;&lt;a href="https://ubuntu.com/engage/optimize-cloud-storage-costs"&gt;Watch the webinar&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you have any questions, or want to reach out to discuss cost-optimization for your custom storage solution, don’t hesitate to &lt;a href="https://ubuntu.com/ceph/contact-us"&gt;reach out to our team&lt;/a&gt;.&lt;/p&gt;
</content:encoded><author>Philip Williams (Philip Williams)</author><category>ceph</category><category>Storage</category><pubDate>Wed, 18 Feb 2026 10:08:59 +0000</pubDate></item><item><title>A year of documentation-driven development</title><link>https://ubuntu.com//blog/a-year-of-documentation-driven-development</link><description>&lt;p&gt;For many software teams, documentation is written after features are built and design decisions have already been made. When that happens, questions about how a feature is understood or used often don’t surface until much later.&amp;nbsp; A little over one year ago, our team began to recognize this pattern in our own work. Features generally [&amp;hellip;]&lt;/p&gt;
</description><content:encoded>
&lt;p&gt;For many software teams, documentation is written after features are built and design decisions have already been made. When that happens, questions about how a feature is understood or used often don’t surface until much later. &lt;/p&gt;
&lt;p&gt;A little over one year ago, our team began to recognize this pattern in our own work. Features generally functioned as intended but were difficult to use or explain. Documentation lagged behind releases. Important design decisions lived mostly in conversation or code, rather than in shared artifacts that could be reviewed, challenged, or built upon by others.&lt;/p&gt;
&lt;p&gt;Documentation-driven development is the framework that grew out of these observations. In this article, I’ll describe what led us to adopt it, how it changed our day-to-day engineering work, and what we observed after a year of using it in practice.&lt;/p&gt;
&lt;p&gt;The reflections in this article are drawn from my experience working on &lt;a href="https://ubuntu.com/landscape" rel="noreferrer noopener" target="_blank"&gt;Landscape&lt;/a&gt;, a systems administration product with a long history at Canonical. Our team includes backend engineers, web engineers, documentation, product management, and UX design. Like many teams responsible for a mature product, we balance the ongoing maintenance and introduction of new features, often while collaborating with other engineering teams.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;What documentation-driven development means in practice&lt;/h2&gt;
&lt;p&gt;Documentation-driven development is an approach to software engineering that treats documentation as an integral part of the development process itself, rather than something that happens afterward. &lt;/p&gt;
&lt;p&gt;Documentation here doesn’t just mean user documentation. It includes engineering design documentation, user-facing documentation, and any internal documentation used to collaborate within the team or with other teams. In practice, this process gave our feature work a clearer sequence to follow.&lt;/p&gt;
&lt;p&gt;This is how documentation-driven development looks on our team:&lt;/p&gt;
&lt;p&gt;New features begin with a design document, which clearly and concisely describes the proposed design of the feature, including the existing state of the system, the problem being addressed, why solving it matters, and the expected design to be implemented. Writing that design documentation forced us to make our decisions explicit and early on, and opened the door to more collaborative discussion around the feature design.&lt;/p&gt;
&lt;p&gt;Once the design is agreed upon, drafting the user documentation begins before any code is written. This stage forces engineers to closely engage with the user experience. If a feature is difficult to explain cleanly, that reflects something unresolved in the design itself.&lt;/p&gt;
&lt;p&gt;As development progresses, code and documentation continue to evolve together. The user documentation should reflect the current state of the feature, and the act of writing the documentation continues to inform the implementation decisions. &lt;/p&gt;
&lt;p&gt;By the time the feature is complete, the documentation is complete as well, not because it was rushed at the end, but because it helped shape the work along the way.&lt;/p&gt;
&lt;p&gt;Documentation-driven development isn’t an effort to improve user documentation in isolation. It’s a way of viewing the user experience as a core concern of the product and engineering practice.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Before we adopted it: What wasn’t working&lt;/h2&gt;
&lt;p&gt;Before we adopted documentation-driven development, feature development followed a general pattern that may feel familiar to many engineers: a need would surface, some members of the team would talk through an approach, informal notes or partially-completed design documentation might get recorded, and implementation would begin.&lt;/p&gt;
&lt;p&gt;It’s not that feature design work &lt;em&gt;didn’t&lt;/em&gt; happen, but much of it was worked out in conversation or reflected in the code. User documentation, when it existed, was written near the end of development and often wasn’t ready by the time the feature was complete.&lt;/p&gt;
&lt;p&gt;There wasn’t a consistent point where we tried to articulate the user experience as a whole. Early discussions centered around the software behavior and feasibility; often, these decisions came as we were problem solving, rather than defining beforehand how we would solve those problems. Questions about how a feature would be discovered, understood, or used by our end-users often surfaced quite late, when revisiting the feature design was difficult.&lt;/p&gt;
&lt;p&gt;Over time the negative effects of the previous system became clear: missing documentation, confusing edge cases, and feedback or ideas that arrived after the feature had already assumed a fixed shape.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;The major shift: Documentation as part of engineering&lt;/h2&gt;
&lt;p&gt;One of the first effects I observed after our team adopted documentation-driven development was that it changed how engineers interacted with their own assumptions about new features. Writing documentation earlier in the process made gaps in design and implementation more visible. While it’s possible to implement a feature without fully resolving edge cases or ambiguities, it’s much harder to &lt;em&gt;explain&lt;/em&gt; a feature without noticing the areas that are unclear.&lt;/p&gt;
&lt;p&gt;This shift was especially apparent during the feature design stage. Design documents became stable points of reference that informed every stage of development. Formalizing the role of design documentation transformed our approach to development and the character of our discussions. Conversations were less likely to only revolve around the internal structure of the feature, and they were more likely to include questions about how the feature would be experienced by our users. Because design documentation made assumptions and tradeoffs explicit, it also provided a stronger foundation for building user documentation later.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Example: Avoiding a costly mistake&lt;/h2&gt;
&lt;p&gt;At one point during the year, we were designing a feature that was closely related to the work done by another team and to the future integration with their system. Our original approach involved Landscape taking on more ownership over managing user identities, which is an area that’s relatively new for us as a product.&lt;/p&gt;
&lt;p&gt;We wrote a detailed design document and created the accompanying UX mockups, carefully considering the implementation details, user experience, and longer-term plans for Landscape. Since this work intersected closely with the work done by an internal identity management team, we shared this document with them for review. &lt;/p&gt;
&lt;p&gt;The identity management team’s input had two important effects. First, they brought domain knowledge about identity management. This helped us see that the approach we were proposing would be complicated to maintain over time, and would be a burden on our team’s capacity. Second, they pointed out that our design could complicate future integration with their system because it was misaligned with their product’s plans. For example, our proposed database schema wasn’t compatible with their data model, and making them compatible would be too large of an effort at this stage.&lt;/p&gt;
&lt;p&gt;As a result of this feedback, we ended up reevaluating the feature itself, and later chose not to implement that design as proposed. We substantially reduced the scope of the feature, and avoided spending time on unnecessary work that was complex and would’ve needed rework in the future. &lt;/p&gt;
&lt;p&gt;Without documentation-driven development, we likely wouldn’t have explored the design in this level of depth or produced a document substantial enough to support this kind of review. The feature we chose not to build was a direct result of treating documentation as a fundamental part of our engineering process.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;What changed over time&lt;/h2&gt;
&lt;p&gt;As documentation-driven development became part of our normal workflow, several changes followed.&lt;/p&gt;
&lt;h3 class="wp-block-heading"&gt;User documentation as part of the development process&lt;/h3&gt;
&lt;p&gt;User documentation became an integral part of feature development, rather than something produced at the end. Because it was written and revised alongside the code, engineers were more engaged with the documentation and more invested in how features would be understood by our users. Over time, this engagement led to a noticeable improvement in quality. The documentation became clearer, more complete, and better aligned with how the product actually behaved.&lt;/p&gt;
&lt;p&gt;That engagement also had some secondary effects. Engineers also became more familiar with our documentation as a whole, not because they were instructed to be, but because they contributed to it and understood it. They were more likely to notice gaps, question existing documentation, and volunteer to make improvements.&lt;/p&gt;
&lt;h3 class="wp-block-heading"&gt;Clearer artifacts and better collaboration&lt;/h3&gt;
&lt;p&gt;Collaboration across different roles on our team improved as well, largely because we were producing higher-quality documentation earlier in the process. Having clearer, more complete designs and user experiences written down made it easier to share work for review without requiring deep prior context about Landscape or its codebase. Discussions between backend engineering, web engineering, documentation, product management, and UX design were grounded in a shared understanding of the feature, rather than summaries or recollection. It became easier to align on what a feature was meant to do, why it was meant to do that, and what the user experience would be.&lt;/p&gt;
&lt;h3 class="wp-block-heading"&gt;A shift in team culture&lt;/h3&gt;
&lt;p&gt;And one of the most significant changes I observed was in our team culture. Documentation, design, and the user experience became easier to talk about because they became part of our everyday engineering work rather than peripheral concerns. The perspective across our team changed: improving the user-facing components of our product became a shared &lt;em&gt;practice&lt;/em&gt;, not just a shared &lt;em&gt;value&lt;/em&gt;.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Limitations&lt;/h2&gt;
&lt;p&gt;Although we experienced many positive outcomes from adopting documentation-driven development, it didn’t address &lt;em&gt;every&lt;/em&gt; problem. The process is designed for new feature development, so it didn’t directly resolve existing issues in our documentation or its ongoing maintenance challenges.&lt;/p&gt;
&lt;p&gt;First of all, adoption was gradual and uneven. Our team was quick to adopt the design documentation stage, but drafting user-facing documentation early in the process required a larger shift in how we were used to working. It took time for that part of the practice to feel natural and useful, and it didn’t take hold at the same time for each member of the team.&lt;/p&gt;
&lt;p&gt;We also learned that the process itself needed some flexibility. Not every feature warranted the same depth of documentation at each stage: some features benefitted more from deeper design work, while others were more complex in the user documentation and implementation stages. We iterated on our process as we went, adjusting how we applied it to different kinds of challenges rather than following it rigidly.&lt;/p&gt;
&lt;p&gt;But along the way, something quieter happened. Adapting the new process to fit our needs as we went gave the team shared experience in tackling unfamiliar problems and learning to work through uncertainty together.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Looking ahead&lt;/h2&gt;
&lt;p&gt;Documentation-driven development wasn’t the solution to all our problems, but it did give us a foundation to build from. By making our user experience and feature design part of our everyday engineering work, it created conditions for other practices to grow, and as a result, the quality of our work is now much higher. Designs are more deliberate and better thought-out before implementation begins, and new user documentation is more clearly written for its audience, more closely aligned with the behavior of the product, and ready when new features are complete. We’ve also been able to build on this foundation by introducing practices like user-story mapping in our engineering work, drawing on the same attention to processes and user journeys that documentation-driven development helped establish.&lt;/p&gt;
&lt;p&gt;One year in, documentation-driven development has helped us build a shared process and language for thinking about our product, code, documentation, and the user experience together. Our team practices and product continue to evolve, but now with more clarity, a stronger connection with how users engage with what we build, and fewer assumptions left unexamined.&lt;/p&gt;
</content:encoded><author>Yanisa Haley Scherber (Yanisa Haley Scherber)</author><category>documentation</category><category>engineering</category><category>Landscape</category><pubDate>Tue, 17 Feb 2026 19:21:45 +0000</pubDate></item><item><title>Announcing FIPS 140-3 for Ubuntu Core22</title><link>https://ubuntu.com//blog/announcing-fips-140-3-for-ubuntu-core22</link><description>&lt;p&gt;FIPS compliance for IoT use cases in Federal space. In this article, we’ll explore what Ubuntu Core is, and how to use it with FIPS.&lt;/p&gt;
</description><content:encoded>
&lt;p&gt;With the release of the &lt;a href="https://ubuntu.com/blog/fips-140-3-for-ubuntu-22-04lts"&gt;FIPS 140-3 certified cryptographic modules&lt;/a&gt; for Ubuntu 22.04 LTS, Canonical is building on its long tradition of enabling customers to deploy Ubuntu in the US Federal marketplace. FIPS 140-3 is a NIST standard that describes how to use cryptography securely, which includes a rigorous certification process to ensure that the implementation is correct. The Ubuntu 22.04 LTS crypto libraries have been through this process and are now available to use with an&lt;a href="https://ubuntu.com/pro"&gt; Ubuntu Pro subscription&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As part of this certification process, we have also included &lt;a href="https://ubuntu.com/core"&gt;Ubuntu Core&lt;/a&gt; as a fully certified Operating Environment for the first time, starting with Ubuntu Core 22, and will continue to support Ubuntu Core with future FIPS certifications. In this article, we’ll explore what Ubuntu Core is, and how to use it with FIPS.&lt;/p&gt;
&lt;p&gt;FIPS is available with an &lt;a href="https://ubuntu.com/pro"&gt;Ubuntu Pro subscription&lt;/a&gt;, along with enhanced security patching for up to 10 years across the Ubuntu software ecosystem, kernel Livepatch, the &lt;a href="http://www.ubuntu.com/landscape"&gt;Landscape asset management tool&lt;/a&gt; and more. Ubuntu Pro is free for personal use on up to 5 machines, with &lt;a href="https://ubuntu.com/pricing/devices"&gt;individual pricing&lt;/a&gt; for device manufacturers.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;What is Ubuntu Core?&lt;/h2&gt;
&lt;p&gt;Ubuntu Core is a minimal, containerized, and immutable version of Ubuntu designed for IoT, embedded devices, and appliances, built using snap packages. Snaps are Linux app packages that are self-contained, rigorously secure, and dependency-free.&lt;/p&gt;
&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="304" loading="lazy" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/3C3Y0xVf8zw?feature=oembed" title="Ubuntu Core 24 LTS | The best OS for embedded devices" width="540"&gt;&lt;/iframe&gt;
&lt;/div&gt;&lt;/figure&gt;
&lt;p&gt;At the heart of Ubuntu Core is a robust security model: every snap package is confined using strict sandboxing with digital signatures, and is fully transactional, enabling system rollback, self-healing, and updates with zero-touch deployment. The OS itself is immutable and read-only, and updates are delivered atomically, reducing operational risk and downtime. Ubuntu Core’s security features, such as secure boot, full disk encryption, and measured boot capabilities, enable device makers to easily deploy a hardware-rooted chain of trust and ensure system integrity from power-on to runtime.&lt;/p&gt;
&lt;p&gt;Once developers have built and validated their snap application in their preferred environment, they &lt;a href="https://documentation.ubuntu.com/core/tutorials/build-your-first-image/"&gt;create their production Ubuntu Core image&lt;/a&gt; that includes only the necessary snaps they need for their targeted application. Their production Ubuntu Core image can then be deployed to devices, reducing provisioning time in the manufacturing line while increasing assurance of reproducibility, confinement, and long-term support through up to 15 years of security maintenance. Ubuntu Core is designed for the full lifecycle of a device and integrates seamlessly with fleet management and automation tools, supporting scalable deployment and maintenance across a wide range of mission-critical use cases.&lt;/p&gt;
&lt;p&gt;Ubuntu Core is &lt;a href="https://ubuntu.com/core/stories"&gt;gaining significant traction&lt;/a&gt; across the IoT and Industrial sectors, with deployments in manufacturing, home automation, agriculture, retail supply chain, robotics, and digital signage.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;How to build snaps for Ubuntu Core&lt;/h2&gt;
&lt;p&gt;Canonical has put together a comprehensive toolchain called &lt;a href="https://documentation.ubuntu.com/snapcraft/stable/"&gt;snapcraft&lt;/a&gt; for building snaps in a clear and consistent way. Snapcraft mandates a simple yaml file declaration to describe how to build a snap, and it includes numerous &lt;a href="https://documentation.ubuntu.com/snapcraft/stable/reference/plugins/"&gt;plugins for common build environments&lt;/a&gt; such as C/C++, Python, Golang, CMake, and many more.&lt;/p&gt;
&lt;p&gt;Snaps are intended to be completely standalone apps that don’t require a litany of dependencies to be installed on the system. There is, however, a common set of system libraries that are bundled together in the “base” snap that is available to all other snaps on the system – this base snap is built from the corresponding Ubuntu LTS distribution, and is called core22 (the number reflects the Ubuntu LTS version, so core22 is built from Ubuntu 22.04 LTS).&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;What is a FIPS-enabled snap?&lt;/h2&gt;
&lt;p&gt;As part of the &lt;a href="https://ubuntu.com/blog/fips-140-3-for-ubuntu-22-04lts"&gt;FIPS certification process for Ubuntu 22.04&lt;/a&gt;, we have also included the userspace cryptographic libraries within the core22 base snap (OpenSSL, libgcrypt, GnuTLS). The version of this core22 snap with the FIPS libraries is available from the snap store within the &lt;strong&gt;fips-updates/stable&lt;/strong&gt; channel. You can see which channels are available for a particular snap using the &lt;strong&gt;snap info &amp;lt;snap&amp;gt;&lt;/strong&gt; command, e.g. &lt;strong&gt;snap info core22&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Just as with classic Ubuntu 22.04 LTS FIPS, these modules require a FIPS kernel to be installed in order to provide a FIPS-validated source of entropy (random numbers). The FIPS kernel sets a dedicated flag in the proc filesystem to indicate that it’s running in FIPS mode, and the crypto modules detect this and thus run in FIPS mode as well:&lt;/p&gt;
&lt;p&gt;$ cat /proc/sys/crypto/fips_enabled&lt;/p&gt;
&lt;p&gt;1&lt;/p&gt;
&lt;p&gt;This means that any application which links against the FIPS modules, such as OpenSSL, can directly use the FIPS-validated crypto algorithms without needing to undergo any modification, or additional NIST certification.&lt;/p&gt;
&lt;p&gt;It is important to note that the core22 base includes OpenSSL, libgcrypt &amp;amp; GnuTLS, so when building a snap &lt;strong&gt;you should make sure you use these libraries and don’t include extra copies in the staging steps&lt;/strong&gt;, as the snap would then not be able to function in FIPS mode.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;An example snap build&lt;/h2&gt;
&lt;p&gt;We’ve created an example snap that shows the OpenSSL crypto providers that are installed. By default, OpenSSL uses its regular “base” provider, but in FIPS mode it also has a dedicated “fips” provider. Check out the code GitHub:&lt;/p&gt;
&lt;p&gt;$ git clone https://github.com/henrycoggillcnc/fipstestsnap.git&lt;/p&gt;
&lt;p&gt;This application is built against OpenSSL – see the &lt;em&gt;CMakeLists.txt&lt;/em&gt; directive:&lt;/p&gt;
&lt;p&gt;target_link_libraries(server PUBLIC microhttpd &lt;strong&gt;crypto&lt;/strong&gt;)&lt;/p&gt;
&lt;p&gt;The snapcraft definition includes the OpenSSL development libraries, but does not stage the OpenSSL runtime libraries, as these are provided within the core22 base snap:&lt;/p&gt;
&lt;p&gt;parts:&lt;/p&gt;
&lt;p&gt;  server:&lt;/p&gt;
&lt;p&gt;    source: .&lt;/p&gt;
&lt;p&gt;    plugin: cmake&lt;/p&gt;
&lt;p&gt;    build-packages:&lt;/p&gt;
&lt;p&gt;      – libmicrohttpd-dev&lt;/p&gt;
&lt;p&gt;      – &lt;strong&gt;libssl-dev&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;    stage-packages:&lt;/p&gt;
&lt;p&gt;      – libmicrohttpd12&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Testing on classic Ubuntu in FIPS mode&lt;/h2&gt;
&lt;p&gt;Snaps can run on classic Ubuntu or Ubuntu Core (as well as many other Linux environments), and it is simplest to develop and test them on classic Ubuntu systems. In order for snaps to become available in the public snapstore they need to be reviewed by Canonical’s security team. You can also build and install snaps locally though:&lt;/p&gt;
&lt;p&gt;$ sudo snap install –dangerous –jailmode testsnap_1_amd64.snap&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;–dangerous&lt;/em&gt; flag tells snapd to not check the signature and security assertions for the snap. These are normally enabled through the snapstore publishing process. The &lt;em&gt;–jailmode&lt;/em&gt; flag tells snapd to still enforce the snap’s security confinement (&lt;em&gt;–dangerous&lt;/em&gt; also disables security confinement). Again, this is needed for testing purposes only.&lt;/p&gt;
&lt;p&gt;For FIPS mode, the system requires both the kernel and userspace libraries to be FIPS enabled. Canonical provides the necessary packages with an Ubuntu Pro subscription, and you can &lt;a href="https://documentation.ubuntu.com/pro-client/en/docs/howtoguides/enable_fips/"&gt;install them using the Pro client&lt;/a&gt;, turning the system into FIPS mode:&lt;/p&gt;
&lt;p&gt;$ sudo pro enable fips-updates&lt;/p&gt;
&lt;p&gt;Our example test snap will run on non-FIPS and FIPS systems, and it lists the OpenSSL providers that are loaded. By default it shows the “base” provider, but on FIPS-enabled systems it also shows the “fips” provider.&lt;/p&gt;
&lt;p&gt;Another easy method to test applications on FIPS systems is to &lt;a href="https://documentation.ubuntu.com/multipass/en/latest/how-to-guides/manage-instances/create-an-instance/"&gt;use multipass&lt;/a&gt;. Launch an Ubuntu 22.04 instance, enable FIPS, install the core22 FIPS base, install the example test snap, and check the results.&lt;/p&gt;
&lt;p&gt;$ multipass launch jammy -n jammyfips -c 2 -m 2g -d 15g&lt;/p&gt;
&lt;p&gt;$ multipass shell jammyfips&lt;/p&gt;
&lt;p&gt;$ sudo pro attach &amp;lt;pro token&amp;gt;&lt;/p&gt;
&lt;p&gt;$ sudo pro enable fips-updates&lt;/p&gt;
&lt;p&gt;$ sudo reboot&lt;/p&gt;
&lt;p&gt;$ multipass transfer testsnap_1_amd64.snap jammyfips:&lt;/p&gt;
&lt;p&gt;$ multipass shell jammyfips&lt;/p&gt;
&lt;p&gt;$ sudo snap install core22 –channel fips-updates/stable&lt;/p&gt;
&lt;p&gt;$ sudo snap install –dangerous –jailmode testsnap_1_amd64.snap&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Ubuntu Core with FIPS&lt;/h2&gt;
&lt;p&gt;To &lt;a href="https://documentation.ubuntu.com/core/tutorials/build-your-first-image/"&gt;build an Ubuntu Core image&lt;/a&gt;, you begin by defining a &lt;strong&gt;model assertion&lt;/strong&gt;, which is a signed JSON document that specifies the structure of the image – including the base system, kernel, gadget, and application snaps – along with device identity and security policies. This model declares which snaps are included and how they interact. Using the ubuntu-image tool, the image is then assembled based on this model, pulling the declared snaps from the Snap Store or a specified source. The result is a reproducible, signed image that is ready for secure deployment to devices.&lt;/p&gt;
&lt;p&gt;An Ubuntu Core image is built up from 4 key snaps: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gadget snap – this defines the layout of the system and includes hardware configuration&lt;/li&gt;
&lt;li&gt;Kernel snap – the Ubuntu Linux kernel&lt;/li&gt;
&lt;li&gt;Snapd – the runtime engine for snaps and Ubuntu Core&lt;/li&gt;
&lt;li&gt;Base snap – core libraries and components that provide basic Linux functionality&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to have a fully FIPS-enabled version of Ubuntu Core, all the snaps need to be built in FIPS mode, apart from the gadget snap which doesn’t contain cryptographic functionality. Canonical has built these FIPS-enabled snaps and they are now available through your Ubuntu Pro subscription.&lt;/p&gt;
&lt;p&gt;Something to bear in mind for IoT systems is whether a custom kernel is required. The FIPS-enabled kernel that we take through the lengthy FIPS validation process is based on the stock Ubuntu kernel version that is published with each new LTS release. This means that if the stock kernel supports the hardware platform then the FIPS kernel will too; However, if the stock kernel does not support the hardware platform, then the FIPS kernel will not support it either.&lt;/p&gt;
&lt;p&gt;Canonical’s &lt;a href="https://ubuntu.com/core/services"&gt;IoT professional services&lt;/a&gt; team is on hand to guide customers through this journey, providing support, customizations, hardware enablement, and access to specialized builds such as these FIPS snaps.&lt;/p&gt;
&lt;p&gt;By engaging with our IoT team, you can get your own &lt;a href="https://documentation.ubuntu.com/core/explanation/stores/dedicated-snap-store/"&gt;Dedicated Snap Store&lt;/a&gt; in order to control the publishing and availability of your custom snap applications, manage accounts and device authentication.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Ubuntu Core is an exciting new approach to Linux security, combining the latest developments in immutability, strict confinement, and application delivery, and customers from all across the IoT world are discovering these benefits. We’re pleased to extend Canonical’s FIPS compliance capabilities to the Ubuntu Core ecosystem and enable even more usage of Core in regulated environments. Please &lt;a href="https://ubuntu.com/core/contact-us"&gt;get in touch to learn more&lt;/a&gt; about Ubuntu Core, FIPS 140, and how our IoT professional services team can help you get up and running in this exciting new world of secure and minimal Linux systems.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://documentation.ubuntu.com/core/"&gt;Ubuntu Core documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://documentation.ubuntu.com/snapcraft/stable/"&gt;Snapcraft documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ubuntu.com/security/fips"&gt;Ubuntu FIPS documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ubuntu.com/core/services"&gt;IoT professional services&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ubuntu.com/blog/fips-140-3-for-ubuntu-22-04lts"&gt;Ubuntu 22.04 LTS FIPS release&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ubuntu.com/core/stories"&gt;Ubuntu Core success stories&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content:encoded><author>Henry Coggill (Henry Coggill)</author><category>FIPS</category><category>FIPS certification</category><category>FIPS for IoT</category><category>Ubuntu Core</category><category>Ubuntu Pro</category><pubDate>Tue, 17 Feb 2026 12:30:09 +0000</pubDate></item><item><title>The foundations of software: open source libraries and their maintainers</title><link>https://ubuntu.com//blog/the-foundations-of-software-open-source-libraries-and-their-maintainers</link><description>&lt;p&gt;Open source libraries are repositories of code that developers can use and, depending on the license, contribute to, modify, and redistribute. Open source libraries are usually developed on a platform like GitHub, and distributed using package registries like PyPI for Python and npm for JavaScript. These repositories contain pre-written, re-usable code that developers use to [&amp;hellip;]&lt;/p&gt;
</description><content:encoded>
&lt;p&gt;Open source libraries are repositories of code that developers can use and, depending on the license, contribute to, modify, and redistribute. Open source libraries are usually developed on a platform like GitHub, and distributed using package registries like PyPI for Python and npm for JavaScript. These repositories contain pre-written, re-usable code that developers use to add elements or features within their software projects. Open source libraries are maintained – updated, patched, improved, and so on – by people who are known, unsurprisingly, as ‘maintainers.’&lt;/p&gt;
&lt;p&gt;That’s the simple definition, at least – but it fails to do justice to what goes on behind the scenes. The truth is that open source library maintainers are communities whose efforts support the overwhelming majority of modern software applications that we all rely on. For example, React is used widely to build user interfaces for web and mobile apps. React Native &lt;a href="https://www.ropstam.com/react-native-apps/"&gt;was used to build many popular apps&lt;/a&gt; which you may have used, including Facebook, Discord, and Airbnb. Libraries like React are sometimes supported by specific foundations, but there are hundreds, if not thousands, of libraries that rely on individual maintainers as well. &lt;/p&gt;
&lt;p&gt;You may have noticed that our YouTube channel recently began a new series, “&lt;a href="https://www.youtube.com/playlist?list=PLwFSk464RMxns9ylPfbaivj-fmIiE-Kld"&gt;Push to Talk | Meet the Maintainers.&lt;/a&gt;” The series highlights the work of prominent open source developers, like Andrew Gallant (the maintainer behind ripgrep), as well as giving a behind-the-scenes look into their journeys in the open source community. In this blog, we’re going to provide some context by peeling back the layers of open source libraries: how they work, why they’re important, and the maintainers that keep them going behind the scenes. &lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;How open source libraries work, and why they’re different from closed source libraries&lt;/h2&gt;
&lt;p&gt;Software doesn’t spring from the void. Someone – or teams of someones – have to build it in the first place, write the code, and make it run. But software is never &lt;em&gt;really&lt;/em&gt; done. Bugs appear that need patching, new functionalities are imagined or requested, performance can always be improved. Moreover, code faces an unrelenting tide of new security threats, changes to external libraries and frameworks, updates to operating systems, browsers and hardware – all of which someone needs to stay on top of to avoid code becoming obsolete or unusable. &lt;/p&gt;
&lt;p&gt;In proprietary software frameworks, broadly speaking, teams of developers are employed to do this work (building and/or maintaining software libraries). However, the library’s source code – human-readable text written in a programming language which computers translate to binary, process, and execute – can only be accessed by authorized individuals, like company employees and specific partners. The company, as the software owner, retains exclusive rights, which usually stops unauthorized viewing, modification, and redistribution of code from the closed source library. That means that only the software owners can include code from the libraries in their projects, and therefore, if software built using the closed source libraries goes wrong, users can only get support through the proprietary vendor who owns the code. &lt;/p&gt;
&lt;p&gt;Open source development frameworks work differently. Open source libraries are usually built through community effort. Depending on the license and contributor agreement, anyone can propose a contribution to the code. More often than not, anyone can request a new feature or modification, or offer bug fixes. Maintainers lead the library, managing and moderating the community of contributors and users. Depending on the library, maintainers may look both at fine details – reviewing contributed code, deciding which bits of code end up in the library, ensuring documentation is clear and accurate – as well as the big-picture decisions, like steering the direction of the open source library as a whole. &lt;/p&gt;
&lt;p&gt;The maintainers of open source libraries are usually not employed to look after the framework. Instead, the work is voluntary. In some cases, individual maintainers may support their work with sponsorships or donations. In other cases, foundations – such as the Eclipse Foundation or Linux Foundation – take a stewardship approach to open source maintenance. In these cases, the foundation acts as a guardian of the project or library, rather than an owner in the traditional sense. The foundation ensures that the project remains available and supported, which may include establishing Technical Steering Committees (TSCs) to provide strategic direction and guide the codebase, acting as a mediator for conflicts within the community regarding project development, and ensuring the code is properly licensed. Most importantly, foundations can collect and organize sponsorship, helping to buy servers, increase bandwidth, and provide grants for the contributors of the project.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Open source libraries: the backbone of modern software&lt;/h2&gt;
&lt;p&gt;Open source software can be found everywhere. &lt;a href="https://canonical.com/blog/state-of-global-open-source-2025"&gt;Our recent report, in collaboration with the Linux Foundation&lt;/a&gt;, revealed that globally, 55% of enterprises have adopted open source operating systems, whilst 49% have adopted open source cloud and container technologies, and 46% open source web and application development. Even if a project isn’t fully open source, it more likely than not contains some open source code, drawn from open source libraries: &lt;a href="https://www.blackduck.com/resources/analyst-reports/open-source-security-risk-analysis.html?utm_source=the+new+stack&amp;amp;utm_medium=referral&amp;amp;utm_content=inline-mention&amp;amp;utm_campaign=tns+platform"&gt;audits of codebases&lt;/a&gt; have found that up to 96% of projects contain open source code. This isn’t surprising: open access to the code means code can potentially be worked on by any number of contributors, as well as offering greater opportunities for thorough testing of the code for bugs.&lt;/p&gt;
&lt;p&gt;Without maintainers to manage the libraries, many open source libraries would stagnate. Some open source libraries have huge numbers of contributors, whilst for others, the maintainer may be the only contributor. In the latter case, the library simply stops if the maintainer does: no more patches, no security updates, no bug fixes. &lt;/p&gt;
&lt;p&gt;If code needs to be continually updated and maintained to ensure that it doesn’t become obsolete, and the majority of software projects contain code from open source libraries, the effort that maintainers make to keep libraries up to date can’t be overstated.&lt;/p&gt;
&lt;p&gt;Why, then, do maintainers do the work? &lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;What motivates maintainers?&lt;/h2&gt;
&lt;p&gt;As the 2023 &lt;a href="https://www.sonarsource.com/open-source-maintainer-survey-2023.pdf"&gt;&lt;em&gt;Tidelift state of the open source maintainer report&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;revealed, most are driven by making a positive impact on the world (70%) and the fact that the work is creative, challenging, and/or enjoyable (62%). Maintainers are motivated by belief in the software, in the community, and in the open source philosophy – broadly speaking. But what does that mean in practice?&lt;/p&gt;
&lt;p&gt;The first episode of Canonical’s “Meet the Maintainers” series begins to reveal what these general motivations mean for the individual maintainers. The maintainer behind ripgrep, Andrew Gallant (or BurntSushi, as he’s also known) explains his earliest contributions to open source were inspired by his desire to host a discussion forum on his fan website for &lt;em&gt;The Simpsons&lt;/em&gt;. Starting with self-described “hacking” on the proprietary forum software VBulletin, Gallant developed an interest in forum software in general. Like the motivations other maintainers report, Gallant enjoyed the challenge and creativity involved with hacking the PHP code to modify it so it will “do little things.” Two decades later, Gallant’s motivations are more community-centered: the satisfaction of creating something that is “free for everyone to use,” and “watching people use the stuff [he] build[s].” To find out more about the many libraries and projects Gallant has contributed to and worked on over the years, &lt;a href="https://www.youtube.com/watch?v=InsUD69qmUw"&gt;watch the video on our YouTube channel&lt;/a&gt;. &lt;/p&gt;
&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="304" loading="lazy" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/InsUD69qmUw?feature=oembed" title="Meet the Maintainers: the mind behind ripgrep - Andrew Gallant’s blazing-fast search tool" width="540"&gt;&lt;/iframe&gt;
&lt;/div&gt;&lt;/figure&gt;
&lt;h2 class="wp-block-heading"&gt;Supporting maintainers: why it helps, and how you can do it&lt;/h2&gt;
&lt;p&gt;Financial incentives may not be the primary motivator for all maintainers; however, providing financial support helps to facilitate the work, ultimately producing stronger software, and faster code development. &lt;/p&gt;
&lt;p&gt;Fortunately, giving back to open source projects and libraries is simple and straightforward. Platforms like thanks.dev help you to donate to the repositories your software draws on, automatically distributing funds proportionally to how often the dependency is used. As &lt;a href="https://thanks.dev/static/why"&gt;thanks.dev suggest on their site&lt;/a&gt;, this makes it easier for larger organizations to “manage the logistics of supporting the thousands of projects they depend on”, including deeply nested packages and crucial indirect dependencies that may otherwise be overlooked.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ubuntu.com/blog/canonical-thanks-dev-giving-back-to-open-source-developers"&gt;Canonical began donating via thanks.dev in April of 2025&lt;/a&gt;. We committed to donating US$120,000 across 12 months at US$10,000 a month through the platform. You can find the full list of recipients so far at &lt;a href="https://thanks.dev/r/canonical"&gt;thanks.dev/r/canonical&lt;/a&gt;. &lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Recognizing maintainers&lt;/h2&gt;
&lt;p&gt;Open source libraries are labors of love that depend on the hard work of communities and talented developers. That work often goes unseen, unpaid, and unrecognized, but – directly, or indirectly – we rely on it more than we may be aware of. &lt;/p&gt;
&lt;p&gt;As an open source company, Canonical is committed to spreading the open source philosophy to as many people and communities as we can. We drive our own projects, but also contribute staff, code, and funding to many more, which you can find out about on our &lt;a href="https://canonical.com/projects"&gt;webpage&lt;/a&gt;. “Meet the Maintainers” sheds light on the maintainers behind the libraries, recognizing them for their work, and, hopefully, inspiring more people to support open source, join communities, and contribute to open source libraries. &lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/playlist?list=PLwFSk464RMxns9ylPfbaivj-fmIiE-Kld" rel="noreferrer noopener" target="_blank"&gt;Watch the series&lt;/a&gt;&lt;a href="https://www.youtube.com/playlist?list=PLwFSk464RMxns9ylPfbaivj-fmIiE-Kld"&gt; &amp;gt;&lt;/a&gt;&lt;/p&gt;
</content:encoded><author>Isobel Kate Maxwell (Isobel Kate Maxwell)</author><category>Community</category><pubDate>Fri, 13 Feb 2026 17:30:52 +0000</pubDate></item><item><title>From inspiration to impact: design students from Regent’s University London explore open design for their dissertation projects</title><link>https://ubuntu.com//blog/from-inspiration-to-impact-design-students-from-regents-university-london-explore-open-design-for-their-dissertation-projects</link><description>&lt;p&gt;Last year, we had the opportunity to speak at Regent’s UX Conference (Regent’s University London’s conference to showcase UX work by staff, students, and alumni), where we engaged with students to make them aware of open design and their ability to contribute design skills to open source projects. The talk sparked great discussion, and we [&amp;hellip;]&lt;/p&gt;
</description><content:encoded>
&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="" height="550" loading="lazy" sizes="(min-width: 974px) 974px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_974/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F62bb%2Fimage.png" srcset="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F62bb%2Fimage.png 460w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F62bb%2Fimage.png 620w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F62bb%2Fimage.png 1036w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1681/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F62bb%2Fimage.png 1681w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1920/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F62bb%2Fimage.png 1920w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_974/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F62bb%2Fimage.png 974w" width="974"/&gt;&lt;/figure&gt;
&lt;p&gt;Last year, we had the opportunity to speak at &lt;a href="https://studentunion.regents.ac.uk/event-details/regents-ux-design-conference" rel="noreferrer noopener" target="_blank"&gt;Regent’s UX Conference&lt;/a&gt; (Regent’s University London’s conference to showcase UX work by staff, students, and alumni), where we engaged with students to make them aware of &lt;a href="https://canonical.design/blog/open-design-the-opportunity-design-students-didnt-know-they-were-missing" rel="noreferrer noopener" target="_blank"&gt;open design&lt;/a&gt; and their ability to contribute design skills to open source projects. The talk sparked great discussion, and we were thrilled when two students, &lt;a href="https://www.linkedin.com/in/khaula-akhtar/" rel="noreferrer noopener" target="_blank"&gt;Khaula Akhtar&lt;/a&gt;, and &lt;a href="https://tanrikuluyaprak6.wixsite.com/portfolio-01" rel="noreferrer noopener" target="_blank"&gt;Yaprak Tanrikulu&lt;/a&gt;, chose to take on our open design briefs as part of their final-year dissertation projects.&lt;/p&gt;
&lt;p&gt;Both students worked independently on their projects, gaining hands-on experience in a real-world work. They explored the growing role of design in open source and brought fresh thinking to areas where design is often overlooked.&lt;/p&gt;
&lt;p&gt;This post celebrates their thoughtful work, shares reflections from their design journey, and invites others to consider contributing to open source projects!&lt;/p&gt;
&lt;p&gt;If you want to explore the project briefs yourself, they are open for anyone to explore, think about, and attempt to solve. Whether you’re thinking of ideas, writing them on a napkin, or developing your solution, there are no limitations to your creativity here! &lt;a href="https://assets.ubuntu.com/v1/7bd9c15e-project_briefs_for_universities.pdf" rel="noreferrer noopener" target="_blank"&gt;Get the briefs here&lt;/a&gt; (PDF).&lt;/p&gt;
&lt;h1 class="wp-block-heading"&gt;&lt;a href="https://www.linkedin.com/in/khaula-akhtar/" rel="noreferrer noopener" target="_blank"&gt;Khaula Akhtar&lt;/a&gt;: Improving UX designer participation in open source software&lt;/h1&gt;
&lt;p&gt;Project brief: A master’s final project for Canonical that explores how to make non-code contribution pathways in open source more welcoming, engaging, and rewarding for UX designers.&lt;br/&gt;&lt;br/&gt;Objective: To identify the key barriers and motivations influencing UX designers’ participation in open source and translate those insights into practical and evidence-based recommendations for improving contribution pathways.&lt;/p&gt;
&lt;p&gt;Check out their &lt;a href="https://assets.ubuntu.com/v1/ffff3c7e-masters_final_project_presentation_2.pdf" rel="noreferrer noopener" target="_blank"&gt;project presentation&lt;/a&gt;!&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Reflection from Khaula&lt;/h2&gt;
&lt;h4 class="wp-block-heading"&gt;What motivated you to take on an open source design brief for your final year project?&lt;/h4&gt;
&lt;p&gt;I chose this brief because open source plays a huge role in digital infrastructure, yet UX work within these communities is still underrepresented and often undervalued. I wanted to understand why designers struggle to participate and what could be changed to make these spaces more inclusive and collaborative. Canonical’s question about improving non-code contribution pathways felt meaningful and aligned with my interest in design as a social and organizational practice, not just as an interface problem. The project was a chance to dig into the culture, incentives, and workflows that shape real-world contribution, which made it a challenging but rewarding topic. I was motivated by the idea that my work could help create clearer paths for designers and potentially make open source more human-centered.&lt;/p&gt;
&lt;h4 class="wp-block-heading"&gt;What surprised you most about open source?&lt;/h4&gt;
&lt;p&gt;I was most surprised by how strongly developer culture influences every part of the contribution experience. I expected some technical bias, but I did not anticipate how deeply it shaped language, decision rights, and even what counts as a meaningful contribution. It was eye-opening to see how much design work gets reduced to visual tasks and how little visibility research, content, and information architecture receive. At the same time, I was surprised by the amount of genuine goodwill among contributors. Many designers are interested in giving back, learning and collaborating, even when recognition is limited. This mix of strong community spirit and structural misalignment helped me understand why participation can feel both inspiring and challenging at the same time.&lt;/p&gt;
&lt;h4 class="wp-block-heading"&gt;What was the biggest challenge you encountered during the project, and how did you tackle it?&lt;/h4&gt;
&lt;p&gt;The biggest challenge was the analysis phase and bringing enough participants together for interviews within a short time frame. Recruiting designers with actual open source experience required a lot of outreach and scheduling flexibility, and I had to rely on professional networks and snowball sampling to reach people. Once interviews were collected, the next challenge was learning how to apply reflexive thematic analysis properly. It was my first time working with this method, and I initially struggled with coding consistently and avoiding over-interpretation. I tackled this by keeping a reflexive journal, revisiting transcripts multiple times and creating an audit trail to track how my ideas developed. Regular check-ins and re-reading the literature also helped me stay grounded and confident in the themes I was building.&lt;/p&gt;
&lt;h4 class="wp-block-heading"&gt;What advice would you give other students considering contributing to open source through design?&lt;/h4&gt;
&lt;p&gt;My advice would be to approach open source with patience and a learning mindset. The environments can feel technical at first, but designers bring valuable skills that many projects genuinely need. Start with projects that clearly welcome UX work, because good documentation, UX labels and approachable maintainers make a big difference. When contributing, keep your proposals clear, connect them to existing components or patterns and explain your rationale so developers can understand your thinking. Open source runs on transparency, so the more you show your process, the smoother collaboration becomes. Most importantly, choose a project that aligns with your interests so you stay motivated while navigating the learning curve.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h1 class="wp-block-heading"&gt;&lt;a href="https://tanrikuluyaprak6.wixsite.com/portfolio-01" rel="noreferrer noopener" target="_blank"&gt;Yaprak Tanrikulu&lt;/a&gt;: Unifying the Juju ecosystem&lt;/h1&gt;
&lt;p&gt;Project brief: A project to unify the Juju ecosystem by creating a centralized, user-focused documentation and learning platform that simplifies onboarding, enhances discoverability, and improves the overall usability of Juju’s tools and resources.&lt;br/&gt;&lt;br/&gt;Objective: The objectives of this project focus on understanding the challenges newcomers face within the Juju ecosystem, evaluating and improving the accessibility and discoverability of its documentation, designing a structured learning path for new users, and exploring how different documentation formats and platforms influence the overall user experience.&lt;br/&gt;&lt;br/&gt;Check out their &lt;a href="https://assets.ubuntu.com/v1/c583686b-unifying_the_juju_ecosystem_proposal.pdf" rel="noreferrer noopener" target="_blank"&gt;project presentation&lt;/a&gt;!&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Reflection from Yaprak&lt;/h2&gt;
&lt;h4 class="wp-block-heading"&gt;What motivated you to take on an open source design brief for your final year project?&lt;/h4&gt;
&lt;p&gt;I was drawn to the open source design brief because it offered an opportunity to work on a project that has real-world impact and is deeply collaborative by nature. Open source communities thrive on transparency, inclusivity, and shared ownership, values that align closely with how I see design as a discipline. I wanted to explore how design could play a role in improving accessibility and usability in open systems, and how it could help bridge the gap between contributors with different skill sets. The idea of designing for a global audience, while contributing to something that anyone can build upon, was incredibly motivating.&lt;/p&gt;
&lt;h4 class="wp-block-heading"&gt;What was your approach to designing a more unified documentation experience?&lt;/h4&gt;
&lt;p&gt;My approach began with understanding the ecosystem of users involved; maintainers, contributors, and newcomers. I conducted user research and mapped out their pain points when navigating and contributing to documentation. From there, I developed a modular design system that emphasized clarity, consistency, and discoverability. I focused on creating a structure that could adapt across different open source projects while maintaining a cohesive visual and information hierarchy.&lt;/p&gt;
&lt;h4 class="wp-block-heading"&gt;What was the biggest challenge you encountered during the project, and how did you tackle it?&lt;/h4&gt;
&lt;p&gt;The biggest challenge was realizing that open source communities have very different needs, and that a one-size-fits-all documentation solution wouldn’t work. Users of the Juju ecosystem have varying levels of experience, which makes it difficult to design a structure that supports everyone equally. To address this, I created a Learning Hub concept that categorizes lessons and resources based on user experience levels. This way, new users can easily identify where to start and see if a lesson has any prerequisites, while existing contributors can quickly find advanced materials relevant to them. By organizing the content in a progressive, user-centred way, the documentation becomes more inclusive and adaptable to different learning journeys within open source communities.&lt;/p&gt;
&lt;h4 class="wp-block-heading"&gt;What advice would you give other students considering contributing to open source through design?&lt;/h4&gt;
&lt;p&gt;My biggest advice would be to approach open source work with curiosity and not get sidetracked by technical details or unfamiliar terminology. For those new to open source, there are often many unknown concepts and terms, so instead of getting caught up in definitions, focus on understanding the general idea and the broader purpose behind the project. Open source design is as much about listening as it is about creating, it’s a dialogue between diverse contributors with shared goals. Start by understanding the community’s context, participate in discussions, and learn how decisions are made. Don’t be afraid to share your early ideas and invite feedback; openness is part of the process. Contributing to open source through design is a powerful way to grow as a designer because it pushes you to think inclusively, work collaboratively, and design for impact beyond a single user group.&lt;br/&gt; &lt;/p&gt;
&lt;h1 class="wp-block-heading"&gt;🎉Congratulations to Khaula &amp;amp; Yaprak!&lt;/h1&gt;
&lt;p&gt;We’re incredibly proud of both students for their initiative, insight, and thoughtful work. Taking on open design briefs independently, and part of a dissertation, shows a deep commitment to using design for real-world impact.&lt;/p&gt;
&lt;p&gt;Their projects offered us valuable insights into how emerging designers interpret open source challenges, reminding us of the importance of clear pathways, supportive communities, and space for creative exploration. Seeing how they approached ambiguity with curiosity has reinforced our commitment to making open design more accessible, and we hope to continue building partnerships that empower the next generation of designers to shape the future of open source.&lt;/p&gt;
&lt;p&gt;We hope this experience has opened the door to future engagement with open source communities, and we encourage them to continue advocating for design in tech, sharing their work, and building on the foundations of open design!&lt;/p&gt;
&lt;h1 class="wp-block-heading"&gt;Interested in running a project like this?&lt;/h1&gt;
&lt;p&gt;Are you part of a university looking to bring open source design into your curriculum? &lt;a href="https://canonical.design/blog/open-design-the-opportunity-design-students-didnt-know-they-were-missing" rel="noreferrer noopener" target="_blank"&gt;We’re looking for academic partners&lt;/a&gt; who want to give students meaningful, real-world experiences while contributing to the global open source ecosystem.&lt;/p&gt;
&lt;p&gt;Reach out at &lt;a href="mailto:opendesign@canonical.com" rel="noreferrer noopener" target="_blank"&gt;opendesign@canonical.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Learn more at &lt;a href="https://canonical.design/blog/open-design-the-opportunity-design-students-didnt-know-they-were-missing" rel="noreferrer noopener" target="_blank"&gt;Canonical.design&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity"/&gt;
&lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Join the Canonical design team&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;We’re looking for designers who care about craft and how systems work under the hood. At Canonical, design sits at the intersection of UX, engineering, and open source where we shape cohesive, accessible experiences across cloud, desktop, and IoT products.&lt;/p&gt;
&lt;p&gt;If you enjoy solving complex problems and turning technical depth into clarity, explore our open roles: &lt;strong&gt;&lt;a href="https://canonical.com/careers/web-and-design" rel="noreferrer noopener" target="_blank"&gt;canonical.com/careers&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
</content:encoded><author>Miguel Divo (Miguel Divo)</author><category>Design</category><category>open design</category><category>University</category><pubDate>Fri, 13 Feb 2026 09:22:29 +0000</pubDate></item><item><title>When an upstream change broke smartcard FIPS authentication – and how we fixed it</title><link>https://ubuntu.com//blog/when-an-upstream-change-broke-smartcard-fips-authentication-and-how-we-fixed-it</link><description>&lt;p&gt;This is the story of how Canonical&amp;#8217;s Support team provided bug-fix support: we tracked down an upstream change in OpenSC that inadvertently broke FIPS compatibility, coordinated with upstream developers across distributions, and delivered both a hotfix and a proper universal solution.&lt;/p&gt;
</description><content:encoded>
&lt;p&gt;A government agency mandated smartcard authentication across their Ubuntu fleet. When they enabled FIPS mode to meet compliance requirements, smartcard authentication stopped working. Nearly 1,000 systems sat waiting for FIPS rollout.&lt;/p&gt;
&lt;p&gt;This is the story of how Canonical’s Support team provided bug-fix support: we tracked down an upstream change in OpenSC that inadvertently broke FIPS compatibility, coordinated with upstream developers across distributions, and delivered both a hotfix and a proper universal solution.&lt;/p&gt;
&lt;h3 class="wp-block-heading"&gt;The setup: finding what broke&lt;/h3&gt;
&lt;p&gt;When the case first opened, the error messages looked like standard authentication failures. Nothing pointed to the root cause.&lt;/p&gt;
&lt;p&gt;Our support engineer started by replicating the issue on a Noble (24.04 LTS) test system. The pkcs11 tooling was segfaulting on Noble FIPS installations, but the same tools worked fine on Jammy (22.04 LTS). Something had changed in the OpenSC package – the open source software toolkit for using cryptographic smartcards across various operating systems, which the government agency used for authentication – between Ubuntu 22.04 LTS and 24.04 LTS. But what?&lt;/p&gt;
&lt;p&gt;Within days, we’d narrowed the problem to changes in the upstream OpenSC codebase, ruling out conflicts with our FIPS implementation. An upstream change in how OpenSC handled provider initialization was causing crashes when it tried to interact with OpenSSL in FIPS mode.&lt;/p&gt;
&lt;h3 class="wp-block-heading"&gt;The investigation: confirming the root cause&lt;/h3&gt;
&lt;p&gt;We filed upstream bug reports with both OpenSSL and OpenSC to see if they had insights. Then we kept debugging.&lt;/p&gt;
&lt;p&gt;The breakthrough came quickly: the FIPS OpenSSL provider was being initialized incorrectly. OpenSC was setting the wrong OpenSSL provider in its backend, using the default provider instead of the FIPS provider, which blocked the hashing mechanisms needed for smartcard authentication.&lt;/p&gt;
&lt;p&gt;We built a test to confirm this was the actual root cause. The customer tested it in their environment. It worked.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;This is the challenge of managing upstream dependencies in enterprise Linux: OpenSC had evolved its provider handling between releases, introducing behavior that broke FIPS compatibility. An upstream development decision had unintended consequences in specific compliance contexts. &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Enterprise bug-fix support bridges this gap: identifying which upstream change caused the regression, understanding why, and coordinating a universal fix.&lt;/p&gt;
&lt;h3 class="wp-block-heading"&gt;The fix: unblock now, solve fully later&lt;/h3&gt;
&lt;p&gt;Working with OpenSC maintainers, we reached the final diagnosis: upstream changes to OpenSC meant it now attempts to load an algorithm disallowed under FIPS policy. OpenSSL correctly returns a NULL pointer, but OpenSC still tries to use it as a valid object. Instead of gracefully handling the situation – trying different crypto options or querying what’s available – it errors out.&lt;/p&gt;
&lt;p&gt;We shared a hotfix – a common practice to restore functionality in our enterprise bug-fix support service – to unblock the customer so they could move forward with their FIPS rollout.&lt;/p&gt;
&lt;p&gt;But a hotfix isn’t the same as a comprehensive fix. This needed a solution that worked upstream – fixing the issue in OpenSC itself so it would work across Ubuntu and any other distribution using OpenSC in FIPS environments, not just a distro-specific patch.&lt;/p&gt;
&lt;p&gt;This week, we agreed on the shape of the final fix. The upstream PR is waiting to merge. Once that’s done, we’ll backport it to Ubuntu. This will resolve all three customer cases we’ve seen with the same root cause.&lt;/p&gt;
&lt;h3 class="wp-block-heading"&gt;The key ingredient: collaboration across the ecosystem&lt;/h3&gt;
&lt;p&gt;Dariusz Gadomski, a member of our Support team who worked on this case, reflected on what made it work:&lt;/p&gt;
&lt;figure class="wp-block-pullquote"&gt;&lt;blockquote&gt;&lt;p&gt;&lt;em&gt;I really like how this case brought different groups together. Even though we come from different backgrounds, we all had the same goal: fixing the problem for our users. This is the real power of open source: people working together as one team to build better software for everyone.&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;/figure&gt;
&lt;p&gt;Fixing this bug involved coordination across support engineering, Canonical’s security team, and the OpenSC upstream community. What appeared as an authentication failure was an upstream change to provider initialization that broke FIPS compatibility. The diagnosis demanded understanding how OpenSC, OpenSSL, and FIPS mode interact across versions, then collaborating with upstream maintainers to develop a universal solution.&lt;/p&gt;
&lt;h3 class="wp-block-heading"&gt;Learn more about bug-fix support through Ubuntu Pro + Support&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://ubuntu.com/blog/what-is-linux-support"&gt;Read more about what Linux support actually means&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ubuntu.com/support"&gt;Explore Ubuntu Pro + Support options&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ubuntu.com/support/contact-us?product=support-overview"&gt;Get in touch to discuss your specific needs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content:encoded><author>Lidia Luna Puerta (Lidia Luna Puerta)</author><category>Bug-fix support</category><category>Enterprise Support</category><category>FIPS</category><pubDate>Thu, 12 Feb 2026 13:36:21 +0000</pubDate></item><item><title>Open platforms, edge AI, and sovereign telco clouds: Ecrio &amp;#038; Canonical at MWC Barcelona</title><link>https://ubuntu.com//blog/open-platforms-edge-ai-and-sovereign-telco-clouds-ecrio-canonical-at-mwc-barcelona</link><description>&lt;p&gt;Building telco clouds with open source At MWC Barcelona 2026, Canonical is demonstrating how telecommunications operators and enterprises can design and operate a cloud on their own terms: sovereign, cost-effective, and built on open platforms that span from core data centers to the intelligent edge. One of the demos is the result of Canonical’s collaboration [&amp;hellip;]&lt;/p&gt;
</description><content:encoded>
&lt;h1 class="wp-block-heading"&gt;Building telco clouds with open source&lt;/h1&gt;
&lt;p&gt;At MWC Barcelona 2026, Canonical is demonstrating how telecommunications operators and enterprises can design and operate a cloud on their own terms: sovereign, cost-effective, and built on open platforms that span from core data centers to the intelligent edge.&lt;/p&gt;
&lt;p&gt;One of the demos is the result of Canonical’s collaboration with Ecrio, a leader in AI-powered critical communication software optimized for private mobile and edge deployments. Ecrio Edge AI Communication Platform is an end-to-end platform that combines edge AI and communications to enable rapid human oversight where automation alone falls short. The platform delivers actionable and distributed inferencing, intelligent human-to-machine communications, and full support for generative and agentic AI, with a companion app for rugged phones, tablets, and smart glasses for human escalation.&lt;/p&gt;
&lt;p&gt;Our demo with Ecrio showcases a compact and ruggedized edge cloud deployment which combines networking and AI resources to serve real-world use cases such as worker safety and crowd management. Together, Canonical and Ecrio are showcasing how cloud-native, open-source infrastructure enables modern telco services while preserving operator control, flexibility, and long-term sustainability.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ubuntu.com/engage/canonical-at-mwc-barcelona-2026"&gt;Visit Canonical’s booth (Hall 2, stand 2D20)&lt;/a&gt; to see a real-world telco cloud stack in action, integrating open infrastructure, edge AI, and mission-critical communications.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;A cloud on your own terms&lt;/h2&gt;
&lt;p&gt;Telecommunications operators face increasing pressure to modernize infrastructure while avoiding vendor lock-in, controlling costs, and meeting sovereignty requirements. This demo demonstrates how Canonical addresses these challenges with a hardened, long-term supported open source foundation consisting of Ubuntu, the securely designed, enterprise-grade operating system for telco workloads, and Canonical Kubernetes, the production-grade platform for cloud-native network functions and AI applications.&lt;/p&gt;
&lt;p&gt;Together, Ubuntu and Canonical Kubernetes form the backbone of an open telco cloud architecture that gives operators the freedom to deploy, scale, and evolve their networks on their own terms, across public cloud, private cloud, and edge environments.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Cloud-native edge AI communications with Ecrio&lt;/h2&gt;
&lt;p&gt;Running on Ubuntu and Canonical Kubernetes, Ecrio’s Edge AI Communication platform demonstrates how critical communication services can be delivered using modern, cloud-native architectures. Ecrio’s solution provides 3GPP standards-compliant voice and video calling, messaging, mission-critical push-to-talk capabilities, and AI-driven orchestration and analytics designed for edge and private mobile networks.&lt;/p&gt;
&lt;p&gt;Built on a modular architecture, the platform includes an MCP-based AI Workload Orchestrator to enable sophisticated automation workflows which use connected cameras, industrial IoT sensors, and Enterprise data. The platform serves a wide range of industries – including oil and gas, mining, chemicals, manufacturing, retail, utilities, healthcare, and more – driving efficiency, safety, and innovation at scale.&lt;/p&gt;
&lt;p&gt;Enabled by Canonical’s open infrastructure software, Ecrio shows how communication platforms can be deployed consistently from the core to the edge, fully integrated with modern cloud-native workflows.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Edge AI and ecosystem demos at MWC&lt;/h2&gt;
&lt;p&gt;The joint demo emphasizes practical, production-ready edge AI use cases, highlighting how open-source infrastructure accelerates innovation across the telco ecosystem. Live demonstrations at Canonical’s booth include real-time video analytics at the edge, workplace safety monitoring and situational awareness, and AI-driven operational intelligence across distributed environments.&lt;/p&gt;
&lt;p&gt;Ecrio’s communication and orchestration layers complement these scenarios, illustrating how AI-powered human-machine communication fits naturally into a broader telco cloud and edge AI architecture.&lt;/p&gt;
&lt;p&gt;This integrated approach enables operators to deploy intelligent services closer to users while maintaining a consistent, open platform across the entire network.&lt;/p&gt;
&lt;h1 class="wp-block-heading"&gt;Build your cloud with Canonical&lt;/h1&gt;
&lt;p&gt;As demonstrated at MWC Barcelona this year, the combination of Canonical’s open source cloud infrastructure with Ecrio’s edge-optimized communication services represents a new generation of telco cloud stacks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sovereign by design&lt;/li&gt;
&lt;li&gt;Extensible through open ecosystems&lt;/li&gt;
&lt;li&gt;Cost-efficient without sacrificing performance or security&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With Canonical’s trusted open source, operators and enterprises can modernize network operations, enable private 5G and edge cloud deployments, and unlock new AI-driven services without surrendering control of their technology stack.&lt;/p&gt;
&lt;p&gt;To learn more about this project or how you could benefit, visit Canonical’s booth at MWC Barcelona 2026 in Hall 2, 2D20 or &lt;a href="https://canonical.com/solutions/telco#get-in-touch"&gt;contact us&lt;/a&gt; for more information.&lt;/p&gt;
</content:encoded><author>Benjamin Ryzman (Benjamin Ryzman)</author><pubDate>Thu, 12 Feb 2026 13:12:34 +0000</pubDate></item><item><title>What is RDMA?</title><link>https://ubuntu.com//blog/what-is-rdma</link><description>&lt;p&gt;Modern data centres are hitting a wall that faster CPUs alone cannot fix. As workloads scale out and latency budgets shrink, the impact of moving data between servers is starting to become the most significant factor in overall performance. Remote Direct Memory Access, or RDMA, is one of the technologies reshaping how that data moves, [&amp;hellip;]&lt;/p&gt;
</description><content:encoded>
&lt;p&gt;Modern data centres are hitting a wall that faster CPUs alone cannot fix. As workloads scale out and latency budgets shrink, the impact of moving data between servers is starting to become the most significant factor in overall performance. Remote Direct Memory Access, or RDMA, is one of the technologies reshaping how that data moves, and it forces a rethink of some long-held assumptions in data centre networking.&lt;/p&gt;
&lt;p&gt;This article is the first in a short series. The follow-ups will look specifically at the two primary network interconnects that enable RDMA, InfiniBand and RoCE. Here, the goal is simpler: explain what RDMA is, why it matters now, and why it can be both powerful and uncomfortable for operators.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Remote Direct Memory Access explained&lt;/h2&gt;
&lt;figure class="wp-block-table"&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Remote Direct Memory Access (RDMA) is a data center networking technology that allows servers to exchange data directly between application memory spaces over the network, bypassing the operating system and CPU on the remote side. This enables lower latency, higher throughput, and more predictable performance compared to traditional TCP/IP networking.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;At its core, RDMA lets one machine read from or write directly into the memory of another machine over the network, without involving the remote CPU or operating system in the data path.&lt;/p&gt;
&lt;p&gt;In a conventional TCP/IP exchange, even a fast one, data is copied multiple times. It moves from the NIC into kernel buffers, through the network stack, into user space, and back again on the other side. Each step adds latency, burns CPU cycles, and introduces jitter.&lt;/p&gt;
&lt;p&gt;RDMA removes most of that path. Once a connection is set up and memory is registered, the NIC performs the transfer directly between application memory regions. The remote CPU is not interrupted. The kernel is not traversed. The result is very low latency, very high throughput, and far more predictable performance.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;➤ RDMA implements kernel bypass and zero-copy networking, avoiding the overheads inherent in TCP/IP-based data center networks.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This is not magic, and it is not free. RDMA shifts responsibility away from the operating system and towards the application and the network. That shift is where both the value and the challenge lie.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Why RDMA matters for modern data center networking&lt;/h2&gt;
&lt;p&gt;RDMA is not new. High-performance computing (HPC) has used it for decades. What has changed is where the bottlenecks now sit for communication service providers (CSPs) and large enterprises.&lt;/p&gt;
&lt;p&gt;Telco clouds, core network functions, analytics pipelines, and AI workloads all rely on intensive server-to-server communication. This east-west traffic dominates over slower, less latency-sensitive north-south flows between the data centers and the Internet. When every microsecond matters, shaving tens of microseconds from each exchange compounds quickly.&lt;/p&gt;
&lt;p&gt;Second, CPUs are expensive and increasingly scarce. Offloading data movement from general-purpose cores frees capacity for actual packet processing, encoding, inference, or control-plane logic. Operators deploying UPFs, databases, or message buses at scale see this immediately in reduced core counts per workload.&lt;/p&gt;
&lt;p&gt;Third, determinism matters more than peak throughput. Many telco and real-time enterprise applications care less about average latency and more about tail latency. RDMA’s bypass of the kernel scheduler and TCP congestion machinery produces tighter latency distributions, which directly translates into more predictable service behaviour.&lt;/p&gt;
&lt;p&gt;This is why RDMA is showing up in modern storage backends, distributed databases, AI training clusters, and increasingly in network-intensive telco workloads. In practice, adopting RDMA is less about enabling a feature in a workload and more about designing the data centre fabric, host configuration, and lifecycle operations to support predictable low-latency behaviour.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;RDMA vs TCP/IP: how data center networking is changing&lt;/h2&gt;
&lt;p&gt;The most important difference is philosophical.&lt;/p&gt;
&lt;p&gt;Traditional Ethernet networking assumes the network is unreliable and the hosts are responsible for recovery. TCP embodies this model. It is flexible, forgiving, and remarkably robust, but it hides performance costs behind abstraction.&lt;/p&gt;
&lt;p&gt;RDMA assumes a far more cooperative environment. Memory must be explicitly registered. Access permissions are managed at connection setup. Packet loss is expected to be rare, not normal. Reliability is often handled below or beside TCP, or avoided entirely.&lt;/p&gt;
&lt;p&gt;From an operator perspective, this means that the network stops being “just IP”. Latency, loss, buffering, and congestion behaviour suddenly matter in very concrete ways. A misconfigured switch buffer or an oversubscribed link that TCP would eventually recover from can stall or break an RDMA workload.&lt;/p&gt;
&lt;p&gt;It also means that debugging shifts. Problems that used to be visible in system call traces or kernel metrics may now live in NIC counters, fabric telemetry, or application-level error paths.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;How CSPs and enterprises use RDMA in real data centers&lt;/h2&gt;
&lt;p&gt;In large telco clouds, RDMA is increasingly used underneath control-plane databases and state stores. Distributed shared memory databases, and message buses backed by RDMA-enabled transports have the potential for lower commit latencies under load. The practical effect is faster convergence during scaling events and fewer cascading timeouts during failures.&lt;/p&gt;
&lt;p&gt;In enterprise data centres, storage is often the first visible win. NVMe over Fabrics using RDMA allows storage traffic to achieve local-disk-like latency over the network. Operators see higher IOPS with fewer CPU cores consumed on both the compute and storage sides.&lt;/p&gt;
&lt;p&gt;AI infrastructure provides another clear example. GPU-to-GPU communication during training is extremely sensitive to latency and jitter. RDMA allows collective operations to scale across nodes without saturating host CPUs, which is why it has become foundational in large training clusters.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ubuntu.com/blog/canonical-kubernetes-enhances-ai-ml-development-capabilities-with-nvidia-integrations"&gt;Explore GPU‑Direct RDMA in Kubernetes&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Across all of these environments, the pattern is the same. RDMA does not merely make things faster. It changes which resources are stressed and where failures surface.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;RDMA challenges in data center networks&lt;/h2&gt;
&lt;p&gt;RDMA is unforgiving of sloppy networks. Packet loss, excessive buffering, and uncontrolled congestion can have disproportionate impact, so fabric design and validation matter more than ever.&lt;/p&gt;
&lt;p&gt;Operational tooling is often immature compared to decades of TCP observability. Teams need to be comfortable with NIC-level metrics, switch telemetry, and application-specific diagnostics.&lt;/p&gt;
&lt;p&gt;In real operator environments, many RDMA issues surface before the workload is even deployed. Inconsistent NIC firmware versions, mismatched PCIe settings, incorrect NUMA alignment, or missing kernel modules can silently degrade performance. This is why RDMA adoption tends to push operators towards tighter control of bare-metal provisioning and hardware lifecycle management, rather than treating servers as interchangeable units. Platforms that standardise bare-metal commissioning, firmware baselines, and kernel configuration, such as MAAS, become critical enablers for running RDMA reliably at scale.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ubuntu.com/blog/data-centre-ai-evolution-combining-maas-and-nvidia-smartnics"&gt;Read more about RDMA and AI networking challenges and solutions.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Finally, RDMA tightens coupling between applications and infrastructure. Memory registration, queue depths, and transport choices leak into application design. This is acceptable when performance is critical, but it reduces portability and increases the cost of mistakes.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Closing thoughts&lt;/h2&gt;
&lt;p&gt;RDMA challenges the comfortable abstraction layers that have served data centres well for years. It trades generality for performance, and flexibility for determinism. For CSPs and enterprises building modern, scale-out infrastructure, that trade-off is increasingly justified.&lt;/p&gt;
&lt;p&gt;Understanding RDMA is no longer optional for architects working on high-performance telco clouds or data-intensive platforms. The question is not whether it will appear in your environment, but whether you will be ready for the architectural and operational consequences when it does.&lt;/p&gt;
&lt;p&gt;Are you interested in Canonical products supporting RDMA? &lt;a href="https://canonical.com/solutions/telco#get-in-touch"&gt;Contact our experts&lt;/a&gt; today.&lt;/p&gt;
</content:encoded><author>Benjamin Ryzman (Benjamin Ryzman)</author><category>AI</category><category>HPC</category><category>networking</category><pubDate>Wed, 11 Feb 2026 11:07:57 +0000</pubDate></item><item><title>Building new revenue streams: 3 strategic cloud opportunities for telcos in 2026</title><link>https://ubuntu.com//blog/new-telco-cloud-opportunity</link><description>&lt;p&gt;PWC claimed the &amp;#8216;fundamental challenge&amp;#8217; behind slowing growth is that telecom’s &amp;#8216;core products and services&amp;#8217; are &amp;#8216;becoming commodities.&amp;#8217; The way forward lies in modernizing and diversifying: evolving from traditional telecommunications to &amp;#8216;techco&amp;#8217; (technology company) services. In 2026, many of these opportunities will come from cloud computing.&lt;/p&gt;
</description><content:encoded>
&lt;p&gt;The telecommunications industry is at a turning point: telcos are seeking ways to turn innovation into new opportunities. Looking at the data, the desire is easy to understand. &lt;a href="https://www.pwc.com/gx/en/industries/tmt/assets/pwc-perspectives-from-the-global-telecom-outlook-2024-2028.pdf"&gt;In 2023, PWC projected&lt;/a&gt; that the sector’s annual growth rate would slow significantly between 2024 and 2028. PWC claimed that the “fundamental challenge” behind this trend was that telecom’s “core products and services”, like fixed broadband service and mobile service, “are becoming commodities.” This means that prices remain stagnant, while the industry “faces a continual need to invest in infrastructure.” Slower projected growth in revenue was particularly noticeable in mature markets. PWC is not alone with its growth projections: Analysys Mason predicted &lt;a href="https://www.analysysmason.com/research/content/regional-forecasts-/global-telecoms-forecast-rddg0/"&gt;CAGR growth of only 1% between 2024 and 2029&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;For many, the way forward lies in modernizing and diversifying from their core offerings: evolving from traditional telecommunications to “techco” (technology company) services. These offerings focus on value added digital services that are client-centric, capitalizing on the established strengths of telecoms. In 2026, many of these opportunities will come from cloud computing.&lt;/p&gt;
&lt;p&gt;In advance of &lt;a href="https://ubuntu.com/engage/canonical-at-mwc-barcelona-2026"&gt;MWC 2026&lt;/a&gt;, this article will consider the issue in more depth, outline some of the strategies that telecoms are taking, and explain how – and why – open source solutions are more important than ever in the conversation.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Strategy one: regional sovereign clouds&lt;/h2&gt;
&lt;p&gt;Geopolitical turbulence and changing legislation have both resulted in organizations increasingly seeking data sovereignty strategies, which we cover in greater depth in “&lt;a href="https://ubuntu.com/engage/sovereign-cloud-guide?_gl=1*1kq1shq*_gcl_au*MTQ1MTcwMTUxLjE3NjI3NzE5Mzc."&gt;Sovereign clouds: the essential guide for enterprises&lt;/a&gt;.” While public cloud vendors are starting to offer sovereign cloud solutions, many organizations would prefer to rely on local or regional clouds to gain more autonomy. &lt;/p&gt;
&lt;p&gt;Some organizations will be motivated to build their own on-premises sovereign clouds to host workloads and achieve digital sovereignty, but not every organization can – or should – do so. This leaves an interesting opportunity for telecommunications vendors, who can build fully open source, cost-effective private clouds using platforms such as &lt;a href="https://canonical.com/openstack"&gt;Canonical OpenStack&lt;/a&gt; to set up regional sovereign clouds. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What makes telecoms a natural fit for the sovereign cloud opportunity?&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Geography&lt;/strong&gt;: since telecommunications providers typically operate as domestic entities, they are subject to regional legal frameworks. This alignment minimizes the likelihood of conflicting regulations and reduces overall compliance risk. Likewise, this makes local telecoms the natural choice for public sector data and workloads.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Trust&lt;/strong&gt;: telecoms can benefit from their strong local presence. These organizations have often built familiarity and trust over time with both customers and local governments in their region, providing a comparatively strong commercial position from which to offer regional sovereign cloud services. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Existing infrastructure&lt;/strong&gt;: telecoms have the unique advantage of a vast footprint of existing infrastructure. This established presence offers a distinct competitive advantage: it is often significantly faster and more cost-effective to retro-fit these facilities for sovereign cloud operations, than it is for competitors to construct new facilities from the ground up. &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;While orchestrating workloads within a regional sovereign cloud is essential, it can often be a complex and demanding task. Kubernetes streamlines this, by providing a layer that makes applications portable across clouds, requiring minimal adaptations or configuration changes. Kubernetes is more nimble than monolithic architectures, and forms the starting point for digital transformation. Canonical offers security maintenance and updates for &lt;a href="https://ubuntu.com/kubernetes"&gt;Canonical Kubernetes &lt;/a&gt;for up to 15 years as part of our Long Term Support (LTS) commitment. This means that organizations can plan their IT lifecycle in advance, and avoid the sudden costs of unexpected migrations. &lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Strategy two: generative AI (GenAI) factories&lt;/h2&gt;
&lt;p&gt;GenAI has become firmly established in the workflows of an increasing number of organizations. With this comes the need for “AI factories”: specialized data centers that can turn raw data into AI models at scale. The capital expenditure of the high-performance hardware and software needed to run AI factories effectively – to ingest huge amounts of data, network at high speeds, and support AI training workloads – is an obstacle to building private AI factories on premises for most organizations. Instead, enterprises are looking to outsource the compute required to build models. This edge cloud opportunity also provides telecoms a new potential stream of revenue.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How can telcos lead the GenAI factory wave?&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Inference at the edge: the pre-existing infrastructure that presents telecoms with a strong position from which to offer regional sovereign clouds also provides a competitive edge for moving into AI factories. As GenAI models shift from training to inference, telecoms’ pre-existing distributed infrastructure means that they can process sensitive data at the edge of the network, providing ultra-low latency required for highly efficient processing. This is particularly critical for AI that relies on instantaneous – or near instantaneous – speed, like agentic AI. &lt;/li&gt;
&lt;li&gt;Sovereign AI: much like sovereign clouds, there is a growing demand globally for LLMs that reflect local culture, values, legislation, and languages. Regionally based telecoms can provide trusted national infrastructure to host sovereign LLMs.&lt;/li&gt;
&lt;li&gt;Private AI Enclaves: telecoms can offer “GenAI-as-a-Service” over a private network slice. This ensures that an organization’s proprietary data used to fine-tune a model never traverses the public internet, providing a level of security that major public cloud vendors cannot match without a complex VPN setup.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Building a GenAI factory demands a cutting-edge software stack – however, with such cutting-edge software comes brand-new security challenges. As &lt;a href="https://ubuntu.com/engage/2025-state-of-software-supply-chain"&gt;our recent report with IDC&lt;/a&gt; indicated:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;43% of organizations are “very” or “extremely” concerned about their ability to secure their AI stack&lt;/li&gt;
&lt;li&gt;60% have only basic, or no, security controls to safeguard their AI/ML systems&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This creates serious risks around data and security. Safely mitigating these is critical for telecoms to grasp the GenAI factory opportunity effectively. &lt;/p&gt;
&lt;p&gt;Through our data and AI portfolio, Canonical helps organizations mitigate security risks as well as simplify the deployment and maintenance of AI models through automated workflows, security patching, and tooling integrations. This ensures that organizations can access trusted, supported ML and AI applications. Find out more on our &lt;a href="https://canonical.com/solutions/ai/infrastructure"&gt;webpage&lt;/a&gt;.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Strategy three: smart factories&lt;/h2&gt;
&lt;p&gt;Telecoms have become the primary enablers of a growing phenomenon: smart factories. Smart factories are highly digitized and connected production facilities. Unlike traditional factories, which rely on fixed assembly lines and manual troubleshooting, smart factories optimize manufacturing through constant feedback loops of data, using these to run themselves with minimal human intervention. &lt;/p&gt;
&lt;p&gt;Smart factories require thousands of sensors, tracking each element of the manufacturing process. This is then translated into a digital twin – a real-time virtual 3-D replica of the factory. Changes to the physical world – communicated through the sensors – are reflected in the digital model. AI models analyze the data provided by sensors to identify patterns and alert factory managers to issues that require their attention. For example, predicting that a motor will fail before it does. In a traditional factory set up, this would require manual labour and time-intensive troubleshooting to fix; in a smart factory, issues are solved much faster. &lt;/p&gt;
&lt;p&gt;For these elements to come together effectively and successfully, smart factories require industrial connectivity – the ultra low-latency speeds of private 5G networks. It is this aspect which provides telcos an advantage in harnessing the smart factory cloud opportunity.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What gives telcos the edge in powering smart factories?&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Private 5G: public Wi-Fi is too interference prone for smart factories to run on. Telecoms can carve a slice of their already-licensed 5G spectrum exclusively for factories, ensuring the greater reliability of connection necessary for smart factories to run efficiently. This carries a significant premium, which can be a lucrative stream of revenue for telecoms. &lt;/li&gt;
&lt;li&gt;Quality on demand: while a system integrator can, for example, build the applications that the smart factory runs on, telecoms own the Network APIs. Because of this, only telecoms have the power to reconfigure the network in real-time, and thus offer quality on demand APIs, ensuring that the factory’s connectivity is never lost. &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The missing piece of the puzzle for telecoms in this opportunity is getting support for edge systems, which smart factories depend on. Canonical’s edge portfolio is designed to support your systems’ security, whilst easily managing and governing your edge infrastructure. &lt;a href="https://ubuntu.com/core"&gt;Ubuntu Core&lt;/a&gt;, our operating system for internet of things (IoT), devices, and embedded systems, features a minimized attack surface and a variety of measures which help to reduce the risk of security breaches. Likewise, &lt;a href="https://ubuntu.com/landscape?_gl=1*117tt2d*_gcl_au*MTQ1MTcwMTUxLjE3NjI3NzE5Mzc."&gt;Canonical Landscape&lt;/a&gt; simplifies managing the thousands of IoT devices needed to run a smart factory by enabling over the air (OTA) updates and automating compliance tasks across your Ubuntu estate. Find out more about our edge portfolio on our &lt;a href="https://canonical.com/solutions/infrastructure/edge-computing"&gt;webpage&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Smart factories rely on huge amounts of data. That makes supporting and securing databases critical – downtime could cause a physical bottleneck, and equipment damage. Such support can be expensive, however, Canonical offers both security and support for databases under the same Ubuntu Pro subscription, reducing costs and enabling telcos to offer value-added services with open source data solutions. Find out more about our database offering on the &lt;a href="https://canonical.com/data"&gt;webpage&lt;/a&gt;.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Supporting your investment&lt;/h2&gt;
&lt;p&gt;To engage with any of these new streams of revenue, telecoms’ underlying software stack will become of critical importance. &lt;/p&gt;
&lt;p&gt;Proprietary “black-box” systems don’t offer the levels of cost-effectiveness, transparency, and agility telecoms need to remain competitive and future-proof their investments. Open source gives telcos the modular agility and transparency needed to turn emerging technologies into profitable services. &lt;/p&gt;
&lt;p&gt;Canonical’s trusted &lt;a href="https://canonical.com/solutions/infrastructure"&gt;open source infrastructure solutions&lt;/a&gt; are modular, offering supported software elements that telcos can use to build frameworks that are customized to their individual needs. On top of that, we optimize our solutions for silicon and offer them on a large variety of hardware, thanks to our collaboration with the world’s leading silicon and hardware vendors.&lt;/p&gt;
&lt;p&gt;Many open source projects are complex to manage, particularly at a large scale. Canonical offers long-term support (LTS) for up to 15 years for our open source software, ensuring that your investment is sustainable in the long term. Likewise, through our &lt;a href="https://ubuntu.com/managed/firefighting-support"&gt;professional support offering&lt;/a&gt;, our global team of experts is on call 24/7 to ensure that if you run into a problem, we’re there to help. &lt;/p&gt;
&lt;p&gt;The opportunity for growth is clear. By combining their inherent physical advantages with a securely-designed, supported, and open software foundation, telecoms can move beyond the limits of commoditized connectivity and break into new streams of revenue, new industries, and new technologies. &lt;/p&gt;
&lt;p&gt;Find out more about Canonical’s telco solutions on our &lt;a href="https://canonical.com/solutions/telco"&gt;webpage&lt;/a&gt;, or meet us in person &lt;a href="https://ubuntu.com/engage/canonical-at-mwc-barcelona-2026"&gt;at MWC in 2026&lt;/a&gt;.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;Further resources:&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://canonical.com/blog/bt-group-and-canonical-deliver-5g-to-uk-stadiums"&gt;BT Group and Canonical deliver 5G to UK stadiums&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://canonical.com/blog/how-telco-companies-can-reduce-5g-infrastructure-costs-with-modern-open-source-cloud-native-technologies"&gt;How telco companies can reduce 5G infrastructure costs with modern open source cloud-native technologies&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://canonical.com/blog/bringing-canonical-kubernetes-to-sylva-a-new-chapter-for-european-telco-clouds"&gt;Bringing Canonical Kubernetes to Sylva: a new chapter for European telco clouds&lt;/a&gt;&lt;/p&gt;
</content:encoded><author>Isobel Kate Maxwell (Isobel Kate Maxwell)</author><category>cloud</category><category>genai</category><category>Sovereign cloud</category><category>Telco</category><pubDate>Tue, 10 Feb 2026 15:52:38 +0000</pubDate></item><item><title>SQL Server 2025 is generally available on Ubuntu 24.04 LTS</title><link>https://ubuntu.com//blog/sql-server-2025-ubuntu-24-04-lts</link><description>&lt;p&gt;Microsoft has announced the General Availability of SQL Server 2025 on Ubuntu 24.04 LTS. Learn about the new CU1 features, including OS-level observability, Contained Availability Groups, and native vector support for enterprise workloads.&lt;/p&gt;
</description><content:encoded>
&lt;p&gt;Microsoft has announced the General Availability (GA) of SQL Server 2025 on Ubuntu 24.04 LTS, starting with the CU1 release. This milestone allows enterprises to deploy mission-critical workloads on our latest Long Term Support release, benefiting from predictable stability and up-to-date kernels.&lt;/p&gt;
&lt;h3 class="wp-block-heading"&gt;Update your repository&lt;/h3&gt;
&lt;p&gt;If you have been testing the preview version, you must switch your repository configuration to ensure you are on the production track. To switch your repository configuration, update your source from mssql-server-preview.repo to mssql-server-2025.repo. Continuing to use the preview repository may result in installing pre-release builds that are not intended for production workloads.&lt;/p&gt;
&lt;h3 class="wp-block-heading"&gt;Enhancements for Linux&lt;/h3&gt;
&lt;p&gt;SQL Server 2025 CU1 introduces specific improvements for managing databases on Linux infrastructure.&lt;/p&gt;
&lt;p&gt;New Dynamic Management Views (DMVs) now expose OS-level metrics directly to database administrators. Views such as sys.dm_os_linux_cpu_stats and sys.dm_os_linux_disk_stats help distinguish between database-specific bottlenecks and infrastructure issues. The release also includes sys.dm_os_linux_net_stats, which provides real-time network interface statistics to troubleshoot connectivity and throughput.&lt;/p&gt;
&lt;p&gt;We also see improvements in Contained Availability Groups (CAG). Administrators can now enable database creation and restoration directly within a CAG session using sp_set_session_context. This simplifies lifecycle operations while maintaining strict access controls: only users with the dbcreator role can create databases, while the db_owner or sysadmin roles are required for restoration.&lt;/p&gt;
&lt;h3 class="wp-block-heading"&gt;AI capabilities&lt;/h3&gt;
&lt;p&gt;This release also brings native vector support for machine learning applications. To learn how to build a secure AI playground with these new capabilities, read our technical guide, &lt;a href="https://ubuntu.com/blog/sql-server-2025-on-ubuntu"&gt;AI meets SQL Server 2025 on Ubuntu.&lt;/a&gt;&lt;/p&gt;
&lt;h3 class="wp-block-heading"&gt;Enterprise security&lt;/h3&gt;
&lt;p&gt;For organizations with strict compliance requirements, Ubuntu Pro is the preferred choice. It extends the standard LTS security coverage to the full software stack covered in Universe and offers FIPS compliance, which is critical for many regulated SQL Server workloads.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ubuntu.com/azure/pro"&gt;Learn more about Ubuntu Pro for Azure ›&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can install the production-ready version of SQL Server 2025 on Ubuntu 24.04 today by following the &lt;a href="https://learn.microsoft.com/en-us/sql/linux/quickstart-install-connect-ubuntu"&gt;official installation guide&lt;/a&gt;.&lt;/p&gt;
</content:encoded><author>Jehudi (Jehudi)</author><category>Database</category><category>Microsoft</category><category>Microsoft Azure</category><category>SQL Server</category><pubDate>Fri, 06 Feb 2026 15:41:36 +0000</pubDate></item><item><title>Hiring the Canonical way: trust, humanity, and remote-first thinking</title><link>https://ubuntu.com//blog/hiring-the-canonical-way</link><description>&lt;p&gt;Discover the human-centric hiring philosophy at Canonical. Learn how the makers of Ubuntu prioritize remote-first talent, human-led CV reviews, and finding the right role for your unique impact. Explore the career with us!&lt;/p&gt;
</description><content:encoded>
&lt;p&gt;Daniele Procida (Director of Engineering) recently shared a practical guide on &lt;a href="https://canonical.com/blog/how-to-get-a-job-at-canonical"&gt;how to get a job at Canonical&lt;/a&gt;. It is an excellent resource for anyone navigating our hiring process. I wanted to build on that and share the philosophy behind those steps. As the Chief of Staff for software engineering, I see how our values shape every hiring decision we make.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Humanity as a hiring principle&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Canonical is the publisher of Ubuntu, one of the world’s most popular open-source Linux distributions. The name “Ubuntu” comes from an African philosophy that translates to “I am what I am because of who we all are.” For our operating system, this means technology should be accessible to everyone and empower people rather than exclude them.&lt;/p&gt;
&lt;p&gt;We apply this same principle to our hiring. We are a truly remote company because we believe talent isn’t bound by a certain geography. If we want software to be accessible to everyone, our career opportunities must be too. Great team members are those who don’t need an office to feel driven, they are motivated by the mission itself. We are willing to find the right place for the right people, regardless of where they are in the world. &lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Work that matters, at a scale that counts&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;When an organization is growing, it’s important to ensure that each new hire can have an impact. A growing organization shouldn’t make it harder for an individual to make their mark. At Canonical, we’re looking for people who won’t just fit into the organization, but who will challenge the way things are done. &lt;/p&gt;
&lt;p&gt;At Canonical, we believe that impact and potential are the true drivers of career progression. My own journey illustrates how the company looks beyond conventional industry templates to find the right person for the right challenge. I joined the company at 28, with five years of experience (and rapid career growth!), as the Chief of Staff for the entire engineering organization. In most companies, a role of this scope is reserved for those meeting predefined experience thresholds.&lt;/p&gt;
&lt;p&gt;When I applied, it was for a Director of DevOps role. However, during the process, my Hiring Lead identified a more strategic need. Canonical was transitioning from a scale-up to an enterprise, and we needed to ensure that our 50 engineering teams were all on the same page, with the same goals, speaking the same language. This is a challenge for any growing organization, but because I had experience aligning large teams in my previous role, Canonical chose to trust that vision and capability.&lt;/p&gt;
&lt;p&gt;Since then, we have scaled from 600 to 950 engineers. We replaced the argument of “it has always been this way” with unified standards and a predictable cadence. This approach also ensures we hire open-minded people who aren’t set in their ways. It’s important that candidates bring experience, but are still happy to challenge the way things are done. &lt;/p&gt;
&lt;p&gt;My role was not to impose a generic industry framework, but to find the best ways of working for Canonical’s specific context. We value sharp minds and high energy because, in a fast-paced environment, the ability to find a better option is more impactful than just following a traditional playbook.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Small teams, big impact&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Our CEO, Mark Shuttleworth, envisions Canonical as the “best little software company in the world.” What this means in practice is that you don’t have to be a huge company to have a far-reaching impact in the world. We intentionally organize our 950-strong engineering force into powerful, nimble squads that prioritize agility and deep expertise over organizational layers. In our culture, everyone is an expert in their field. By choosing depth over speed and strong signal over high volume, we create a space for those who are ready to go the extra mile for their colleagues. This ensures that you wake up eager to collaborate with and learn from exceptional peers. &lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;A human approach to screening&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;We do not use automated systems to screen your application. We believe that hiring needs to be fair, and that algorithms can reinforce hidden biases. Instead, hiring leads and associates review every CV manually.&lt;/p&gt;
&lt;p&gt;When you apply, we look closely at your answers to our application questions. If your answers are informative and reflect clear thinking, we progress you to the next stage and give you the chance to show your skills and give some context to your CV.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Finding the right home for talent&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Recognizing individual potential is central to how we work. We don’t just see the role you applied for. We look for the best place for your talents to thrive within our community. Applying for the “wrong” role is not a disqualification. If we see potential that matches our core principles, but not necessarily the role you applied for, we will redirect you. We do not lower the  bar, but we do invest time in finding the right fit for strong candidates.&lt;/p&gt;
&lt;p&gt;We have seen engineers apply for a specific product team, only to find a far more significant impact in a different domain where their passion and technical rigor were the missing piece of the puzzle. For instance, one engineer applied for a role in container orchestration, but we identified a stronger alignment with our distributed database team. By finding that perfect fit, they were able to thrive and eventually grow into a staff engineer role, owning the performance and correctness programs for the entire organization.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;A process designed for excellence&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Our hiring process is rigorous, and that is by design. We protect our culture by being deliberate, not fast. This depth of engagement is the first investment we make in your career here, ensuring you’re entering an environment where you can actually thrive.&lt;/p&gt;
&lt;p&gt;At Canonical,  every individual is a vital part of the whole. Our goal is to find the exact role where your expertise “clicks” into place, ensuring that you  have the greatest possible impact. We are looking for that perfect alignment where your unique strengths meet our most meaningful work.&lt;/p&gt;
&lt;p&gt;The most important thing for you to know is that the people in the hiring pipeline are on your side. We lean towards giving a candidate a chance and offering them the opportunity to prove their skills, rather than being rigid in our filtering.&lt;/p&gt;
&lt;p&gt;While our process is strict, your hiring lead is often your biggest advocate. We intentionally keep the system rigorous, but we work to move promising candidates through because we want to see you succeed. We want to see how you stand out amongst your peers. If you believe you can make a real difference here, I encourage you to &lt;a href="https://canonical.com/careers"&gt;apply&lt;/a&gt;!&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Further Reading&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://canonical.com/careers"&gt;Canonical Careers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://canonical.com/blog/how-to-get-a-job-at-canonical"&gt;How to get a job at Canonical&lt;/a&gt; – A blog by Daniele Procida (Director of Engineering)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://canonical.com/careers/hiring-process"&gt;Canonical hiring process&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content:encoded><author>Maksim Beliaev (Maksim Beliaev)</author><category>careers</category><pubDate>Fri, 06 Feb 2026 14:44:31 +0000</pubDate></item><item><title>SpacemiT announces the availability of  Ubuntu on K3/K1 series RISC-V AI computing platforms</title><link>https://ubuntu.com//blog/spacemit-announces-availability-of-ubuntu-on-k3-k1-series</link><description>&lt;p&gt;SpacemiT (Hangzhou) Technology Co., Ltd. today announced a  collaboration with Canonical to make  Ubuntu available on SpacemiT&amp;#8217;s new K3 SoC and the existing K1 series RISC-V computing platforms. This collaboration marks a deep integration between open-source operating systems and open RISC-V silicon, bringing powerful, flexible, and reliable intelligent computing solutions to developers worldwide. Hardware platforms [&amp;hellip;]&lt;/p&gt;
</description><content:encoded>
&lt;p&gt;SpacemiT (Hangzhou) Technology Co., Ltd. today announced a  collaboration with Canonical to make  Ubuntu available on SpacemiT’s new K3 SoC and the existing K1 series RISC-V computing platforms. This collaboration marks a deep integration between open-source operating systems and open RISC-V silicon, bringing powerful, flexible, and reliable intelligent computing solutions to developers worldwide.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Hardware platforms designed for high-performance intelligent computing&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;SpacemiT’s new K3 series and the existing K1 series chips are built on proprietary RISC-V processor cores and integrate homogeneous AI computing capabilities. Designed for high-performance AI edge inference and AI robotics, the platforms are well suited for applications including robotics, intelligent security at the edge, industrial vision, and AI edge inference.&lt;/p&gt;
&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="" height="1139" loading="lazy" sizes="(min-width: 1267px) 1267px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1267/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fac59%2Fimage-1.jpeg" srcset="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fac59%2Fimage-1.jpeg 460w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fac59%2Fimage-1.jpeg 620w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fac59%2Fimage-1.jpeg 1036w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1681/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fac59%2Fimage-1.jpeg 1681w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1920/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fac59%2Fimage-1.jpeg 1920w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1267/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fac59%2Fimage-1.jpeg 1267w" width="1267"/&gt;&lt;/figure&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Advancing intelligent computing: seamless OS integration &lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;With the rapid advancement of artificial intelligence, edge computing, and robotics, the market increasingly requires hardware and software platforms that deliver both strong computing performance and high development efficiency. Built on advanced RISC-V architecture and innovative system design, SpacemiT’s K3 and K1 series chips provide an ideal platform for RISC-V high-performance computing. Ubuntu, one of the world’s most widely used Linux distributions, is known for its stability, rich software ecosystem, and strong community support. The availability of  Ubuntu on the K3 and K1 series means developers benefit from seamless hardware-software integration as well as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Familiar environment: low barrier to entry for  existing Ubuntu users&lt;/li&gt;
&lt;li&gt;Ecosystem compatibility: seamless access to Ubuntu’s vast software repositories and application ecosystem&lt;/li&gt;
&lt;li&gt;Community-driven innovation: access to global Ubuntu community resources, documentation and shared expertise&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="" height="713" loading="lazy" sizes="(min-width: 1267px) 1267px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1267/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fa872%2Fimage.png" srcset="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fa872%2Fimage.png 460w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fa872%2Fimage.png 620w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fa872%2Fimage.png 1036w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1681/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fa872%2Fimage.png 1681w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1920/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fa872%2Fimage.png 1920w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1267/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fa872%2Fimage.png 1267w" width="1267"/&gt;&lt;/figure&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Introducing one of the first RVA23 compliant platforms with Ubuntu&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The RISC-V RVA23 profile, introduced in 2024, defines a mandatory set of hardware features for 64-bit processors to help facilitate a consistent software-to-hardware interface. Some of the mandatory features include vector computing for AI workloads, advanced security extensions and hypervisor support. SpacemiT’s K3 SoC is one of the first RVA23 compliant platforms available today. Canonical announced  &lt;a href="https://canonical.com/blog/canonical-releases-ubuntu-25-10-questing-quokka"&gt;adoption of RVA23&lt;/a&gt; as a baseline profile for RISC-V builds with the release of Ubuntu 25.10. This enables users of RVA23 compliant platforms with Ubuntu to leverage the full power of the chip out-of-the-box and gain a stable, high performing experience. &lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Collaborating on platform enablement &lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;With RVA23 compliance in mind, SpacemiT has taken the lead in enabling the Ubuntu 26.04 LTS Preview release on the K3 SoC, delivering a new RISC-V desktop experience. The enablement of Ubuntu 26.04 LTS, which will officially be released in April 2026, on the K3 chip will support RVA23-compliant computing systems and products worldwide, representing a milestone in the development of the RISC-V software ecosystem. This is expected to attract more developers and partners, accelerating ecosystem maturity with greater speed and efficiency.&lt;/p&gt;
&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="" height="713" loading="lazy" sizes="(min-width: 1267px) 1267px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1267/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fef14%2Fimage.png" srcset="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fef14%2Fimage.png 460w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fef14%2Fimage.png 620w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fef14%2Fimage.png 1036w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1681/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fef14%2Fimage.png 1681w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1920/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fef14%2Fimage.png 1920w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1267/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2Fef14%2Fimage.png 1267w" width="1267"/&gt;&lt;/figure&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Ubuntu 24.04 LTS availability on SpacemiT’s existing K1 platform&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In addition to enabling Ubuntu on the new K3 chip, SpacemiT and Canonical have also collaborated to enable Ubuntu 24.04 LTS across the broader SpacemiT lineup, including the K1 MUSE Book and MUSE Pi Pro. This initiative reflects the companies’ shared mission to bring a world-class Linux experience to the entire SpacemiT community. &lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;A win-win collaboration to accelerate the RISC-V ecosystem&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;“Ubuntu is one of the most popular Linux distributions globally, and we are delighted to work with Canonical to bring Ubuntu to our K3/K1 intelligent computing platforms and jointly advance the open RISC-V architecture and Linux software ecosystem,” said Sun Yanbang, President of SpacemiT. “This collaboration not only provides developers with a more complete technical solution, but also reflects SpacemiT’s long-term commitment to open standards and open-source ecosystems. By combining Ubuntu’s mature software ecosystem with our advanced RISC-V hardware technologies, we aim to drive innovation and growth in intelligent computing,” he added.&lt;/p&gt;
&lt;p&gt;”SpacemiT has significantly contributed to the fast-growing RISC-V ecosystem and leading adoption of RVA23 with Ubuntu 26.04,” said Cindy Goldberg, VP of Cloud and Silicon Partnerships at Canonical. “We’re delighted to work with SpacemiT to make the intelligent computing capabilities of the K3 and K1 series easily accessible to developers. Canonical is committed to amplifying the impact of open source software and making it available to users everywhere. The collaboration with SpacemiT aligns perfectly with our vision and our commitment to the RISC-Vecosystem.”&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;About Canonical&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Canonical, the publisher of Ubuntu, provides open source security, support and services. Our portfolio covers critical systems, from the smallest devices to the largest clouds, from the kernel to containers, from databases to AI. With customers that include top tech brands, emerging startups, governments and home users, Canonical delivers trusted open source for everyone. Learn more at &lt;a href="https://canonical.com/"&gt;https://canonical.com/&lt;/a&gt; &lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;About SpacemiT&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Founded in November 2021, SpacemiT is a computing ecosystem company built on next-generation RISC-V architecture. The company focuses on full-stack computing technologies including high-performance RISC-V CPU cores, RISC-V AI cores, NoC interconnects, RISC-V AI CPU chips, and software systems. SpacemiT delivers end-to-end computing system solutions and is committed to building the optimal native computing platform for the next AI era, enabling new applications such as AI computers and AI robots. &lt;a href="https://www.spacemit.com"&gt;https://www.spacemit.com&lt;/a&gt; &lt;/p&gt;
&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="" height="535" loading="lazy" sizes="(min-width: 1267px) 1267px, 100vw" src="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1267/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F41d4%2Fimage-1.jpeg" srcset="https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_460/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F41d4%2Fimage-1.jpeg 460w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_620/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F41d4%2Fimage-1.jpeg 620w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1036/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F41d4%2Fimage-1.jpeg 1036w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1681/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F41d4%2Fimage-1.jpeg 1681w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1920/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F41d4%2Fimage-1.jpeg 1920w, https://res.cloudinary.com/canonical/image/fetch/f_auto,q_auto,fl_sanitize,c_fill,w_1267/https%3A%2F%2Fubuntu.com%2Fwp-content%2Fuploads%2F41d4%2Fimage-1.jpeg 1267w" width="1267"/&gt;&lt;/figure&gt;
</content:encoded><author>Canonical (Canonical)</author><category>RISC-V</category><category>silicon</category><category>Ubuntu</category><pubDate>Thu, 05 Feb 2026 12:00:00 +0000</pubDate></item></channel></rss>