<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="https://inside.java/feed.xml" rel="self" type="application/atom+xml" /><link href="https://inside.java/" rel="alternate" type="text/html" /><updated>2026-01-22T11:04:55+00:00</updated><id>https://inside.java/feed.xml</id><title type="html">insidejava</title><subtitle>News and views from members of the Java team at Oracle</subtitle><entry><title type="html">Carrier Classes; Beyond Records - Inside Java Newscast #105</title><link href="https://inside.java/2026/01/22/newscast-105/" rel="alternate" type="text/html" title="Carrier Classes; Beyond Records - Inside Java Newscast #105" /><published>2026-01-22T00:00:00+00:00</published><updated>2026-01-22T00:00:00+00:00</updated><id>https://inside.java/2026/01/22/Newscast-105</id><content type="html" xml:base="https://inside.java/2026/01/22/newscast-105/"><![CDATA[<div class="youtube-embed">
    <iframe src="https://www.youtube.com/embed/cpGceyn7DBE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</div>

<p><em>This episode presents Project Amber lead Brian Goetz’s recent email “Data Oriented Programming, Beyond Records”, wherein he describes plans to improve Java’s data handling capabilities by introducing carrier classes, a generalization of records.</em> 
<em>Like them, carrier classes describe their state through a component list that defines the type’s external API: accessors, a constructor, and matching deconstructor - this allows carrier classes to participate in pattern matching and reconstruction. Unlike records, the implementation of this API remains the developer’s task although component fields offer a shortcut for the common case where the API does map to a field. Carrier classes don’t have to be final (and can hence participate in inheritance) and neither do their fields (so they can be mutable data carriers).</em></p>

<p><em>The email also mentions carrier interfaces, allowing records to be abstract as well as a relaxation of deconstruction patterns that make them more amenable to evolution of the matched type. This episode also briefly touches on Gavin Bierman’s mail to the Project Amber mailing list that announces pattern assignments and constant patterns.</em></p>

<p><em>Make sure to check the <a href="https://www.youtube.com/watch?v=cpGceyn7DBE">show-notes</a>.</em></p>]]></content><author><name>[&quot;NicolaiParlog&quot;]</name></author><category term="Amber" /><summary type="html"><![CDATA[This episode presents Project Amber lead Brian Goetz’s recent email “Data Oriented Programming, Beyond Records”, wherein he describes plans to improve Java’s data handling capabilities by introducing carrier classes, a generalization of records.]]></summary></entry><entry><title type="html">Optimizing GPU Programs from Java using Babylon and HAT</title><link href="https://inside.java/2026/01/19/hat-matmul-gpu/" rel="alternate" type="text/html" title="Optimizing GPU Programs from Java using Babylon and HAT" /><published>2026-01-19T00:00:00+00:00</published><updated>2026-01-19T00:00:00+00:00</updated><id>https://inside.java/2026/01/19/hat-matmul-gpu</id><content type="html" xml:base="https://inside.java/2026/01/19/hat-matmul-gpu/"><![CDATA[<p><img class="webfeedsFeaturedVisual" style="display: none;" src="/images/thumbnail/code.jpg" /></p>]]></content><author><name>[&quot;JuanFumero&quot;]</name></author><category term="Babylon" /><summary type="html"><![CDATA[The Heterogeneous Accelerator Toolkit (HAT) is a parallel programming framework that allows Java developers to offload Java code and dispatch the generated code on modern hardware accelerators, such as Graphics Processing Units (GPUs). This article provides an overview of the HAT programming model: using matrix-multiplication as an example, we demonstrate how Java developers can tune GPU workloads from the Java side to achieve performance close to native cuBLAS, scaling from 7 GFLOP/s on CPUs to 14 TFLOP/s on an NVIDIA A10 GPU.]]></summary></entry><entry><title type="html">1B Rows with the Memory API - JEP Cafe #25</title><link href="https://inside.java/2026/01/17/jepcafe25/" rel="alternate" type="text/html" title="1B Rows with the Memory API - JEP Cafe #25" /><published>2026-01-17T00:00:00+00:00</published><updated>2026-01-17T00:00:00+00:00</updated><id>https://inside.java/2026/01/17/JEPCafe25</id><content type="html" xml:base="https://inside.java/2026/01/17/jepcafe25/"><![CDATA[<div class="youtube-embed">
    <iframe src="https://www.youtube.com/embed/lVORyDhQzf8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</div>

<p><em>In this JEP Café episode, we take on the well-known ‘1 Billion Rows Challenge’ and implement it using the standard Memory API introduced in JDK 22. The goal of this implementation is not to break any speed records, but to show you how one can process binary data using standard patterns offered by the JDK, in a clean and efficient way.</em></p>

<p><em>You will see how the different elements of the Memory API are working together. First, you will see which Arena implementation to choose from, among the four given to you by the API. Then you will create Memory Segments to map your multi gigabytes file in the off-heap memory. And then, you will define MemoryLayouts to describe their content in a structured way. Lastly, you will use VarHandles and parallel streams to access and process this data. You see how fast you can go with these clean and standard patterns, that you can use in your own application.</em></p>

<p>Make sure to check the <a href="https://www.youtube.com/watch?v=lVORyDhQzf8">show-notes</a>!</p>]]></content><author><name>[&quot;JosePaumard&quot;]</name></author><category term="Panama" /><category term="Performance" /><summary type="html"><![CDATA[In this JEP Café episode, we take on the well-known '1 Billion Rows Challenge' and implement it using the standard Memory API introduced in JDK 22. The goal of this implementation is not to break any speed records, but to show you how can process binary data using standard patterns offered by the JDK, in a clean and efficient way.]]></summary></entry><entry><title type="html">One Giant Leap: 95% Less Sampling Cost</title><link href="https://inside.java/2026/01/14/user-cpu-time-jvm/" rel="alternate" type="text/html" title="One Giant Leap: 95% Less Sampling Cost" /><published>2026-01-14T00:00:00+00:00</published><updated>2026-01-14T00:00:00+00:00</updated><id>https://inside.java/2026/01/14/user-cpu-time-jvm</id><content type="html" xml:base="https://inside.java/2026/01/14/user-cpu-time-jvm/"><![CDATA[<p><img class="webfeedsFeaturedVisual" style="display: none;" src="/images/thumbnail/code.jpg" /></p>]]></content><author><name>[&quot;JonasNorlinder&quot;]</name></author><category term="JDK 26" /><category term="HotSpot" /><category term="Performance" /><summary type="html"><![CDATA[The public standard in Linux for reading thread CPU time is the computational equivalent of printing a spreadsheet to paper, then scanning it with a camera, only to digitize it back into a spreadsheet just to read a value. It works, but it destroys throughput. I recently integrated a patch into OpenJDK that replaces this legacy logic with direct calls. The result is a 20x speedup for thread monitoring of user CPU time. Here is the story behind the optimization and why "everything is a file" isn't the right philosophy for performance.]]></summary></entry><entry><title type="html">The Static Dynamic JVM – A Many Layered Dive #JVMLS</title><link href="https://inside.java/2026/01/11/jvmls-static-dynamic-jvm/" rel="alternate" type="text/html" title="The Static Dynamic JVM – A Many Layered Dive #JVMLS" /><published>2026-01-11T00:00:00+00:00</published><updated>2026-01-11T00:00:00+00:00</updated><id>https://inside.java/2026/01/11/JVMLS-Static-Dynamic-JVM</id><content type="html" xml:base="https://inside.java/2026/01/11/jvmls-static-dynamic-jvm/"><![CDATA[<div class="youtube-embed">
    <iframe src="https://www.youtube.com/embed/RCxYsdeglDA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</div>

<p><em>Dive deep into the Java Virtual Machine and discover how it masterfully balances static analysis with dynamic execution. John Rose explores what makes the JVM both powerful and efficient, from theoretical computer science to real-world optimization techniques.</em></p>

<p><em>Make sure to check <a href="https://www.youtube.com/playlist?list=PLX8CzqL3ArzUOgZpIX6GsoRhPbnij-sco">the JVMLS 2025 playlist</a>.</em></p>]]></content><author><name>[&quot;JohnRose&quot;]</name></author><category term="Performance" /><summary type="html"><![CDATA[Dive deep into the Java Virtual Machine and discover how it masterfully balances static analysis with dynamic execution. John Rose explores what makes the JVM both powerful and efficient, from theoretical computer science to real-world optimization techniques.]]></summary></entry><entry><title type="html">Run Into the New Year with Java’s Ahead-of-Time Cache Optimizations</title><link href="https://inside.java/2026/01/09/run-aot-cache/" rel="alternate" type="text/html" title="Run Into the New Year with Java’s Ahead-of-Time Cache Optimizations" /><published>2026-01-09T00:00:00+00:00</published><updated>2026-01-09T00:00:00+00:00</updated><id>https://inside.java/2026/01/09/run-aot-cache</id><content type="html" xml:base="https://inside.java/2026/01/09/run-aot-cache/"><![CDATA[<p>As a new year begins, turn your focus to boosting your Java application performance by applying Ahead-of-Time (AOT) cache features added in recent JDK releases. This article guides you through using AOT cache optimizations in your application, thereby minimizing startup time and achieving faster peak performance.</p>

<h2 id="what-is-the-ahead-of-time-cache-in-the-jdk">What Is the Ahead-of-Time Cache in the JDK</h2>

<p>JDK 24 introduced the Ahead-Of-Time (AOT) cache, a HotSpot JVM feature that stores classes after they are read, parsed, loaded, and linked. 
Creating an AOT cache is specific to an application, and you can reuse it in subsequent runs of that application to improve the time to the first functional unit of work (startup time) and time to peak performance (warm up time).</p>

<p>To generate an AOT cache, you need to perform two steps:</p>

<ul>
  <li>Training by recording observations of the application in action. You can trigger a recording by setting <code class="language-plaintext highlighter-rouge">-XX:AOTMode=record</code> and giving a destination for the configuration file via <code class="language-plaintext highlighter-rouge">-XX:AOTConfiguration</code>:</li>
</ul>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>java <span class="nt">-XX</span>:AOTMode<span class="o">=</span>record <span class="nt">-XX</span>:AOTConfiguration<span class="o">=</span>app.aotconf <span class="se">\</span>
  <span class="nt">-cp</span> app.jar com.example.App ...
</code></pre></div></div>

<p>This step aims to answer questions like “Which classes does the application load and initialize?”, “Which methods become hot?” and store the results in a configuration file (<code class="language-plaintext highlighter-rouge">app.aotconf</code>).</p>

<ul>
  <li>Assembly then converts the observations from the configuration file into an AOT cache (<code class="language-plaintext highlighter-rouge">app.aot</code>).</li>
</ul>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>java <span class="nt">-XX</span>:AOTMode<span class="o">=</span>create <span class="nt">-XX</span>:AOTConfiguration<span class="o">=</span>app.aotconf <span class="se">\</span>
  <span class="nt">-XX</span>:AOTCache<span class="o">=</span>app.aot <span class="nt">-cp</span> app.jar
</code></pre></div></div>

<p>To benefit from a better startup time, run the application by pointing the <code class="language-plaintext highlighter-rouge">-XX:AOTCache</code> flag to the AOT cache you created in the previous steps.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>java <span class="nt">-XX</span>:AOTCache<span class="o">=</span>app.aot <span class="nt">-cp</span> app.jar com.example.App ...
</code></pre></div></div>

<p>The improved startup time is the result of shifting work, usually done just-in-time when the program runs, earlier to the second step, which creates the cache. Thereafter, the program starts up faster in the third phase because its classes are available from the cache immediately.</p>

<p>The three-step workflow (train+assemble+run) became available starting with JDK 24, via <a href="https://openjdk.org/jeps/483">JEP 483: Ahead-of-Time Class Loading &amp; Linking</a>, the first feature merged from the research done by <a href="https://openjdk.org/projects/leyden/">Project Leyden</a>.
A set of <a href="https://github.com/openjdk/leyden/blob/634547513c2a2b707ae43a735dc24fd1977da2ae/README.md#5-benchmarking">benchmarks</a> proves the effectiveness of this feature and other Leyden performance-related ones, as displayed by <em>Figure 1</em>.</p>

<p><img src="/images/performance/aot-cache-bench-jdk24.png" alt="JDK 24 AOT Cache Benchmarks" />
Figure 1: AOT Cache Benchmarks as of JDK 24</p>

<p>In JDK 25, the changes in <a href="https://openjdk.org/jeps/515">JEP 515 - Ahead-of-Time Method Profiling</a> enabled frequently executed method profiles to be part of the AOT cache.
This addition improves application warm up by allowing the JIT to start generating native code immediately at application startup. The new AOT feature does not require you to add more constraints to your application execution, just use the existing AOT cache creation commands.
Moreover, <a href="https://github.com/openjdk/leyden/blob/634547513c2a2b707ae43a735dc24fd1977da2ae/README.md">benchmarks</a> showed improved startup time too (<em>Figure 2</em>).</p>

<p><img src="/images/performance/aot-cache-bench-jdk25.png" alt="JDK 25 AOT Cache Benchmarks" />
Figure 2: AOT Cache Benchmarks as of JDK 25</p>

<p>JDK 25 also simplified the process for generating an AOT cache by making it possible to do it in a single step, through setting the argument for <code class="language-plaintext highlighter-rouge">-XX:AOTCacheOutput</code> flag:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Training Run + Assembly Phase</span>
java <span class="nt">-XX</span>:AOTCacheOutput<span class="o">=</span>app.aot <span class="se">\</span>
     <span class="nt">-cp</span> app.jar com.example.App ...
</code></pre></div></div>
<p>Upon passing <code class="language-plaintext highlighter-rouge">-XX:AOTCacheOutput=[cache location]</code>, the JVM creates the cache on its shutdown.
<a href="https://openjdk.org/jeps/514">JEP 514 - Ahead-of-Time Command-Line Ergonomics</a> introduced the two-step process for creating and using the AOT cache.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Training Run + Assembly Phase</span>
java <span class="nt">-XX</span>:AOTCacheOutput<span class="o">=</span>app.aot <span class="se">\</span>
     <span class="nt">-cp</span> app.jar com.example.App ...
     
<span class="c"># Deployment Run</span>
java <span class="nt">-XX</span>:AOTCache<span class="o">=</span>app.aot <span class="nt">-cp</span> app.jar com.example.App ...
</code></pre></div></div>

<p>The two-step workflow may not work as expected in resource-constrained environments. The sub-invocation that creates the AOT cache uses its own Java heap with the same size as the heap used for the <a href="https://openjdk.org/projects/leyden/notes/05-training-runs">training run</a>. 
As a result, the memory needed to complete the one-step AOT cache generation is double the heap size specified on the command line. 
For example, if the step <code class="language-plaintext highlighter-rouge">java -XX:AOTCacheOutput=...</code> has also <code class="language-plaintext highlighter-rouge">-Xms2g -Xmx2g</code> options appended, specifying a 2GB heap, then the environment needs 4GB to complete the workflow.</p>

<p>A division of steps, as in a three-phase workflow, may be a better choice if you intend to deploy an application to small cloud tenancies. 
In such cases, you could run the training on a small instance while creating the AOT cache on a larger one. That way, the training run reflects the deployment environment, while the AOT cache creation can leverage the additional CPU cores and memory of the large instance.</p>

<p>Regardless of which workflow you choose, let’s take a closer look at AOT cache requirements and how to set it up to serve your application needs best.</p>

<h2 id="how-to-craft-the-aot-cache-your-application-needs">How to Craft the AOT Cache Your Application Needs</h2>

<p>Training and production runs should produce consistent results, just faster in deployment runs. To achieve that, the assembly phase intermediates what happens between training and production runs (<em>Figure 3</em>).</p>

<p><img src="/images/performance/train-assembly-run.png" alt="Train-Assembly-Deploy" />
Figure 3: Training / Assembly / Deployment</p>

<p>For consistent training runs and the subsequent ones, make sure that:</p>

<ul>
  <li>The timestamp of the JARs is preserved across training runs.</li>
  <li>Your training runs and the production one use the same JDK release for the same hardware architecture and operating system.</li>
  <li>Your application behavior in your training run resembles the expected behavior of your application in production (e.g. the most highly used areas of your application in your training run, are the mostly highly used areas of your application in production).</li>
  <li>Provide the classpath for your application as a list of JARs, without any directories, wildcards or nested JARs.</li>
  <li>Production run classpath must be a superset of the training one.</li>
  <li>Do not use use JVMTI agents that call the <code class="language-plaintext highlighter-rouge">AddToBootstrapClassLoaderSearch</code>and <code class="language-plaintext highlighter-rouge">AddToSystemClassLoaderSearch</code> APIs.</li>
</ul>

<p>To check if your JVM is correctly configured to use the AOT cache, you can add the option <code class="language-plaintext highlighter-rouge">-XX:AOTMode=on</code> to the command line:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>java <span class="nt">-XX</span>:AOTCache<span class="o">=</span>app.aot <span class="nt">-XX</span>:AOTMode<span class="o">=</span>on <span class="se">\</span>
    <span class="nt">-cp</span> app.jar com.example.App ...
</code></pre></div></div>

<p>The JVM will report an error if the AOT cache does not exist or if your setup disregards any of the above requirements. 
The features introduced in JDK 24 and 25 did not support the Z Garbage Collector (ZGC). Yet, this limitation no longer applies as of JDK 26, with the introduction of <a href="https://openjdk.org/jeps/516">JEP 516: Ahead-of-Time Object Caching with Any GC</a>.</p>

<p>To ensure the AOT cache works effectively in production, the training run and all following runs must be essentially identical.
Training runs are a way of observing what an application is doing across different runs and are primarily two types:</p>

<ul>
  <li>integration tests, which run at build time</li>
  <li>production workloads, which require training in production.</li>
</ul>

<p>Avoid loading unused classes during the training step and skip rich test frameworks to keep the AOT cache minimal. Mock external dependencies in training to load needed classes, but be aware that this may introduce extra cache entries.
AOT cache effectiveness depends on how closely the training run matches production behavior. If you rebuild the application or upgrade its JDK, you must regenerate the AOT cache.</p>

<p>Note also that the AOT cache is only valid for the current state of your application. Making code changes to your application, adding libraries, updating existing libraries, will invalidate your cache. So you will need to regenerate your cache with every new build of your application.
Otherwise, you risk crashes or undefined behavior (methods missing from cache). If you are noticing issues, or not getting the expected performance improvement to your application, try running it <code class="language-plaintext highlighter-rouge">-Xlog:aot,class+path=info</code> to monitor what it loads from cache.</p>

<h2 id="tips-for-efficient-training-runs">Tips for Efficient Training Runs</h2>

<p>There is a trade-off between performance and how easy it is to run the training.
Using a production run for training is not always practical, especially for server applications, which can create log files, open network connections, access databases, etc. 
For such cases, it is better to make a synthetic training run that closely resembles actual production runs.</p>

<p>Aligning the training run to load the same classes as production helps to achieve an optimized startup time.
To determine which classes are loaded by your training run, you can append the <code class="language-plaintext highlighter-rouge">-verbose:class</code> flag upon launching it. 
Or observe the loaded classes by enabling the <code class="language-plaintext highlighter-rouge">jdk.ClassLoad</code> JFR event and profiling your application with it:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># configure the event</span>
jfr configure jdk.ClassLoad#enabled<span class="o">=</span><span class="nb">true</span>

<span class="c"># profile as soon as your application launches</span>
java <span class="nt">-XX</span>:StartFlightRecording:settings<span class="o">=</span>custom.jfc,duration<span class="o">=</span>60s,filename<span class="o">=</span>/tmp/AOT.jfr
</code></pre></div></div>
<p>Once you have a recording file, you may check the loaded classes, but also which methods your application frequently uses by running the following <code class="language-plaintext highlighter-rouge">jfr</code> commands:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># print jdk.ClassLoad events from a recording file</span>
jfr print <span class="nt">--events</span> <span class="s2">"jdk.ClassLoad"</span> /tmp/AOT.jfr

<span class="c"># view frequently executed methods</span>
jfr view hot-methods /tmp/AOT.jfr
</code></pre></div></div>
<p>If you determine that there are methods frequently used but not detected by your training run, exercise them. You can work out the standard modes of your application using a temporary file directory, a local network configuration, and a mocked database, if needed.
Avoid loading unused classes during training and skip rich test frameworks to keep the AOT cache minimal. Instead, use smoke tests to cover typical startup paths; avoid extensive suites and stress/regression tests.</p>

<h2 id="takeaways">Takeaways</h2>

<p>To conclude, crafting an AOT cache for better performance requires you to look over:</p>

<ul>
  <li>Cache validity or staleness; if you rebuild the application or upgrade the JDK, you must regenerate the AOT cache.</li>
  <li>Portability, as the AOT cache is JVM and platform-specific.</li>
  <li>Startup path coverage; the training run must cover typical application startup paths. If your training run is shallow, you will not warm up enough, and the benefits of the cache will be limited.</li>
  <li>Operational setup as both the application JAR and the AOT cache must run with least privilege and according to immutable infrastructure practices.</li>
</ul>

<p>Application performance is an ongoing task because software evolves: new features are added, libraries and frameworks change, workloads grow, and infrastructure shifts (e.g., to the cloud, container orchestration, etc.).
And depending on those evolutions, your application performance goals evolve too. Invest in training your application today and keep up with JDK releases to unlock available optimizations, as performance improves with each of them!</p>

<p><em>The content of this article was initially shared in the <a href="https://www.javaadvent.com/2025/12/run-java-aot-cache-optimizations.html">The JVM Programming Advent Calendar</a>.</em></p>]]></content><author><name>[&quot;Ana-MariaMihalceanu&quot;]</name></author><category term="JDK 25" /><category term="Leyden" /><category term="Performance" /><summary type="html"><![CDATA[As a new year begins, turn your focus to boosting your Java application performance by applying Leyden-related features added in recent JDK releases. This article guides you on how to use Ahead-of-Time cache optimizations in your application, thereby minimizing startup time and achieving faster peak performance.]]></summary></entry><entry><title type="html">Java’s Plans for 2026 - Inside Java Newscast #104</title><link href="https://inside.java/2026/01/08/newscast-104/" rel="alternate" type="text/html" title="Java’s Plans for 2026 - Inside Java Newscast #104" /><published>2026-01-08T00:00:00+00:00</published><updated>2026-01-08T00:00:00+00:00</updated><id>https://inside.java/2026/01/08/Newscast-104</id><content type="html" xml:base="https://inside.java/2026/01/08/newscast-104/"><![CDATA[<div class="youtube-embed">
    <iframe src="https://www.youtube.com/embed/1lYsDMOc7hM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</div>

<p><em>In 2026, Java keeps evolving: Project Valhalla is gunning for merging its value types preview in the second half of this year; Babylon wants to incubate code reflection; Loom will probably finalize the structured concurrency API; Leyden plans to ship AOT code compilation; and Amber hopes to present JEPs on constant patterns and pattern assignments. And those are just the most progressed features - more are in the pipeline and discussed in this episode of the Inside Java Newscast.</em></p>

<p><em>Make sure to check the <a href="https://www.youtube.com/watch?v=1lYsDMOc7hM">show-notes</a>.</em></p>]]></content><author><name>[&quot;NicolaiParlog&quot;]</name></author><category term="Amber" /><category term="Babylon" /><category term="Leyden" /><category term="Loom" /><category term="Panama" /><category term="Valhalla" /><summary type="html"><![CDATA[In 2026, Java keeps evolving: Project Valhalla is gunning for merging its value types preview in the second half of this year; Babylon wants to incubate code reflection; Loom will probably finalize the structured concurrency API; Leyden plans to ship AOT code compilation; and Amber hopes to present JEPs on constant patterns and pattern assignments. And those are just the most progressed features - more are in the pipeline and discussed in this episode of the Inside Java Newscast.]]></summary></entry><entry><title type="html">The Inside Java Newsletter: JavaOne Sessions and Keynotes!</title><link href="https://inside.java/2026/01/05/inside-java-newsletter/" rel="alternate" type="text/html" title="The Inside Java Newsletter: JavaOne Sessions and Keynotes!" /><published>2026-01-05T00:00:00+00:00</published><updated>2026-01-05T00:00:00+00:00</updated><id>https://inside.java/2026/01/05/Inside-Java-Newsletter</id><content type="html" xml:base="https://inside.java/2026/01/05/inside-java-newsletter/"><![CDATA[<p><img class="webfeedsFeaturedVisual" src="/images/thumbnail/code.jpg" style="display: none;" /></p>]]></content><author><name>[&quot;JimGrisanzio&quot;]</name></author><category term="Oracle" /><category term="Community" /><summary type="html"><![CDATA[The Inside Java Newsletter for December 2025 includes a warm year-end message from Sharat Chander to the Java community, plus a special limited-time discount code for JavaOne 2026 registration. The JavaOne 2026 keynotes and technical sessions are now announced, so check them out and register soon. This issue also highlights the latest Java content from the Java Platform Group teams. Visit learn.java, dev.java, and inside.java for videos, articles, and other resources for developers, learners, educators, and customers. You can view the newsletter archives, subscribe, and share it with a friend!]]></summary></entry><entry><title type="html">Episode 43 “Predictability or Innovation? Both!” with Georges Saab</title><link href="https://inside.java/2025/12/26/podcast-043/" rel="alternate" type="text/html" title="Episode 43 “Predictability or Innovation? Both!” with Georges Saab" /><published>2025-12-26T00:00:00+00:00</published><updated>2025-12-26T00:00:00+00:00</updated><id>https://inside.java/2025/12/26/Podcast-043</id><content type="html" xml:base="https://inside.java/2025/12/26/podcast-043/"><![CDATA[<p><img class="webfeedsFeaturedVisual" style="display: none;" src="/images/thumbnail/ChadMic.jpg?994168026" /></p>

<p><br /></p>
<iframe title="Libsyn Player" style="border: none" src="//html5-player.libsyn.com/embed/episode/id/39492755/height/90/theme/custom/thumbnail/yes/direction/forward/render-playlist/no/custom-color/000000/" height="90" width="100%" scrolling="no"></iframe>

<div class="youtube-embed">
<iframe src="https://www.youtube.com/embed/BMVQjn7cMNs?si=c3wfTOItdA738hpO" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</div>

<p><br /></p>

<p>This Inside Java Podcast takes a meta approach. Instead of focusing on specific features, it explores the bigger picture: What are the right problems for Java to tackle? What are the current and future challenges for the Java platform? Why is predictability so important for Java, and what’s driving the recent focus on learners and students?</p>

<p>Nicolai Parlog discusses these topics with Georges Saab, Senior Vice President of the Java Platform Group and Chair of the OpenJDK Governing Board.</p>

<p><br />
Make sure to also check the <strong>Duke’s Corner podcast</strong> on <a href="https://dev.java/duke/corner/">dev.java</a>.</p>

<p><br /></p>

<h3 id="additional-resources">Additional resources</h3>

<ul>
  <li><a href="https://inside.java">Inside.java</a> : News and views from members of the Java team at Oracle</li>
  <li><a href="https://dev.java">Dev.java</a> : The Destination for Java Developers</li>
  <li><a href="https://openjdk.java.net/">OpenJDK</a></li>
  <li><a href="https://www.oracle.com/java/">Oracle Java</a></li>
</ul>

<p>For more episodes, check out <a href="https://inside.java/podcast">Inside Java</a>, our <a href="https://www.youtube.com/playlist?list=PLX8CzqL3ArzV_hXbRevwzrXSMcGNzhxiZ">YouTube playlist</a>, and follow <a href="https://twitter.com/java">@Java</a> on Twitter.</p>

<p>Contact us <a href="https://inside.java/about/">here</a>.</p>]]></content><author><name>[&quot;GeorgesSaab&quot;, &quot;NicolaiParlog&quot;]</name></author><category term="Oracle" /><summary type="html"><![CDATA[Nicolai Parlog discusses these topics with Georges Saab, Senior Vice President of the Java Platform Group and Chair of the OpenJDK Governing Board...]]></summary></entry><entry><title type="html">Virtual Threads in the Real World: Fast, Robust Java Microservices with Helidon</title><link href="https://inside.java/2025/12/21/virtual-threads-robust-java-microservices/" rel="alternate" type="text/html" title="Virtual Threads in the Real World: Fast, Robust Java Microservices with Helidon" /><published>2025-12-21T00:00:00+00:00</published><updated>2025-12-21T00:00:00+00:00</updated><id>https://inside.java/2025/12/21/Virtual-Threads-Robust-Java-Microservices</id><content type="html" xml:base="https://inside.java/2025/12/21/virtual-threads-robust-java-microservices/"><![CDATA[<div class="youtube-embed">
    <iframe src="https://www.youtube.com/embed/2vhq2I5bSG0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</div>

<p><em>In 2022, the Helidon team made a significant decision: re-write our Netty-based Helidon Web Server to be fully implemented using virtual threads. The result is Helidon 4: the first microservices framework designed from the ground up for virtual threads. We are thrilled with the results, and have learned a few things along the way. Come join us as we introduce you to the benefits of virtual threads, share our lessons learned, give a few tips-and-tricks, and highlight what to look forward to in Java 24 and beyond.</em></p>

<p><em>Make sure to check the <a href="https://www.youtube.com/playlist?list=PLX8CzqL3ArzVV1xRJkRbcM2tOgVwytJAi">JavaOne 2025 playlist</a>.</em></p>]]></content><author><name></name></author><category term="Loom" /><summary type="html"><![CDATA[In 2022, our project made a significant decision: re-write our Netty-based Helidon Web Server to be fully implemented using virtual threads. The result is Helidon 4: the first microservices framework designed from the ground up for virtual threads. We are thrilled with the results, and have learned a few things along the way. Come join us as we introduce you to the benefits of virtual threads, share our lessons learned, give a few tips-and-tricks, and highlight what to look forward to in Java 24 and beyond.]]></summary></entry></feed>